holas@cogsci:~/rllab/sandbox/asl$ rl
(rllab3) holas@cogsci:~/rllab/sandbox/asl$ runs/example.py
1 #define _CUDA_NDARRAY_C
2
3 #include <Python.h>
4 #include <structmember.h>
5 #include "theano_mod_helper.h"
6
7 #include <numpy/arrayobject.h>
8 #include <iostream>
9
10 #include "cuda_ndarray.cuh"
11
12 #ifndef CNMEM_DLLEXPORT
13 #define CNMEM_DLLEXPORT
14 #endif
15
16 #include "cnmem.h"
17 #include "cnmem.cpp"
18
19 //If true, when there is a gpu malloc or free error, we print the size of allocated memory on the device.
20 #define COMPUTE_GPU_MEM_USED 0
21
22 //If true, we fill with NAN allocated device memory.
23 #define ALLOC_MEMSET 0
24
25 //If true, we print out when we free a device pointer, uninitialize a
26 //CudaNdarray, or allocate a device pointer
27 #define PRINT_FREE_MALLOC 0
28
29 //If true, we do error checking at the start of functions, to make sure there
30 //is not a pre-existing error when the function is called.
31 //You probably need to set the environment variable
32 //CUDA_LAUNCH_BLOCKING=1, and/or modify the CNDA_THREAD_SYNC
33 //preprocessor macro in cuda_ndarray.cuh
34 //if you want this to work.
35 #define PRECHECK_ERROR 0
36
37 cublasHandle_t handle = NULL;
38 int* err_var = NULL;
39
40 /////////////////////////
41 // Alloc and Free
42 /////////////////////////
43
44 static int g_gpu_context_active = 0;
45
46
47 PyObject *
48 CudaNdarray_Dimshuffle(PyObject* _unused, PyObject* args);
49 static PyObject *CudaNdarray_get_shape(CudaNdarray *self, void *closure);
50
51
52 /**
53  *
54  * In the test program I'm using, the _outstanding_mallocs decreases with every call.
55  * This suggests there are more free() calls being made than alloc(), but I can't figure out why.
56  *
57  */
58 int _outstanding_mallocs[] = {0,0};
59
60 #if COMPUTE_GPU_MEM_USED
61 size_t _allocated_size = 0;
62 size_t _max_allocated_size = 0;
63
64 const int TABLE_SIZE = 10000;
65 struct table_struct{
66     void* ptr;
67     size_t size;
68 };
69 table_struct _alloc_size_table[TABLE_SIZE];
70 #endif
71
72 void * device_malloc(size_t size)
73 {
74     return device_malloc(size, VERBOSE_DEVICE_MALLOC);
75 }
76
77 ///@TODO: thejaswi: link this option to a theano config variable?
78 static bool g_use_cnmem = false;
79 static const int g_max_devices = 8;
80 int initCnmem(int card_number_provided, int card_nb, size_t mem) {
81     static bool cnmemInitialized = false;
82     if(cnmemInitialized) {
83         return 0;
84     }
85     // On stderr to be at the same place as "Using gpu device..."
86     int numDevices = 0;
87     cnmemDevice_t devices[g_max_devices];
88     if(cudaGetDeviceCount(&numDevices) != cudaSuccess) {
89         PyErr_Format(PyExc_RuntimeError,
90                      "initCnmem: 'cudaGetDeviceCount' failed! Reason=%s\n",
91                      cudaGetErrorString(cudaGetLastError()));
92         return -1;
93     }
94     if(card_number_provided){
95         numDevices = 1;
96         int i = 0;
97         devices[i].device = card_nb;
98         devices[i].size = mem;
99         ///@TODO: thejaswi: add support for multiple streams
100         devices[i].numStreams = 0;
101         devices[i].streams = NULL;
102         devices[i].streamSizes = NULL;
103     }else{
104         for(int i=0;i<numDevices;++i) {
105             devices[i].device = i;
106             devices[i].size = mem;
107             ///@TODO: thejaswi: add support for multiple streams
108             devices[i].numStreams = 0;
109             devices[i].streams = NULL;
110         }
111     }
112
113     ///@TODO: thejaswi: passing custom cnmem flags?
114     cnmemStatus_t status = cnmemInit(numDevices, devices, CNMEM_FLAGS_DEFAULT);
115     if(status != CNMEM_STATUS_SUCCESS) {
116         PyErr_Format(PyExc_RuntimeError,
117                      "initCnmem: cnmemInit call failed! Reason=%s. numdev=%d\n",
118                      cnmemGetErrorString(status), numDevices);
119         return -1;
120     }
121     cnmemInitialized = true;
122     return 0;
123 }
124
125 void * device_malloc(size_t size, int verbose)
126 {
127     #if PRECHECK_ERROR
128         cudaThreadSynchronize();
129         cudaError_t prevError = cudaGetLastError();
130         if (cudaSuccess != prevError)
131         {
132             fprintf(stderr,
133                     "Error existed before calling device_malloc. %s\n",
134                     cudaGetErrorString(prevError)
135                     );
136         }
137     #endif
138     void * rval=NULL;
139     ///@TODO: thejaswi: support for multiple-streams?
140     if(g_use_cnmem) {
141         cnmemStatus_t status = CNMEM_STATUS_SUCCESS;
142         status = cnmemMalloc(&rval, size, NULL);
143         if(status != CNMEM_STATUS_SUCCESS) {
144             PyErr_Format(PyExc_MemoryError,
145                          "Error allocating %llu bytes of device memory (%s).",
146                          (unsigned long long)size, cnmemGetErrorString(status));
147             return NULL;
148         }
149     }
150     else {
151         cudaError_t err = cudaMalloc(&rval, size);
152         if (cudaSuccess != err)
153         {
154             // Clear the error flag, cudaMalloc doesn't do it.
155             // Currently this returns the same thing as err, but if in future
156             // it returns something else I still don't see why we should ignore
157             // it.  All we want to do here is reset the flag.
158             cudaGetLastError();
159             if (verbose)
160             {
161                 size_t free = 0, total = 0;
162                 cudaError_t err2 = cudaMemGetInfo(&free, &total);
163                 if (err2 != cudaSuccess){
164                     cudaGetLastError();
165                     fprintf(stderr,
166                             "Error when trying to find the memory information"
167                             " on the GPU: %s\n", cudaGetErrorString(err2));
168                 }
169                 #if COMPUTE_GPU_MEM_USED
170                     fprintf(stderr,
171                             "Error allocating %llu bytes of device memory (%s)."
172                             " new total bytes allocated: %llu."
173                             " Driver report %llu bytes free and %llu bytes total \n",
174                             (unsigned long long)size, cudaGetErrorString(err), (unsigned long long)_allocated_size,
175                             (unsigned long long)free, (unsigned long long)total);
176                 #else
177                     fprintf(stderr,
178                             "Error allocating %llu bytes of device memory (%s)."
179                             " Driver report %llu bytes free and %llu bytes total \n",
180                             (unsigned long long)size, cudaGetErrorString(err), (unsigned long long)free, (unsigned long long)total);
181                 #endif
182             }
183             PyErr_Format(PyExc_MemoryError,
184                          "Error allocating %llu bytes of device memory (%s).",
185                          (unsigned long long)size, cudaGetErrorString(err));
186             return NULL;
187         }
188     }
189     if (rval != NULL){
190         // Can it happen that cudaMalloc return cudaSuccess, but return a NULL ptr?
191         // Could this be what happen if size is 0?
192         _outstanding_mallocs[0] += 1;
193
194 #if COMPUTE_GPU_MEM_USED
195         _allocated_size += size;
196         _max_allocated_size = std::max(_max_allocated_size, _allocated_size);
197         int i = 0;
198         for(;i<TABLE_SIZE;i++){
199             if(NULL==_alloc_size_table[i].ptr){
200                 _alloc_size_table[i].ptr=rval;
201                 _alloc_size_table[i].size=size;
202                 break;
203             }
204         }
205         if (i == TABLE_SIZE){
206             fprintf(stderr,
207                     "When tracking GPU malloc, our table size wasn't big enough."
208                     " So we loose some tracking. Raise the value of TABLE_SIZE in the file cuda_ndarra.cu");
209         }
210 #endif
211     }
212     //fprintf(stderr,
213     //"allocated %li bytes of device memory (%s). new total bytes allocated: %d. ptr: %p\n",
214     //(long)size, cudaGetErrorString(err),_allocated_size,rval);
215
216     if(ALLOC_MEMSET){
217         //We init them to nan to make sure we catch more debug case.
218         cudaMemset(rval, 0xFF, size);
219         //printf("MEMSET\n");
220     }
221     #if PRINT_FREE_MALLOC
222         fprintf(stderr, "device malloc %p of size %d\n", rval, size);
223     #endif
224     return rval;
225 }
226
227 int device_free(void *ptr)
228 {
229     #if PRECHECK_ERROR
230         cudaThreadSynchronize();
231         cudaError_t prevError = cudaGetLastError();
232         if (cudaSuccess != prevError)
233         {
234             fprintf(stderr,
235                     "Error existed before calling device_free. %s\n",
236                     cudaGetErrorString(prevError)
237                     );
238         }
239     #endif
240     #if PRINT_FREE_MALLOC
241         size_t free = 0, total = 0;
242         cudaError_t err2 = cudaMemGetInfo(&free, &total);
243         if (err2 != cudaSuccess){
244             cudaGetLastError();
245             fprintf(stderr,
246                     "Error when tring to find the memory information"
247                     " on the GPU: %s\n", cudaGetErrorString(err2));
248         }
249         #if COMPUTE_GPU_MEM_USED
250         {
251             int i = 0;
252             for(;i<TABLE_SIZE;i++)
253                 if(_alloc_size_table[i].ptr==ptr){
254                     break;
255                 }
256             assert(i<TABLE_SIZE);
257             fprintf(stderr, "device_free %p of size %d."
258                     " Driver report %d bytes free and %d bytes total \n",
259                     ptr, _alloc_size_table[i].size, free, total);
260         }
261         #else
262             fprintf(stderr, "device_free %p."
263                     " Driver report %d bytes free and %d bytes total \n",
264                     ptr, free, total);
265         #endif
266     #endif
267
268     // if there is no gpu context, the call to cudaFree will fail; skip it entirely
269     if(!g_gpu_context_active) {
270         return 0;
271     }
272
273     ///@TODO: thejaswi: multi-stream support
274     if(g_use_cnmem) {
275         cnmemStatus_t status = cnmemFree(ptr, NULL);
276         if(status != CNMEM_STATUS_SUCCESS) {
277             fprintf(stderr, "device_free: cnmemFree call failed! Reason=%s\n",
278                     cnmemGetErrorString(status));
279         }
280     }
281     else {
282         // We need sync as the Theano's GC could remove intermediate variable that
283         // are still needed as the gpu kernel are running or in the queue.
284         CNDA_BEGIN_ALLOW_THREADS
285         cudaThreadSynchronize();
286         CNDA_END_ALLOW_THREADS
287
288         cudaError_t err =  cudaFree(ptr);
289         if (cudaSuccess != err)
290         {
291             // Clear the error flag, cudaFree doesn't do it.
292             // Currently this returns the same thing as err, but if in future
293             // it returns something else I still don't see why we should ignore
294             // it.  All we want to do here is reset the flag.
295             cudaGetLastError();
296             size_t free = 0, total = 0;
297             cudaError_t err2 = cudaMemGetInfo(&free, &total);
298             if (err2 != cudaSuccess){
299                 cudaGetLastError();
300                 fprintf(stderr,
301                         "Error when tring to find the memory information"
302                         " on the GPU: %s\n", cudaGetErrorString(err2));
303             }
304             #if COMPUTE_GPU_MEM_USED
305             {
306                 int i = 0;
307                 for(;i<TABLE_SIZE;i++)
308                     if(_alloc_size_table[i].ptr==ptr){
309                         break;
310                     }
311                 assert(i<TABLE_SIZE);
312                 fprintf(stderr,
313                         "Error freeing device pointer %p (%s) of size %llu. %llu byte already allocated."
314                         " Driver report %llu bytes free and %llu bytes total \n",
315                         ptr, cudaGetErrorString(err),
316                         (unsigned long long)_alloc_size_table[i].size, (unsigned long long)_allocated_size, (unsigned long long)free, (unsigned long long)total);
317             }
318             #else
319                 fprintf(stderr,
320                         "Error freeing device pointer %p (%s)."
321                         " Driver report %llu bytes free and %llu bytes total \n",
322                         ptr,
323                         cudaGetErrorString(err), (unsigned long long)free, (unsigned long long)total);
324             #endif
325             if (NULL != PyErr_Occurred()){
326                 fprintf(stderr,
327                         "device_free: cudaFree() returned an error, but there is already an"
328                         " Python error set. This happen during the clean up when there is a"
329                         " first error and the CUDA driver is in a so bad state that it don't"
330                         " work anymore. We keep the previous error set to help debugging it.");
331                 return -1;
332             }
333             PyErr_Format(PyExc_MemoryError,
334                     "error freeing device pointer %p (%s)",
335                     ptr,
336                     cudaGetErrorString(err));
337             return -1;
338         }
339     }
340     _outstanding_mallocs[0] -= (ptr != NULL);
341     #if COMPUTE_GPU_MEM_USED
342         int i=0;
343         size_t total_freed = 0;
344         for(;i<TABLE_SIZE;i++)
345             if(_alloc_size_table[i].ptr==ptr){
346                 _allocated_size -= _alloc_size_table[i].size;
347                 total_freed += _alloc_size_table[i].size;
348                 _alloc_size_table[i].ptr=0;
349                 _alloc_size_table[i].size=0;
350
351                 break;
352             }
353         //if(i==TABLE_SIZE)
354         //    printf("Unallocated unknow size!\n");
355         //fprintf(stderr, "freed %li bytes of device memory (%s). %d already allocated, ptr=%p\n", (long)total_freed, cudaGetErrorString(err),_allocated_size,ptr);
356     #endif
357     return 0;
358 }
359
360 static PyObject *
361 outstanding_mallocs(PyObject* self, PyObject * args)
362 {
363     return PyInt_FromLong(_outstanding_mallocs[0]);
364 }
365
366
367 static void *work_mem = NULL;
368 static size_t work_size = 0;
369
370 /*
371  * Returns a chunk of memory for temporary work inside of an op. You can only
372  * request a single chunk of memory at a time since it is reused.
373  */
374 void *get_work_mem(size_t sz) {
375     if (sz <= work_size)
376         return work_mem;
377     device_free(work_mem);
378     work_mem = device_malloc(sz);
379     work_size = sz;
380     if (work_mem == NULL)
381         work_size = 0;
382     return work_mem;
383 }
384
385 /////////////////////////
386 // Static helper methods
387 /////////////////////////
388
389 static void
390 CudaNdarray_null_init(CudaNdarray*self)
391 {
392     self->base = NULL;
393     self->nd = -1;
394     self->host_structure = NULL;
395     self->data_allocated = 0;
396     self->dev_structure_fresh = 1;
397     self->dev_structure = NULL;
398     self->devdata = NULL;
399 }
400
401 static int
402 CudaNdarray_uninit(CudaNdarray*self)
403 {
404     #if PRINT_FREE_MALLOC
405         fprintf(stderr, "CudaNdarray_uninit %p\n", self);
406     #endif
407     int rval = 0;
408     if (self->data_allocated) {
409         assert(self->devdata);
410         if (device_free(self->devdata))
411         {
412             fprintf(stderr,
413                     "CudaNdarray_uninit: error freeing self->devdata. (self=%p, self->devata=%p)\n",
414                     self, self->devdata);
415             rval = -1;
416         }
417         self->devdata = NULL;
418         self->data_allocated = 0;
419     }
420     if (self->dev_structure)
421     {
422         if (device_free(self->dev_structure))
423         {
424             fprintf(stderr,
425                     "CudaNdarray_uninit: error freeing dev_structure memory %p (self=%p)\n",
426                     self->dev_structure, self);
427             rval = -1;
428         }
429         self->dev_structure = NULL;
430     }
431     if (self->host_structure)
432     {
433         free(self->host_structure);
434         self->host_structure = NULL;
435     }
436     self->nd = -1;
437     Py_XDECREF(self->base);
438     self->base = NULL;
439     return rval;
440 }
441
442
443 //make the rightmost coords change fastest
444 //TODO: why does a downward for-loop not work????
445 //TODO: use the log2_dims and driver code to remove / and %
446 //TODO: skip the last division (when d == 0)
447 #define decl_k_elemwise_unary_rowmajor(name, F) \
448 __global__ void name (unsigned int numEls,  \
449         unsigned int nd, \
450         const int * dim,  \
451         const float * a_data, const int * a_str, \
452         float * z_data, const int * z_str) \
453 { \
454     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x; \
455     const unsigned int numThreads = blockDim.x * gridDim.x; \
456  \
457     for (unsigned int i = idx; i < numEls; i += numThreads) \
458     { \
459         unsigned int ii = i; \
460         const float * a_i = a_data; \
461         float * z_i = z_data; \
462         for (unsigned int _d = 0; _d < nd; ++_d) \
463         { \
464             unsigned int d = nd - _d-1;  \
465             int i_d = ii % dim[d]; /* i_d is our position in the d'th dimension   */ \
466             ii = ii / dim[d]; \
467             a_i += i_d * a_str[d]; /* increment our a and z pointers by i_d elements */ \
468             z_i += i_d * z_str[d]; \
469         } \
470         z_i[0] = F(a_i[0]); \
471     } \
472 }
473
474 template<typename T> __device__ T unary_copy(T a) { return a; }
475 decl_k_elemwise_unary_rowmajor(k_elemwise_unary_rowmajor_copy, unary_copy<float>)
476
477 template<typename T> __device__ T unary_exp(T a) { return exp(a); }
478 decl_k_elemwise_unary_rowmajor(k_elemwise_unary_rowmajor_exp, unary_exp<float>)
479
480 /////////////////////////////
481 // Satisfying reqs to be Type
482 /////////////////////////////
483
484 //DON'T use directly(if their is other CudaNdarray that point to it, it will cause problem)! use Py_DECREF() instead
485 static void
486 CudaNdarray_dealloc(CudaNdarray* self)
487 {
488     if (0) std::cerr << "CudaNdarray dealloc " << self << " " << self->devdata << '\n';
489     if(Py_REFCNT(self) > 1)
490       printf("WARNING:CudaNdarray_dealloc called when there is still active reference to it.\n");
491     CudaNdarray_uninit(self);
492     Py_TYPE(self)->tp_free((PyObject*)self);
493     --_outstanding_mallocs[1];
494     if (0)
495     {
496         fprintf(stderr, "device_malloc_counts: (device) %i (obj) %i\n",
497                 _outstanding_mallocs[0],
498                 _outstanding_mallocs[1]);
499     }
500 }
501
502 static PyObject *
503 CudaNdarray_new(PyTypeObject *type, PyObject *args, PyObject *kwds)
504 {
505     CudaNdarray *self;
506
507     self = (CudaNdarray *)type->tp_alloc(type, 0);
508     if (self != NULL)
509     {
510         CudaNdarray_null_init(self);
511         ++_outstanding_mallocs[1];
512     }
513     return (PyObject *)self;
514 }
515 static int
516 CudaNdarray_init(CudaNdarray *self, PyObject *args, PyObject *kwds)
517 {
518     PyObject *arr=NULL;
519
520     if (! PyArg_ParseTuple(args, "O", &arr))
521         return -1;
522     if (! PyArray_Check(arr))
523     {
524         PyErr_SetString(PyExc_TypeError, "PyArray arg required");
525         return -1;
526     }
527     int rval = CudaNdarray_CopyFromArray(self, (PyArrayObject*)arr);
528     return rval;
529 }
530 static PyMemberDef CudaNdarray_members[] =
531 {
532     /*
533     {"first", T_OBJECT_EX, offsetof(CudaNdarray, first), 0,
534      "first name"},
535     {"last", T_OBJECT_EX, offsetof(CudaNdarray, last), 0,
536      "last name"},
537     {"number", T_INT, offsetof(CudaNdarray, number), 0,
538      "noddy number"},
539      */
540     {NULL}  /* Sentinel */
541 };
542
543 PyObject * CudaNdarray_CreateArrayObj(CudaNdarray * self, PyObject *args)
544 {
545     PyObject * dtype = NULL;
546     if (args && !PyArg_ParseTuple(args, "|O", &dtype))
547         return NULL;
548     if (dtype) {
549         PyArray_Descr* dtype2;
550         // PyArray_DescrConverter try to convert anything to a PyArray_Descr.
551         if(!PyArray_DescrConverter(dtype, &dtype2))
552         {
553             PyObject * str = PyObject_Repr(dtype);
554             PyErr_Format(PyExc_TypeError,
555                          "CudaNdarray dtype parameter not understood: %s",
556                          PyString_AsString(str)
557                          );
558             Py_CLEAR(str);
559             return NULL;
560         }
561         int typeNum = dtype2->type_num;
562         Py_DECREF(dtype2);
563         if (typeNum != NPY_FLOAT32)
564         {
565             PyObject * str = PyObject_Repr(dtype);
566             PyErr_Format(PyExc_TypeError,
567                          "CudaNdarray support only support float32 dtype, provided: %d",
568                          typeNum
569                          );
570             Py_CLEAR(str);
571             return NULL;
572         }
573     }
574
575     int verbose = 0;
576     if(self->nd>=0 && CudaNdarray_SIZE(self)==0){
577         npy_intp * npydims = (npy_intp*)malloc(self->nd * sizeof(npy_intp));
578         assert (npydims);
579         for (int i = 0; i < self->nd; ++i) npydims[i] = (npy_intp)(CudaNdarray_HOST_DIMS(self)[i]);
580         PyObject * rval = PyArray_SimpleNew(self->nd, npydims, REAL_TYPENUM);
581         free(npydims);
582         if (!rval){
583             return NULL;
584         }
585         assert (PyArray_ITEMSIZE((PyArrayObject *)rval) == sizeof(real));
586         return rval;
587     }
588     if ((self->nd < 0) || (self->devdata == 0))
589     {
590         PyErr_SetString(PyExc_ValueError, "can't copy from un-initialized CudaNdarray");
591         return NULL;
592     }
593     CudaNdarray * contiguous_self = NULL;
594     if (CudaNdarray_is_c_contiguous(self))
595     {
596         contiguous_self = self;
597         Py_INCREF(contiguous_self);
598         if (verbose) std::cerr << "CreateArrayObj already contiguous" << contiguous_self << '\n';
599     }
600     else
601     {
602         contiguous_self = (CudaNdarray*)CudaNdarray_Copy(self);
603         if (verbose) std::cerr << "CreateArrayObj created contiguous" << contiguous_self << '\n';
604     }
605     if (!contiguous_self)
606     {
607         return NULL;
608     }
609
610     npy_intp * npydims = (npy_intp*)malloc(self->nd * sizeof(npy_intp));
611     assert (npydims);
612     for (int i = 0; i < self->nd; ++i)
613         npydims[i] = (npy_intp)(CudaNdarray_HOST_DIMS(self)[i]);
614     PyArrayObject * rval = (PyArrayObject *) PyArray_SimpleNew(self->nd,
615                                                                npydims,
616                                                                REAL_TYPENUM);
617     free(npydims);
618     if (!rval)
619     {
620         Py_DECREF(contiguous_self);
621         return NULL;
622     }
623
624     assert (PyArray_ITEMSIZE(rval) == sizeof(real));
625
626     npy_intp rval_size = PyArray_SIZE(rval);
627     void *rval_data = PyArray_DATA(rval);
628     cudaError_t err;
629     CNDA_BEGIN_ALLOW_THREADS;
630
631     err = cudaMemcpy(rval_data, contiguous_self->devdata,
632                      rval_size * sizeof(real),
633                      cudaMemcpyDeviceToHost
634                      );
635     //CNDA_THREAD_SYNC;  // unneeded because cudaMemcpy is blocking anyway
636     CNDA_END_ALLOW_THREADS;
637
638     if (cudaSuccess != err)
639     {
640         PyErr_Format(PyExc_RuntimeError, "error (%s)copying data to host",
641                      cudaGetErrorString(err));
642         Py_DECREF(rval);
643         rval = NULL;
644     }
645
646     Py_DECREF(contiguous_self);
647     return (PyObject *)rval;
648 }
649
650 // TODO-- we have two functions here, ZEROS and Zeros.
651 // ZEROS is meant to be called just from C code (you don't need to pass it PyObject * s)
652 // but this naming is very weird, makes it look like a macro
653 // we should figure out the correct convention and change to that
654 PyObject* CudaNdarray_ZEROS(int n, int * dims)
655 {
656
657     size_t total_elements = 1;
658
659     for(size_t i=0;i<n;i++){
660         // Detect overflow on unsigned integer
661         if (dims[i] != 0 && total_elements > (SIZE_MAX / dims[i])) {
662             PyErr_Format(PyExc_RuntimeError,
663                          "Can't store in size_t for the bytes requested %llu * %llu",
664                          (unsigned long long)total_elements,
665                          (unsigned long long)dims[i]);
666             return NULL;
667         }
668         total_elements*=dims[i];
669     }
670
671     // total_elements now contains the size of the array, in reals
672     if (total_elements > (SIZE_MAX / sizeof(real))){
673         PyErr_Format(PyExc_RuntimeError,
674                      "Can't store in size_t for the bytes requested %llu * 4",
675                      (unsigned long long)total_elements);
676         return NULL;
677     }
678     size_t total_size = total_elements * sizeof(real);
679
680     CudaNdarray* rval = (CudaNdarray*)CudaNdarray_New();
681     if (!rval)
682     {
683         PyErr_SetString(PyExc_RuntimeError, "CudaNdarray_ZEROS: call to New failed");
684         return NULL;
685     }
686
687     if (CudaNdarray_alloc_contiguous(rval, n, dims))
688     {
689         PyErr_SetString(PyExc_RuntimeError, "CudaNdarray_ZEROS: allocation failed.");
690         Py_DECREF(rval);
691         return NULL;
692     }
693
694     // Fill with zeros
695     //fprintf(stdout, "Sizeof: %d\n", total_size);
696     if (cudaSuccess != cudaMemset(rval->devdata, 0, total_size))
697     {
698         PyErr_Format(PyExc_MemoryError,
699                      "CudaNdarray_ZEROS: Error memsetting %llu bytes of device memory.",
700                      (unsigned long long)total_size);
701         Py_DECREF(rval);
702         return NULL;
703     }
704
705     if (cnda_copy_structure_to_device(rval))
706     {
707         PyErr_SetString(PyExc_RuntimeError, "CudaNdarray_ZEROS: syncing structure to device failed");
708         Py_DECREF(rval);
709         return NULL;
710     }
711     return (PyObject*) rval;
712 }
713
714 // declared as a static method (hence 1st parameter is not used)
715 // Based on _Copy and _dimshuffle
716 PyObject* CudaNdarray_Zeros(PyObject* _unused, PyObject* shape)
717 {
718     if(!shape)
719     {
720         PyErr_SetString(PyExc_TypeError, "CudaNdarray_Zeros: function takes at least 1 argument (0 given)");
721         return NULL;
722     }
723     if(!PySequence_Check(shape))
724     {
725         PyErr_SetString(PyExc_TypeError, "shape argument must be a sequence");
726         return NULL;
727     }
728
729     int shplen = PySequence_Length(shape);
730
731     if (shplen == 0)
732     {
733         return CudaNdarray_ZEROS(0, NULL);
734     }
735
736     int* newdims = (int *)malloc(sizeof(int) * shplen);
737
738     if (!newdims)
739     {
740         PyErr_SetString(PyExc_MemoryError,
741             "CudaNdarray_Zeros: Failed to allocate temporary space");
742         return NULL;
743     }
744
745     // start from the end to compute strides
746     for (int i = shplen-1; i >= 0; --i)
747     {
748         PyObject* shp_el_obj = PySequence_GetItem(shape, i);
749         if(shp_el_obj == NULL)
750         {
751             // shouldn't happen since we checked length before...
752             PyErr_SetString(PyExc_RuntimeError, "CudaNdarray_Zeros: Index out of bound in sequence");
753             free(newdims);
754             return NULL;
755         }
756
757         int shp_el = PyInt_AsLong(shp_el_obj);
758         Py_DECREF(shp_el_obj);
759
760         if (shp_el < 0)
761         {
762             PyErr_SetString(PyExc_ValueError, "CudaNdarray_Zeros: shape must contain only non-negative values for size of a dimension");
763             free(newdims);
764             return NULL;
765         }
766
767         newdims[i] = shp_el;
768     }
769
770     PyObject* rval = CudaNdarray_ZEROS(shplen,newdims);
771
772     free(newdims);
773
774     return (PyObject*)rval;
775 }
776
777
778
779
780
781 PyObject * CudaNdarray_Copy(const CudaNdarray * self)
782 {
783     PyObject * rval = CudaNdarray_New();
784     if ((!rval) || (-1 == self->nd))
785     {
786         return rval;
787     }
788     if (CudaNdarray_alloc_contiguous((CudaNdarray*)rval, self->nd, CudaNdarray_HOST_DIMS(self)))
789     {
790         Py_DECREF(rval);
791         return NULL;
792     }
793     if (CudaNdarray_CopyFromCudaNdarray((CudaNdarray*)rval, self))
794     {
795         Py_DECREF(rval);
796         return NULL;
797     }
798     return rval;
799 }
800 PyObject * CudaNdarray_DeepCopy(CudaNdarray * self, PyObject * memo)
801 {
802     assert(PyDict_Check(memo));
803     PyObject * selfkey = PyInt_FromLong((long)self);
804     assert(selfkey);
805     if (PyDict_Contains(memo, selfkey))
806     {
807         PyObject * rval = PyDict_GetItem(memo, selfkey);
808         Py_DECREF(selfkey);
809         Py_XINCREF(rval);
810         return rval;
811     }
812     else
813     {
814         PyObject * rval = CudaNdarray_Copy(self);
815         if (0) std::cerr << "DeepCopy created " << rval << " devdata " << ((CudaNdarray*)rval)->devdata << "\n";
816         if (NULL == rval)
817         {
818             Py_DECREF(selfkey);
819             return NULL;
820         }
821         if (PyDict_SetItem(memo, selfkey, rval))
822         {
823             Py_DECREF(rval);
824             Py_DECREF(selfkey);
825             return NULL;
826         }
827         Py_DECREF(selfkey);
828         return rval;
829     }
830 }
831 PyObject * CudaNdarray_ReduceSum(CudaNdarray * self, PyObject * py_reduce_mask)
832 {
833     if (!PySequence_Check(py_reduce_mask))
834     {
835         PyErr_SetString(PyExc_TypeError, "reduce_mask must be sequence of ints");
836         return NULL;
837     }
838     int len = PySequence_Length(py_reduce_mask);
839     if (len != self->nd)
840     {
841         PyErr_SetString(PyExc_TypeError, "length of reduce_mask must match self->nd");
842         return NULL;
843     }
844     CudaNdarray * self_sum = (CudaNdarray*)CudaNdarray_New();
845     if (!self_sum)
846     {
847         return NULL;
848     }
849     //TODO: allocate a fixed size dimshuffle_pattern_cache on the stack,
850     //      and use it if it is big enough.
851     int * dimshuffle_pattern = (int*)malloc(len * 2 * sizeof(int));
852     int * sum_dims = dimshuffle_pattern + len;
853     int n_remaining_dims = 0;
854     if (!dimshuffle_pattern)
855     {
856         Py_DECREF(self_sum);
857         PyErr_SetString(PyExc_MemoryError, "failed to alloc internal storage");
858         return NULL;
859     }
860     for (int i = 0; i < len; ++i)
861     {
862         PyObject *o_i = PySequence_GetItem(py_reduce_mask, i);
863         int o_i_int = PyInt_AsLong(o_i);
864         Py_XDECREF(o_i);
865         if (PyErr_Occurred())
866         {
867             Py_DECREF(self_sum);
868             free(dimshuffle_pattern);
869             return NULL;
870         }
871         if (o_i_int) // this is a dimension over which we are reducing
872         {
873             sum_dims[i] = 1;
874         }
875         else
876         {
877             sum_dims[i] = CudaNdarray_HOST_DIMS(self)[i];
878             dimshuffle_pattern[n_remaining_dims++] = i;
879         }
880     }
881     if (0   || CudaNdarray_alloc_contiguous(self_sum, len, sum_dims)
882             || CudaNdarray_reduce_sum(self_sum, self)
883             || CudaNdarray_dimshuffle(self_sum, n_remaining_dims, dimshuffle_pattern))
884     {
885         Py_DECREF(self_sum);
886         free(dimshuffle_pattern);
887         return NULL;
888     }
889     free(dimshuffle_pattern);
890     return (PyObject*)self_sum;
891 }
892
893 // Reshape self to the new shape gived by the tuple shape.
894 //
895 // If self is c contiguous, it return a view. Otherwise it always do a copy.
896 // TODO: make it return a view when the strides allow it even if it is not
897 //       c contiguous
898 PyObject * CudaNdarray_Reshape(CudaNdarray * self, PyObject * shape)
899 {
900     if(!CudaNdarray_is_c_contiguous(self))
901     {
902         // allocate new space
903         //TODO: test to see if we can re-use old one and take a new param to
904         //  use this
905         CudaNdarray* rval = (CudaNdarray*) CudaNdarray_Copy(self);
906         if (!rval)
907         {
908             return NULL;
909         }
910
911         CudaNdarray* ret = (CudaNdarray*) CudaNdarray_Reshape(rval, shape);
912         Py_XDECREF(rval);
913         return (PyObject*)ret;
914     }
915
916     // check shape tuple
917     unsigned int rval_nd;
918     unsigned int * rval_dims;
919     size_t rval_size = 1;
920
921     if (PyTuple_Check(shape)){
922         // copy shape to integer array
923         rval_nd = PyTuple_Size(shape);
924     }else if (PyInt_Check(shape)){
925         rval_nd = 1;
926     }else{
927         PyErr_SetString(PyExc_TypeError, "shape must be tuple of integers or an integer");
928         return NULL;
929     }
930     rval_dims = (unsigned int*)malloc(rval_nd * sizeof(int));
931
932     if(PyTuple_Check(shape)){
933         for (int i = 0; i < rval_nd; ++i)
934         {
935             rval_dims[i] = PyInt_AsLong(PyTuple_GetItem(shape, i)); //GetItem returns borrowed reference
936             if (PyErr_Occurred()) //error in AsLong
937             {
938                 free(rval_dims);
939                 return NULL;
940             }
941             if(rval_dims[i]<0){
942                 PyErr_Format(PyExc_ValueError, "Reshape has invalid dimension %i (must be >=0)",rval_dims[i]);
943                 free(rval_dims);
944                 return NULL;
945             }
946             rval_size = rval_size * rval_dims[i];
947         }
948     }else{
949         rval_size = PyInt_AsLong(shape);
950         rval_dims[0] = rval_size;
951     }
952     // calculate new size, assert same as old size
953     if (rval_size != CudaNdarray_SIZE(self))
954     {
955         PyErr_Format(PyExc_ValueError, "size must remain unchanged, changed from %lld to %lld", CudaNdarray_SIZE(self), rval_size);
956         free(rval_dims);
957         return NULL;
958     }
959     if (rval_size==0)
960     {
961         PyObject * rval = CudaNdarray_NewDims(rval_nd, rval_dims);
962         free(rval_dims);
963         return rval;
964     }
965
966     //return a view, not a copy
967     //we can do this as we checked self is c_contiguous
968     CudaNdarray * rval = (CudaNdarray * )CudaNdarray_New(rval_nd);
969
970     if (!rval || 0 != rval->data_allocated
971         ||CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))
972     {
973         Py_XDECREF(rval);
974         free(rval_dims);
975         return NULL;
976     }
977     //set dim and stride
978     int size = 1;
979     for (int i = rval_nd-1; i >= 0; --i)
980     {
981         CudaNdarray_set_stride(rval, i, (rval_dims[i] == 1) ? 0 : size);
982         CudaNdarray_set_dim(rval, i, rval_dims[i]);
983         size = size * rval_dims[i];
984     }
985     free(rval_dims);
986     return (PyObject*)rval;
987 }
988
989 PyObject * CudaNdarray_View(const CudaNdarray * self)
990 {
991     CudaNdarray * rval = (CudaNdarray*)CudaNdarray_New(self->nd);
992     if (!rval || CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))
993     {
994         Py_XDECREF(rval);
995         rval = NULL;
996     }
997     else
998     {
999         for (int i = 0; i < self->nd; ++i)
1000         {
1001             CudaNdarray_set_dim(rval, i, CudaNdarray_HOST_DIMS(self)[i]);
1002             CudaNdarray_set_stride(rval, i, CudaNdarray_HOST_STRIDES(self)[i]);
1003         }
1004     }
1005     return (PyObject*)rval;
1006 }
1007
1008 /*
1009  * d0,... are the output dims
1010  * indices are a list of index to operate on
1011  *         They are int32 viewed as float32.
1012  * a is the output
1013  * b is the input
1014  * dB0, the source leading dimensions size
1015  */
1016 template <int operator_num>
1017 __global__ void k_take_3(const int d0, const int d1, const int d2,
1018                          const npy_int64* indices,
1019                          float* a,
1020                          const int sA0, const int sA1, const int sA2,
1021                          const float* b, const int dB0,
1022                          const int sB0, const int sB1, const int sB2,
1023                          int* err){
1024     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){
1025         npy_int64 idx = indices[i0];
1026         if (idx<0)
1027             idx += dB0; // To allow negative indexing.
1028         if ((idx < 0) || (idx >= dB0)){
1029             // Any value other the 0 probably work. But to be more safe, I want
1030             // to change all bits to prevent problem with concurrent write that
1031             // could cross cache line. But this should not happen with the
1032             // current code and driver.
1033             *err = 0xFFFF;
1034             continue;
1035         }
1036         for (int i1 = threadIdx.x; i1 < d1; i1 += blockDim.x){
1037             for (int i2 = threadIdx.y; i2 < d2; i2 += blockDim.y){
1038                 int a_idx = i0*sA0 + i1*sA1 + i2*sA2;
1039                 int b_idx = idx*sB0 + i1*sB1 + i2*sB2;
1040                 a[a_idx] = b[b_idx];
1041             }
1042         }
1043     }
1044 }
1045
1046 // We try to be similar to the PyArray_TakeFrom function
1047 //http://docs.scipy.org/doc/numpy/reference/c-api.array.html
1048 //TODO: support other clip mode then raise(clip, wrap)
1049 //self is the input that we copy data from.
1050 //The indices that we receive MUST be an CudaNdarray(float32)
1051 //    that is in fact a view to int64 indices
1052 PyObject*
1053 CudaNdarray_TakeFrom(CudaNdarray * self, PyObject *args){
1054     int verbose = 0;
1055     PyObject * indices_obj = NULL;
1056     //int axis; Default None, that mean the flattened array.
1057     PyObject * axis_obj = Py_None;
1058     PyObject * out_obj = Py_None;
1059     PyObject * clipmode_obj = NULL;
1060     int max_threads = 1; // max threads per blocks
1061
1062     if (! PyArg_ParseTuple(args, "O|OOOi", &indices_obj, &axis_obj,
1063                            &out_obj, &clipmode_obj, &max_threads))
1064         return NULL;
1065
1066     //Check argument indices
1067     //TODO: if not a numpy.ndarray, convert to numpy.ndarray
1068     //TODO: If a CudaNdarray, accept it and suppose the data is int32? is float32 number of int?
1069     //TODO: Support ndarray of other dtype then int32
1070     //TODO: support list of indices that are not c_contiguous
1071     CudaNdarray * indices = NULL;
1072     if (CudaNdarray_Check(indices_obj)) {
1073         if (verbose) printf("cudandarray indices\n");
1074         indices = (CudaNdarray*) indices_obj;
1075         Py_INCREF(indices);
1076     } else if (PyArray_Check(indices_obj)) {
1077         if (verbose) printf("ndarray indices\n");
1078         if (PyArray_TYPE((PyArrayObject *)indices_obj) != NPY_INT64) {
1079             PyErr_SetString(PyExc_TypeError,
1080                             "CudaNdarray_TakeFrom: need a ndarray for indices"
1081                             " with dtype int64");
1082             return NULL;
1083         }
1084         if (PyArray_NDIM(((PyArrayObject*)indices_obj)) != 1) {
1085             PyErr_SetString(PyExc_TypeError,
1086                             "CudaNdarray_TakeFrom: need a CudaNdarray of"
1087                             " indices with only 1 dimensions");
1088             return NULL;
1089         }
1090         // We need indices_obj to be contiguous, in order to take a view
1091         // with a different dtype.
1092         if (!PyArray_IS_C_CONTIGUOUS((PyArrayObject*) indices_obj)) {
1093             PyObject* indices_obj_contig = PyArray_NewCopy((PyArrayObject*) indices_obj, NPY_CORDER);
1094             if (!indices_obj_contig)
1095                 return NULL;
1096             indices_obj = indices_obj_contig;
1097         } else {
1098             // Keep the refcount consistent
1099             Py_INCREF(indices_obj);
1100         }
1101         PyArray_Descr* float32_descr = PyArray_DescrFromType(NPY_FLOAT32);
1102         PyObject * indices_float32 = NULL;
1103         indices_float32 = PyArray_View((PyArrayObject*)indices_obj,
1104                                                   float32_descr, NULL);
1105         if (verbose) printf("ndarray indices\n");
1106         if (!indices_float32) {
1107             Py_DECREF(indices_obj);
1108             return NULL;
1109         }
1110
1111         indices = (CudaNdarray*) CudaNdarray_New();
1112         if (verbose) printf("\nndarray after new\n");
1113         if (! indices){
1114             Py_DECREF(indices_obj);
1115             Py_DECREF(indices_float32);
1116             return NULL;
1117         }
1118         if (CudaNdarray_CopyFromArray(indices,
1119                                       (PyArrayObject *)indices_float32)){
1120             Py_DECREF(indices_obj);
1121             Py_DECREF(indices_float32);
1122             return NULL;
1123         }
1124         Py_DECREF(indices_obj);
1125         Py_DECREF(indices_float32);
1126     } else {
1127         PyObject* py_s = PyObject_Str(indices_obj);
1128         const char* s = PyString_AsString(py_s);
1129         Py_DECREF(py_s);
1130         PyErr_Format(PyExc_TypeError,
1131                      "CudaNdarray_TakeFrom: need an ndarray of int64 or a"
1132                      " CudaNdarray(float32) that is a view from int64 data"
1133                      " for indices. Got %s", s);
1134         return NULL;
1135     }
1136
1137     if (verbose) {
1138         printf("indices used on the gpu\n");
1139         fprint_CudaNdarray(stdout, indices);
1140         PyObject * used_indices = CudaNdarray_CreateArrayObj(indices);
1141         PyObject_Print(used_indices, stdout, 0);
1142         Py_DECREF(used_indices);
1143     }
1144     if (verbose) printf("after print of object\n");
1145     if(!CudaNdarray_is_c_contiguous(indices) != 0) {
1146         PyErr_SetString(PyExc_NotImplementedError,
1147                         "CudaNdarray_TakeFrom: The indices must be contiguous in memory.");
1148         Py_DECREF(indices);
1149         return NULL;
1150     }
1151     int nb_indices = CudaNdarray_SIZE((CudaNdarray *)indices) / 2;// int64 are 8 bytes, float32 are 4 bytes
1152
1153     //Check argument axis
1154     //TODO: implement the default and other axis
1155     long axis = PyInt_AsLong(axis_obj);
1156
1157     if (axis != 0) {
1158         PyErr_Format(PyExc_NotImplementedError,
1159                      "CudaNdarray_TakeFrom: only axis=0 is currently supported."
1160                      " Got %ld.", axis);
1161         Py_DECREF(indices);
1162         return NULL;
1163     }
1164
1165     //Check argument out_obj
1166     CudaNdarray * out = NULL;
1167     if (out_obj && CudaNdarray_Check(out_obj))
1168         out = (CudaNdarray*) out_obj;
1169     if (out && (out->nd != self->nd ||
1170                 CudaNdarray_HOST_DIMS(out)[0] != nb_indices))
1171         out = NULL;
1172     int * dims = (int *)malloc(sizeof(int) * self->nd);
1173     dims[0] = nb_indices;
1174
1175     for (int i=1 ; i<self->nd ; i++) {
1176         dims[i] = CudaNdarray_HOST_DIMS(self)[i];
1177         if (out && CudaNdarray_HOST_DIMS(out)[i] != dims[i]) {
1178             out = NULL;
1179         }
1180     }
1181     if (!out) {
1182         out = (CudaNdarray*)CudaNdarray_New();
1183         if (!out){
1184             Py_DECREF(indices);
1185             free(dims);
1186             return NULL;
1187         }
1188         if (CudaNdarray_alloc_contiguous(out, self->nd, dims)) {
1189             Py_DECREF(out);
1190             Py_DECREF(indices);
1191             free(dims);
1192             return NULL;
1193         }
1194     }else {
1195         Py_INCREF(out);
1196     }
1197
1198     //Check argument clipmode
1199     if (clipmode_obj) {
1200         char * clipmode = PyString_AsString(clipmode_obj);
1201         if (! clipmode){
1202             Py_DECREF(indices);
1203             Py_DECREF(out);
1204             free(dims);
1205             return NULL;
1206         }
1207         if (strcmp(clipmode, "raise") != 0) {
1208             PyErr_Format(PyExc_NotImplementedError,
1209                          "CudaNdarray_TakeFrom: only the raise mode is currently supported. Got '%s'",
1210                          clipmode);
1211             Py_DECREF(indices);
1212             Py_DECREF(out);
1213             free(dims);
1214             return NULL;
1215         }
1216     }
1217     void (*k3)(const int, const int, const int,
1218                const npy_int64*,
1219                float*, const int, const int, const int,
1220                const float*, const int,
1221                const int, const int, const int,
1222                int*);
1223     k3 = k_take_3<CPY>;
1224
1225     // Create the memory place that will store the error information.
1226     if(init_err_var() != 0) return NULL;
1227
1228     dim3 n_blocks(std::min(CudaNdarray_HOST_DIMS(out)[0],65535),1,1);
1229     if(CudaNdarray_HOST_DIMS(out)[0] == 0){
1230         // We take 0 elements, so no need for the rest of the code.
1231         // This speed up that case AND fix crash otherwise.
1232         free(dims);
1233         Py_DECREF(indices);
1234         return (PyObject *)out;
1235     }
1236
1237     switch (self->nd) {
1238         case 1:
1239             {
1240                 dim3 n_threads(1, 1, 1);
1241                 if (verbose)
1242                     printf("cudaGetLastError=%d, nd=%d"
1243                            " kernel config: (n_blocks.x=%d, n_blocks.y=%d,"
1244                            " n_threads.x=%i, n_threads.y=%i)\n",
1245                            cudaGetLastError(), self->nd,
1246                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);
1247                 k3<<<n_blocks, n_threads>>>(
1248                         dims[0],
1249                         1,
1250                         1,
1251                         (npy_int64*) CudaNdarray_DEV_DATA(indices),
1252                         CudaNdarray_DEV_DATA(out),
1253                         CudaNdarray_HOST_STRIDES(out)[0], //strides
1254                         1,
1255                         1,
1256                         CudaNdarray_DEV_DATA(self),
1257                         CudaNdarray_HOST_DIMS(self)[0], //For indices check
1258                         CudaNdarray_HOST_STRIDES(self)[0], //strides
1259                         1,
1260                         1,
1261                         err_var);
1262             }
1263             break;
1264         case 2:
1265             {
1266                 dim3 n_threads(std::min(CudaNdarray_HOST_DIMS(out)[1], max_threads), 1, 1);
1267
1268                 if (verbose)
1269                     printf("cudaGetLastError=%d, nd=%d"
1270                            " kernel config: (n_blocks.x=%d, n_blocks.y=%d,"
1271                            " n_threads.x=%i, n_threads.y=%i)\n",
1272                            cudaGetLastError(), self->nd,
1273                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);
1274
1275                 k3<<<n_blocks, n_threads>>>(
1276                         dims[0], //dimensions
1277                         dims[1],
1278                         1,
1279                         (npy_int64*) CudaNdarray_DEV_DATA(indices),
1280                         CudaNdarray_DEV_DATA(out),
1281                         CudaNdarray_HOST_STRIDES(out)[0], //strides
1282                         CudaNdarray_HOST_STRIDES(out)[1],
1283                         1,
1284                         CudaNdarray_DEV_DATA(self),
1285                         CudaNdarray_HOST_DIMS(self)[0], //For indices check
1286                         CudaNdarray_HOST_STRIDES(self)[0], //strides
1287                         CudaNdarray_HOST_STRIDES(self)[1],
1288                         1,
1289                         err_var);
1290             }
1291             break;
1292         case 3:
1293             {
1294                 int ty = std::min(CudaNdarray_HOST_DIMS(out)[2], max_threads);
1295                 int tx = std::min(CudaNdarray_HOST_DIMS(out)[1], max_threads / ty);
1296                 dim3 n_threads(tx, ty, 1);
1297                 if (verbose)
1298                     printf("cudaGetLastError=%d, nd=%d"
1299                            " kernel config: (n_blocks.x=%d, n_blocks.y=%d,"
1300                            " n_threads.x=%i, n_threads.y=%i)\n",
1301                            cudaGetLastError(), self->nd,
1302                            n_blocks.x, n_blocks.y, n_threads.x, n_threads.y);
1303                 k3<<<n_blocks, n_threads>>>(
1304                         dims[0], //dimensions
1305                         dims[1],
1306                         dims[2],
1307                         (npy_int64*) CudaNdarray_DEV_DATA(indices),
1308                         CudaNdarray_DEV_DATA(out),
1309                         CudaNdarray_HOST_STRIDES(out)[0], //strides
1310                         CudaNdarray_HOST_STRIDES(out)[1],
1311                         CudaNdarray_HOST_STRIDES(out)[2],
1312                         CudaNdarray_DEV_DATA(self),
1313                         CudaNdarray_HOST_DIMS(self)[0], //For indices check
1314                         CudaNdarray_HOST_STRIDES(self)[0], //strides
1315                         CudaNdarray_HOST_STRIDES(self)[1],
1316                         CudaNdarray_HOST_STRIDES(self)[2],
1317                         err_var);
1318             }
1319             break;
1320     default:
1321         PyErr_SetString(PyExc_NotImplementedError,
1322                         "CudaNdarray_TakeFrom: only input with 1, 2 or 3"
1323                         " dimensions are currently supported");
1324
1325     }
1326     free(dims);
1327     CNDA_THREAD_SYNC;
1328     cudaError_t err = cudaGetLastError();
1329     if (cudaSuccess != err) {
1330         PyErr_Format(PyExc_RuntimeError,
1331                      "Cuda error: %s: %s.\n",
1332                      "CudaNdarray_TakeFrom",
1333                      cudaGetErrorString(err));
1334         Py_DECREF(indices);
1335         Py_DECREF(out);
1336         return NULL;
1337     }
1338
1339     int index_err = check_err_var();
1340     Py_DECREF(indices);
1341     if (index_err != 0) {
1342         Py_DECREF(out);
1343         return NULL;
1344     }
1345
1346     if (verbose) printf("TAKE SUCCEDED\n");
1347     return (PyObject *)out;
1348 }
1349
1350
1351 PyObject * CudaNdarray_SetStride(CudaNdarray * self, PyObject *args)
1352 {
1353     int pos, stride;
1354     if (! PyArg_ParseTuple(args, "ii", &pos, &stride))
1355         return NULL;
1356     if ((pos < 0) || (pos >= self->nd))
1357     {
1358         PyErr_Format(PyExc_ValueError, "position argument out of legal range [0, %i)", self->nd);
1359         return NULL;
1360     }
1361     CudaNdarray_set_stride(self, pos, stride);
1362     if (cnda_copy_structure_to_device(self))
1363     {
1364         return NULL;
1365     }
1366     Py_INCREF(Py_None);
1367     return Py_None;
1368 }
1369 PyObject * CudaNdarray_SetShapeI(CudaNdarray * self, PyObject *args)
1370 {
1371     int pos, dim;
1372     if (! PyArg_ParseTuple(args, "ii", &pos, &dim))
1373         return NULL;
1374     if ((pos < 0) || (pos >= self->nd))
1375     {
1376         PyErr_Format(PyExc_ValueError, "position argument out of legal range [0, %i)", self->nd);
1377         return NULL;
1378     }
1379     CudaNdarray_set_dim(self, pos, dim);
1380     if (cnda_copy_structure_to_device(self))
1381     {
1382         return NULL;
1383     }
1384     Py_INCREF(Py_None);
1385     return Py_None;
1386 }
1387
1388 static PyObject *
1389 CudaNdarray_exp(CudaNdarray* self)
1390 {
1391     CudaNdarray * rval = (CudaNdarray *)CudaNdarray_New();
1392     if ((NULL == rval) || CudaNdarray_alloc_contiguous(rval, self->nd, CudaNdarray_HOST_DIMS(self)))
1393     {
1394         Py_XDECREF(rval);
1395         return NULL;
1396     }
1397     unsigned int size = 1;
1398     for (int i = 0; i < self->nd; i++)
1399     {
1400         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];
1401     }
1402     unsigned int threads_per_block = std::min(size, (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);
1403     unsigned int n_blocks = std::min(ceil_intdiv(size,threads_per_block), (unsigned int)NUM_VECTOR_OP_BLOCKS);
1404     k_elemwise_unary_rowmajor_exp<<<n_blocks,threads_per_block>>>(size, self->nd, CudaNdarray_DEV_DIMS(self),
1405             CudaNdarray_DEV_DATA(self), CudaNdarray_DEV_STRIDES(self),
1406             CudaNdarray_DEV_DATA(rval), CudaNdarray_DEV_STRIDES(rval));
1407
1408     //TODO: don't do this right away, do it when we need the result
1409     CNDA_THREAD_SYNC;
1410     cudaError_t err = cudaGetLastError();
1411     if( cudaSuccess != err)
1412     {
1413         Py_DECREF(rval);
1414         PyErr_Format(PyExc_RuntimeError, "Cuda error: %s: %s.\n", "kExp", cudaGetErrorString(err));
1415         return NULL;
1416     }
1417
1418     return (PyObject*)rval;
1419 }
1420
1421 static PyMethodDef CudaNdarray_methods[] =
1422 {
1423     {"__array__",
1424         (PyCFunction)CudaNdarray_CreateArrayObj, METH_VARARGS,
1425         "Copy from the device to a numpy ndarray"},
1426     {"__copy__",
1427         (PyCFunction)CudaNdarray_View, METH_NOARGS,
1428         "Create a shallow copy of this object. used by module copy"},
1429     {"__deepcopy__",
1430         (PyCFunction)CudaNdarray_DeepCopy, METH_O,
1431         "Create a copy of this object"},
1432     {"zeros",
1433         (PyCFunction)CudaNdarray_Zeros, METH_STATIC | METH_O,
1434         "Create a new CudaNdarray with specified shape, filled with zeros."},
1435     {"copy",
1436         (PyCFunction)CudaNdarray_Copy, METH_NOARGS,
1437         "Create a copy of this object"},
1438     {"is_c_contiguous",
1439         (PyCFunction)CudaNdarray_IS_C_Contiguous, METH_NOARGS,
1440         "Return True is the object is c contiguous. False otherwise."},
1441     {"reduce_sum",
1442         (PyCFunction)CudaNdarray_ReduceSum, METH_O,
1443         "Reduce over the given dimensions by summation"},
1444     {"exp",
1445         (PyCFunction)CudaNdarray_exp, METH_NOARGS,
1446         "Return the exponential of all elements"},
1447     {"reshape",
1448         (PyCFunction)CudaNdarray_Reshape, METH_O,
1449         "Return a reshaped view (or copy) of this ndarray\n\
1450             The required argument is a tuple of integers specifying the shape of the new ndarray."},
1451     {"view",
1452         (PyCFunction)CudaNdarray_View, METH_NOARGS,
1453         "Return an alias of this ndarray"},
1454     {"_set_stride",
1455         (PyCFunction)CudaNdarray_SetStride, METH_VARARGS,
1456         "For integer arguments (i, s), set the 'i'th stride to 's'"},
1457     {"take",
1458         (PyCFunction)CudaNdarray_TakeFrom, METH_VARARGS,
1459         "Equivalent of numpy.take"},
1460     {"_set_shape_i",
1461         (PyCFunction)CudaNdarray_SetShapeI, METH_VARARGS,
1462         "For integer arguments (i, s), set the 'i'th shape to 's'"},
1463     {NULL, NULL, NULL, NULL}  /* Sentinel */
1464 };
1465
1466
1467 ////////////////////
1468 // Number protocol
1469 ////////////////////
1470
1471 __global__ void kAdd_contiguous(float* a, float* b, float* dest, unsigned int numEls) {
1472     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;
1473     const unsigned int numThreads = blockDim.x * gridDim.x;
1474
1475     for (unsigned int i = idx; i < numEls; i += numThreads) {
1476         dest[i] = a[i] + b[i];
1477     }
1478 }
1479
1480 // Will be called by __add__ in Python
1481 static PyObject *
1482 CudaNdarray_add(PyObject* py_self, PyObject * py_other)
1483 {
1484     if (! CudaNdarray_Check(py_self)) {
1485         PyErr_SetString(PyExc_TypeError, "need a CudaNdarray on left");
1486         return NULL;
1487     }
1488     if (! CudaNdarray_Check(py_other)) {
1489         PyErr_SetString(PyExc_TypeError, "need a CudaNdarray on right");
1490         return NULL;
1491     }
1492     CudaNdarray * self = (CudaNdarray *)py_self;
1493     CudaNdarray * other = (CudaNdarray *)py_other;
1494     if(!CudaNdarray_is_c_contiguous(self) || !CudaNdarray_is_c_contiguous(other)){
1495         PyErr_SetString(PyExc_TypeError, "We have implementet only the c_contiguous version for now.");
1496         return NULL;
1497     }
1498
1499     //standard elemwise size checks
1500     if (self->nd != other->nd)
1501     {
1502         PyErr_SetString(PyExc_TypeError, "CudaNdarray_add: need same number of dims");
1503         return NULL;
1504     }
1505     //standard elemwise dim checks
1506     unsigned int size = 1;
1507     for (int i = 0; i< self->nd; ++i)
1508     {
1509         if (CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(other)[i])
1510         {
1511             PyErr_SetString(PyExc_TypeError, "need same dimensions");
1512             return NULL;
1513         }
1514         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];
1515     }
1516     CudaNdarray * rval = (CudaNdarray *)CudaNdarray_New();
1517     if (!rval || CudaNdarray_alloc_contiguous(rval, self->nd, CudaNdarray_HOST_DIMS(self)))
1518     {
1519         Py_XDECREF(rval);
1520         return NULL;
1521     }
1522
1523     if(CudaNdarray_SIZE((CudaNdarray *)py_self)==0 && CudaNdarray_SIZE((CudaNdarray *)py_other)==0){
1524       return (PyObject *) rval;
1525     }
1526
1527     int threads_per_block = std::min(size, (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);
1528     int n_blocks = std::min(ceil_intdiv(size,(unsigned int)threads_per_block), (unsigned int)NUM_VECTOR_OP_BLOCKS);
1529     kAdd_contiguous<<<n_blocks,threads_per_block>>>(
1530             self->devdata, other->devdata, rval->devdata, size);
1531     CNDA_THREAD_SYNC;
1532     cudaError_t err = cudaGetLastError();
1533     if( cudaSuccess != err)
1534     {
1535         PyErr_Format(PyExc_RuntimeError, "Cuda error: %s: %s.\n", "kAdd", cudaGetErrorString(err));
1536         Py_DECREF(rval);
1537         return NULL;
1538     }
1539     return (PyObject *) rval;
1540 }
1541
1542 template <int operator_num>
1543 __global__ void k_ielem_3(const int d0, const int d1, const int d2,
1544         float* a, const int sA0, const int sA1, const int sA2,
1545         const float* b, const int sB0, const int sB1, const int sB2){
1546     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){
1547         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){
1548             for (int i2 = threadIdx.x; i2 < d2; i2 += blockDim.x){
1549                 switch (operator_num)
1550                 {
1551                   case IADD:
1552                     a[i0*sA0 + i1*sA1 + i2*sA2] += b[i0*sB0 + i1*sB1 + i2*sB2];
1553                     break;
1554                   case IDIV:
1555                     a[i0*sA0 + i1*sA1 + i2*sA2] /= b[i0*sB0 + i1*sB1 + i2*sB2];
1556                     break;
1557                   case CPY:
1558                     a[i0*sA0 + i1*sA1 + i2*sA2] = b[i0*sB0 + i1*sB1 + i2*sB2];
1559                     break;
1560                 }
1561             }
1562         }
1563     }
1564 }
1565
1566 template <int operator_num>
1567 __global__ void k_ielem_4(const int d0, const int d1, const int d2, const int d3,
1568                          float* a, const int sA0, const int sA1,
1569                          const int sA2, const int sA3,
1570                          const float* b, const int sB0, const int sB1,
1571                          const int sB2, const int sB3){
1572     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){
1573         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){
1574             for (int i2 = threadIdx.x; i2 < d2; i2 += blockDim.x){
1575                 for (int i3 = threadIdx.y; i3 < d3; i3 += blockDim.y){
1576                     switch (operator_num) {
1577                         case IADD:
1578                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]
1579                             += b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];
1580                             break;
1581                         case IDIV:
1582                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]
1583                             /= b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];
1584                             break;
1585                         case CPY:
1586                             a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3]
1587                             = b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3];
1588                             break;
1589                     }
1590                 }
1591             }
1592         }
1593     }
1594 }
1595
1596 template <int operator_num>
1597 __global__ void k_ielem_6(const int d0, const int d1,
1598                           const int d2, const int d3,
1599                           const int d4, const int d5,
1600                           float* a, const int sA0, const int sA1,
1601                           const int sA2, const int sA3,
1602                           const int sA4, const int sA5,
1603                           const float* b, const int sB0, const int sB1,
1604                           const int sB2, const int sB3,
1605                           const int sB4, const int sB5
1606                           ){
1607     for (int i0 = blockIdx.x; i0 < d0; i0 += gridDim.x){
1608         for (int i1 = blockIdx.y; i1 < d1; i1 += gridDim.y){
1609             for (int i2 = blockIdx.z; i2 < d2; i2 += gridDim.z){
1610                 for (int i3 = threadIdx.x; i3 < d3; i3 += blockDim.x){
1611                     for (int i4 = threadIdx.y; i4 < d4; i4 += blockDim.y){
1612                         for (int i5 = threadIdx.z; i5 < d5; i5 += blockDim.z){
1613                             switch (operator_num) {
1614                             case IADD:
1615                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]
1616                                     += b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];
1617                                 break;
1618                             case IDIV:
1619                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]
1620                                     /= b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];
1621                                 break;
1622                             case CPY:
1623                                 a[i0*sA0 + i1*sA1 + i2*sA2 + i3*sA3 + i4*sA4 + i5*sA5]
1624                                     = b[i0*sB0 + i1*sB1 + i2*sB2 + i3*sB3 + i4*sB4 + i5*sB5];
1625                                 break;
1626                             }
1627                         }
1628                     }
1629                 }
1630             }
1631         }
1632     }
1633 }
1634
1635 /*
1636 CudaNdarray_inplace_elemwise
1637 Compute elemwise, working inplace on A.
1638 Currently implemented A / B, A + B and A = B
1639 (the last is not tested and not used!)
1640
1641 py_self - the CudaNdarray that we'll modify (A)
1642 py_other - the other argument (B)
1643 fct_nb - which operation to perform (operator_t)
1644
1645 Returns 0 on success.
1646 Returns -1 on failure, and sets Python exception.
1647
1648 */
1649 int
1650 CudaNdarray_inplace_elemwise(PyObject* py_self, PyObject * py_other, operator_t fct_nb)
1651 {
1652     int verbose = 0;
1653     void (*k3)(const int, const int, const int,
1654                     float*, const int, const int, const int,
1655                     const float*, const int, const int, const int);
1656     void (*k4)(const int, const int, const int, const int,
1657                     float*, const int, const int,
1658                     const int, const int,
1659                     const float*, const int, const int,
1660                     const int, const int);
1661     void (*k6)(const int, const int,
1662                const int, const int,
1663                const int, const int,
1664                float*, const int, const int,
1665                const int, const int,
1666                const int, const int,
1667                const float*, const int, const int,
1668                const int, const int,
1669                const int, const int);
1670     switch (fct_nb)
1671     {
1672         case IADD:
1673             k3 = k_ielem_3<IADD>;
1674             k4 = k_ielem_4<IADD>;
1675             k6 = k_ielem_6<IADD>;
1676             break;
1677         case IDIV:
1678             k3 = k_ielem_3<IDIV>;
1679             k4 = k_ielem_4<IDIV>;
1680             k6 = k_ielem_6<IDIV>;
1681             break;
1682         case CPY:
1683             k3 = k_ielem_3<CPY>;
1684             k4 = k_ielem_4<CPY>;
1685             k6 = k_ielem_6<CPY>;
1686             break;
1687         default:
1688             assert (0);
1689             PyErr_Format(
1690                 PyExc_TypeError,
1691                 "CudaNdarray_inplace_elemwise invalid fct_nb (%i).",
1692                 (int)fct_nb);
1693             return -1;
1694     }
1695     if (!CudaNdarray_Check(py_self)) {
1696         PyErr_SetString(
1697             PyExc_TypeError,
1698             "CudaNdarray_inplace_elemwise need a CudaNdarray on left");
1699         return -1;
1700     }
1701     CudaNdarray * new_other = NULL;
1702     if (!CudaNdarray_Check(py_other)) {
1703         new_other = (CudaNdarray*) CudaNdarray_New();
1704         if(!new_other)
1705         {
1706             return -1;
1707         }
1708         if(CudaNdarray_CopyFromArray(new_other, (PyArrayObject *) py_other))
1709         {
1710             Py_XDECREF(new_other);
1711             return -1;
1712         }
1713         py_other = (PyObject *) new_other;
1714     }
1715
1716     CudaNdarray * self = (CudaNdarray *)py_self;
1717     CudaNdarray * other = (CudaNdarray *)py_other;
1718
1719     if (verbose)
1720     {
1721         fprintf(stderr,
1722             "INPLACE ADD/DIV for self->nd=%d other->nd=%d\n",
1723             self->nd, other->nd);
1724     }
1725
1726     //standard elemwise nb dim checks
1727     if (self->nd < other->nd)
1728     {
1729         PyErr_Format(
1730             PyExc_TypeError,
1731             "CudaNdarray_inplace_elemwise: The destination need more or the"
1732             " same number of dimensions then the source. Got %d and %d.",
1733             self->nd, other->nd);
1734         Py_XDECREF(new_other);
1735         return -1;
1736     }
1737
1738     //broadcast to the same number of dimensions.
1739     int* other_dims = (int*) alloca(self->nd * sizeof(int));
1740     int* other_strides = (int*) alloca(self->nd * sizeof(int));
1741     int added_dims = self->nd - other->nd;
1742     // Add the added broadcasted dimensions
1743     for (int i = 0; i< added_dims; ++i)
1744     {
1745         other_dims[i] = 1;
1746         other_strides[i] = 0;
1747     }
1748     // Copy the existing dimensions
1749     for (int i = 0; i< other->nd; ++i)
1750     {
1751         other_dims[i+added_dims] = CudaNdarray_HOST_DIMS(other)[i];
1752         other_strides[i+added_dims] = CudaNdarray_HOST_STRIDES(other)[i];
1753     }
1754
1755     //standard elemwise dim checks
1756     unsigned int size = 1;
1757     for (int i = 0; i< self->nd; ++i)
1758     {
1759         if ((CudaNdarray_HOST_DIMS(self)[i] != other_dims[i])
1760             && (other_dims[i] != 1))
1761         {
1762             PyErr_SetString(
1763                 PyExc_ValueError,
1764                 "CudaNdarray_inplace_elemwise need same dimensions (or broadcastable dimension)");
1765             Py_XDECREF(new_other);
1766             return -1;
1767         }
1768         // if we're broadcasting other, then make sure it has stride 0
1769         assert ((CudaNdarray_HOST_DIMS(self)[i] == other_dims[i])
1770             || (other_strides[i] == 0));
1771         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];
1772     }
1773
1774     if (size==0)
1775     {
1776         int other_size = CudaNdarray_SIZE((CudaNdarray *)py_other);
1777         if (!(other_size == 0 || other_size == 1))
1778         {
1779             PyErr_SetString(
1780                 PyExc_ValueError,
1781                 "CudaNdarray_inplace_elemwise cannot work inplace on"
1782                 " un-initialized array when the new value have more than"
1783                 " 0 or 1 broadcastable dimensions");
1784             Py_XDECREF(new_other);
1785             return 0;
1786         }
1787         Py_XDECREF(new_other);
1788         return 0;
1789     }
1790
1791     switch(self->nd)
1792     {
1793         case 0:
1794             {
1795                 dim3 n_blocks(1, 1, 1);
1796                 dim3 n_threads(1);
1797                 k3<<<n_blocks, n_threads>>>(
1798                         1, //d0
1799                         1, //d1
1800                         1, //d2
1801                         CudaNdarray_DEV_DATA(self),
1802                         1, //strides
1803                         1,
1804                         1,
1805                         CudaNdarray_DEV_DATA(other),
1806                         1, //strides
1807                         1,
1808                         1);
1809                 CNDA_THREAD_SYNC;
1810                 cudaError_t err = cudaGetLastError();
1811                 if (cudaSuccess != err)
1812                 {
1813                     PyErr_Format(
1814                         PyExc_RuntimeError,
1815                         "CudaNdarray_inplace_elemwise case0: Cuda error: %s: %s.\n",
1816                         "k3",
1817                         cudaGetErrorString(err));
1818                     Py_XDECREF(new_other);
1819                     return -1;
1820                 }
1821             }
1822             break;
1823         case 1:
1824             {
1825                 dim3 n_blocks(1, 1, 1);
1826                 dim3 n_threads(
1827                         std::min(
1828                             CudaNdarray_HOST_DIMS(self)[0],
1829                             NUM_VECTOR_OP_THREADS_PER_BLOCK));
1830                 k3<<<n_blocks, n_threads>>>(
1831                         1, //dimensions
1832                         1,
1833                         CudaNdarray_HOST_DIMS(self)[0],
1834                         CudaNdarray_DEV_DATA(self),
1835                         1, //strides
1836                         1,
1837                         CudaNdarray_HOST_STRIDES(self)[0],
1838                         CudaNdarray_DEV_DATA(other),
1839                         1, //strides
1840                         1,
1841                         other_strides[0]);
1842                 CNDA_THREAD_SYNC;
1843                 cudaError_t err = cudaGetLastError();
1844                 if (cudaSuccess != err)
1845                 {
1846                     PyErr_Format(
1847                         PyExc_RuntimeError,
1848                         "CudaNdarray_inplace_elemwise case1: Cuda error: %s: %s.\n",
1849                         "k3",
1850                         cudaGetErrorString(err));
1851                     Py_XDECREF(new_other);
1852                     return -1;
1853                 }
1854             }
1855             break;
1856         case 2:
1857             {
1858                 //TODO:  if both self and other are f-contiguous
1859                 //       Then flip the block and thread dimensions
1860                 //       to make contiguous reads & writes
1861                 dim3 n_blocks(1,
1862                         std::min(
1863                             CudaNdarray_HOST_DIMS(self)[0],
1864                             NUM_VECTOR_OP_BLOCKS));
1865                 dim3 n_threads(
1866                         std::min(
1867                             CudaNdarray_HOST_DIMS(self)[1],
1868                             NUM_VECTOR_OP_THREADS_PER_BLOCK));
1869                 k3<<<n_blocks, n_threads>>>(1,
1870                         CudaNdarray_HOST_DIMS(self)[0],
1871                         CudaNdarray_HOST_DIMS(self)[1],
1872                         CudaNdarray_DEV_DATA(self),
1873                         1,
1874                         CudaNdarray_HOST_STRIDES(self)[0],
1875                         CudaNdarray_HOST_STRIDES(self)[1],
1876                         CudaNdarray_DEV_DATA(other),
1877                         1,
1878                         other_strides[0],
1879                         other_strides[1]);
1880                 CNDA_THREAD_SYNC;
1881                 cudaError_t err = cudaGetLastError();
1882                 if (cudaSuccess != err)
1883                 {
1884                     PyErr_Format(
1885                         PyExc_RuntimeError,
1886                         "CudaNdarray_inplace_elemwise case2: Cuda error: %s: %s.\n",
1887                         "k3",
1888                         cudaGetErrorString(err));
1889                     Py_XDECREF(new_other);
1890                     return -1;
1891                 }
1892             }
1893             break;
1894         case 3:
1895             {
1896                 //TODO:  Dimshuffle so that at least one of the arrays
1897                 //       has a contiguous dimension on the thread idx.
1898                 dim3 n_blocks(
1899                         std::min(
1900                             CudaNdarray_HOST_DIMS(self)[0],
1901                             NUM_VECTOR_OP_BLOCKS),
1902                         CudaNdarray_HOST_DIMS(self)[1]);
1903                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)
1904                     n_blocks.y /= 2;
1905                 dim3 n_threads(
1906                         std::min(
1907                             CudaNdarray_HOST_DIMS(self)[2],
1908                             NUM_VECTOR_OP_THREADS_PER_BLOCK));
1909                 k3<<<n_blocks, n_threads>>>(
1910                         CudaNdarray_HOST_DIMS(self)[0],
1911                         CudaNdarray_HOST_DIMS(self)[1],
1912                         CudaNdarray_HOST_DIMS(self)[2],
1913                         CudaNdarray_DEV_DATA(self),
1914                         CudaNdarray_HOST_STRIDES(self)[0],
1915                         CudaNdarray_HOST_STRIDES(self)[1],
1916                         CudaNdarray_HOST_STRIDES(self)[2],
1917                         CudaNdarray_DEV_DATA(other),
1918                         other_strides[0],
1919                         other_strides[1],
1920                         other_strides[2]);
1921                 CNDA_THREAD_SYNC;
1922                 cudaError_t err = cudaGetLastError();
1923                 if (cudaSuccess != err)
1924                 {
1925                     PyErr_Format(
1926                         PyExc_RuntimeError,
1927                         "CudaNdarray_inplace_elemwise case3: Cuda error: %s: %s.\n",
1928                         "k3",
1929                         cudaGetErrorString(err));
1930                     Py_XDECREF(new_other);
1931                     return -1;
1932                 }
1933             }
1934             break;
1935         case 4:
1936             {
1937                 dim3 n_blocks(
1938                         std::min(
1939                             CudaNdarray_HOST_DIMS(self)[0],
1940                             NUM_VECTOR_OP_BLOCKS),
1941                         CudaNdarray_HOST_DIMS(self)[1]
1942                         );
1943                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)
1944                     n_blocks.y /= 2;
1945                 dim3 n_threads(
1946                         std::min(
1947                             CudaNdarray_HOST_DIMS(self)[2],
1948                             NUM_VECTOR_OP_THREADS_PER_BLOCK)
1949                     //TODO: DON"T YOU NEED OT PUT DIMS[3] in here???
1950                             );
1951                 k4<<<n_blocks, n_threads>>>(
1952                         CudaNdarray_HOST_DIMS(self)[0],
1953                         CudaNdarray_HOST_DIMS(self)[1],
1954                         CudaNdarray_HOST_DIMS(self)[2],
1955                         CudaNdarray_HOST_DIMS(self)[3],
1956                         CudaNdarray_DEV_DATA(self),
1957                         CudaNdarray_HOST_STRIDES(self)[0],
1958                         CudaNdarray_HOST_STRIDES(self)[1],
1959                         CudaNdarray_HOST_STRIDES(self)[2],
1960                         CudaNdarray_HOST_STRIDES(self)[3],
1961                         CudaNdarray_DEV_DATA(other),
1962                         other_strides[0],
1963                         other_strides[1],
1964                         other_strides[2],
1965                         other_strides[3]);
1966                 CNDA_THREAD_SYNC;
1967                 cudaError_t err = cudaGetLastError();
1968                 if (cudaSuccess != err)
1969                 {
1970                     PyErr_Format(
1971                         PyExc_RuntimeError,
1972                         "CudaNdarray_inplace_elemwise case4: Cuda error: %s: %s.\n",
1973                         "k4",
1974                         cudaGetErrorString(err));
1975                     Py_XDECREF(new_other);
1976                     return -1;
1977                 }
1978             }
1979             break;
1980         case 5:
1981             {
1982                 dim3 n_blocks(
1983                         std::min(
1984                             CudaNdarray_HOST_DIMS(self)[1],
1985                             NUM_VECTOR_OP_BLOCKS),
1986                         CudaNdarray_HOST_DIMS(self)[2]);
1987                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)
1988                     n_blocks.y /= 2;
1989                 dim3 n_threads(
1990                         std::min(
1991                             CudaNdarray_HOST_DIMS(self)[3],
1992                             NUM_VECTOR_OP_THREADS_PER_BLOCK)
1993                     //TODO: DON"T YOU NEED OT PUT DIMS[3] in here???
1994                     );
1995                 for (int i = 0; i < CudaNdarray_HOST_DIMS(self)[0]; ++i)
1996                 {
1997                      k4<<<n_blocks, n_threads>>>(
1998                             CudaNdarray_HOST_DIMS(self)[1],
1999                             CudaNdarray_HOST_DIMS(self)[2],
2000                             CudaNdarray_HOST_DIMS(self)[3],
2001                             CudaNdarray_HOST_DIMS(self)[4],
2002                             CudaNdarray_DEV_DATA(self) + i * CudaNdarray_HOST_STRIDES(self)[0],
2003                             CudaNdarray_HOST_STRIDES(self)[1],
2004                             CudaNdarray_HOST_STRIDES(self)[2],
2005                             CudaNdarray_HOST_STRIDES(self)[3],
2006                             CudaNdarray_HOST_STRIDES(self)[4],
2007                             CudaNdarray_DEV_DATA(other) + i * other_strides[0],
2008                             other_strides[1],
2009                             other_strides[2],
2010                             other_strides[3],
2011                             other_strides[4]);
2012                     CNDA_THREAD_SYNC;
2013                     cudaError_t err = cudaGetLastError();
2014                     if( cudaSuccess != err)
2015                     {
2016                         PyErr_Format(
2017                             PyExc_RuntimeError,
2018                             "CudaNdarray_inplace_elemwise case5: Cuda error: %s: %s. n_block=(%ld,%ld) n_threads=%ld\n",
2019                             "k5 with loop over k4",
2020                             cudaGetErrorString(err),
2021                             (long) n_blocks.x, (long) n_blocks.y, (long) n_threads.x);
2022                         Py_XDECREF(new_other);
2023                         return -1;
2024                     }
2025                 }
2026             }
2027             break;
2028         case 6:
2029             {
2030                 dim3 n_blocks(
2031                         std::min(
2032                             CudaNdarray_HOST_DIMS(self)[0],
2033                             NUM_VECTOR_OP_BLOCKS),
2034                         CudaNdarray_HOST_DIMS(self)[1],
2035                         CudaNdarray_HOST_DIMS(self)[2]
2036                         );
2037                 while (n_blocks.x * n_blocks.y > NUM_VECTOR_OP_BLOCKS)
2038                     n_blocks.y /= 2;
2039                 // GTX285(compute capabilities 1.3) don't support n_blocks.z > 1
2040                 // (compute capabilities 2.0) support 65535 for n_blocks.z
2041                 //while (n_blocks.x * n_blocks.y * n_blocks.z > NUM_VECTOR_OP_BLOCKS)
2042                 //    n_blocks.z /= 2;
2043                 n_blocks.z = 1;
2044                 dim3 n_threads(
2045                         std::min(
2046                             CudaNdarray_HOST_DIMS(self)[3],
2047                             NUM_VECTOR_OP_THREADS_PER_BLOCK)
2048                     //TODO: DON'T YOU NEED TO PUT DIMS[4] in here???
2049                     //TODO: DON'T YOU NEED TO PUT DIMS[5] in here???
2050                             );
2051                 k6<<<n_blocks, n_threads>>>(
2052                         CudaNdarray_HOST_DIMS(self)[0],
2053                         CudaNdarray_HOST_DIMS(self)[1],
2054                         CudaNdarray_HOST_DIMS(self)[2],
2055                         CudaNdarray_HOST_DIMS(self)[3],
2056                         CudaNdarray_HOST_DIMS(self)[4],
2057                         CudaNdarray_HOST_DIMS(self)[5],
2058                         CudaNdarray_DEV_DATA(self),
2059                         CudaNdarray_HOST_STRIDES(self)[0],
2060                         CudaNdarray_HOST_STRIDES(self)[1],
2061                         CudaNdarray_HOST_STRIDES(self)[2],
2062                         CudaNdarray_HOST_STRIDES(self)[3],
2063                         CudaNdarray_HOST_STRIDES(self)[4],
2064                         CudaNdarray_HOST_STRIDES(self)[5],
2065                         CudaNdarray_DEV_DATA(other),
2066                         other_strides[0],
2067                         other_strides[1],
2068                         other_strides[2],
2069                         other_strides[3],
2070                         other_strides[4],
2071                         other_strides[5]);
2072                 CNDA_THREAD_SYNC;
2073                 cudaError_t err = cudaGetLastError();
2074                 if (cudaSuccess != err)
2075                 {
2076                     PyErr_Format(
2077                         PyExc_RuntimeError,
2078                         "CudaNdarray_inplace_elemwise case6: Cuda error: %s: %s. n_blocks=(%ld, %ld, %ld) n_threads=(%ld)\n",
2079                         "k6",
2080                         cudaGetErrorString(err),
2081                         (long) n_blocks.x, (long) n_blocks.y, (long) n_blocks.z,
2082                         (long) n_threads.x);
2083                     Py_XDECREF(new_other);
2084                     return -1;
2085                 }
2086             }
2087             break;
2088         default:
2089         {
2090             PyErr_Format(
2091                 PyExc_NotImplementedError,
2092                 "inplace_elemwise w nd=%i\n",
2093                 self->nd);
2094             Py_XDECREF(new_other);
2095             return -1;
2096         }
2097     }
2098     if (verbose)
2099         fprintf(stderr, "INPLACE ADD/DIV end\n");
2100     Py_XDECREF(new_other);
2101     return 0;
2102 }
2103
2104 /*
2105  * We need this inplace Add to support IncSubTensor
2106  * It returns py_self on success with an additional reference. Else NULL.
2107  */
2108 // Will be called by __iadd__ in Python
2109 PyObject *
2110 CudaNdarray_inplace_add(PyObject* py_self, PyObject * py_other)
2111 {
2112     if (CudaNdarray_inplace_elemwise(py_self, py_other, IADD))
2113     {
2114         return NULL;
2115     }
2116     Py_INCREF(py_self);
2117     return py_self;
2118 }
2119
2120 /*
2121  * We need this inplace div for cuda/tests/test_basic_ops.py:test_shared_options
2122  * It returns py_self on success with an additional reference. Else NULL.
2123  */
2124 // Will be called by __idiv__ in Python
2125 static PyObject *
2126 CudaNdarray_inplace_div(PyObject* py_self, PyObject * py_other)
2127 {
2128     if (CudaNdarray_inplace_elemwise(py_self, py_other, IDIV))
2129     {
2130         return NULL;
2131     }
2132     Py_INCREF(py_self);
2133     return py_self;
2134 }
2135
2136 // The PyNumberMethods struct layout changed in a non-trivial way from 2 to 3.
2137 #if PY_MAJOR_VERSION == 3
2138 static PyNumberMethods CudaNdarrayNumberMethods =
2139 {
2140     (binaryfunc)CudaNdarray_add,  //binaryfunc nb_add;  __add__
2141     0,  //binaryfunc nb_subtract;
2142     0,  //binaryfunc nb_multiply;
2143     0,  //binaryfunc nb_remainder;
2144     0,  //binaryfunc nb_divmod;
2145     0,  //ternaryfunc nb_power;
2146     0,  //unaryfunc nb_negative;
2147     0,  //unaryfunc nb_positive;
2148     0,  //unaryfunc nb_absolute;
2149     0,  //inquiry nb_bool;
2150     0,  //unaryfunc nb_invert;
2151     0,  //binaryfunc nb_lshift;
2152     0,  //binaryfunc nb_rshift;
2153     0,  //binaryfunc nb_and;
2154     0,  //binaryfunc nb_xor;
2155     0,  //binaryfunc nb_or;
2156     0,  //unaryfunc nb_int;
2157     0,  //void *nb_reserved;
2158     0,  //unaryfunc nb_float;
2159
2160     (binaryfunc)CudaNdarray_inplace_add,  //binaryfunc nb_inplace_add;  __iadd__
2161     0,  //binaryfunc nb_inplace_subtract;
2162     0,  //binaryfunc nb_inplace_multiply;
2163     0,  //binaryfunc nb_inplace_remainder;
2164     0,  //ternaryfunc nb_inplace_power;
2165     0,  //binaryfunc nb_inplace_lshift;
2166     0,  //binaryfunc nb_inplace_rshift;
2167     0,  //binaryfunc nb_inplace_and;
2168     0,  //binaryfunc nb_inplace_xor;
2169     0,  //binaryfunc nb_inplace_or;
2170
2171     0,  //binaryfunc nb_floor_divide;
2172     0,  //binaryfunc nb_true_divide;
2173     0,  //binaryfunc nb_inplace_floor_divide;
2174     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_true_divide;        __idiv__
2175
2176     0,  //unaryfunc nb_index
2177 };
2178 #else
2179 static PyNumberMethods CudaNdarrayNumberMethods =
2180 {
2181     (binaryfunc)CudaNdarray_add,  //binaryfunc nb_add;  __add__
2182     0,  //binaryfunc nb_subtract;      __sub__
2183     0,  //binaryfunc nb_multiply;      __mul__
2184     0,  //binaryfunc nb_divide;        __div__
2185     0,  //binaryfunc nb_remainder;     __mod__
2186     0,  //binaryfunc nb_divmod;        __divmod__
2187     0,  //ternaryfunc nb_power;        __pow__
2188     0,  //unaryfunc nb_negative;       __neg__
2189     0,  //unaryfunc nb_positive;       __pos__
2190     0,  //unaryfunc nb_absolute;       __abs__
2191     0,  //inquiry nb_nonzero;          __nonzero__     /* Used by PyObject_IsTrue */
2192     0,  //unaryfunc nb_invert;         __invert__
2193     0,  //binaryfunc nb_lshift;        __lshift__
2194     0,  //binaryfunc nb_rshift;        __rshift__
2195     0,  //binaryfunc nb_and;           __and__
2196     0,  //binaryfunc nb_xor;           __xor__
2197     0,  //binaryfunc nb_or;            __or__
2198     0,  //coercion nb_coerce;          __coerce__     /* Used by the coerce() function */
2199     0,  //unaryfunc nb_int;            __int__
2200     0,  //unaryfunc nb_long;           __long__
2201     0,  //unaryfunc nb_float;          __float__
2202     0,  //unaryfunc nb_oct;            __oct__
2203     0,  //unaryfunc nb_hex;            __hex__
2204
2205     /* Added in release 2.0 */
2206     (binaryfunc)CudaNdarray_inplace_add,  //binaryfunc nb_inplace_add;  __iadd__
2207     0,  //binaryfunc nb_inplace_subtract;      __isub__
2208     0,  //binaryfunc nb_inplace_multiply;      __imul__
2209     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_divide;        __idiv__
2210     0,  //binaryfunc nb_inplace_remainder;     __imod__
2211     0,  //ternaryfunc nb_inplace_power;        __ipow__
2212     0,  //binaryfunc nb_inplace_lshift;        __ilshift__
2213     0,  //binaryfunc nb_inplace_rshift;        __irshift__
2214     0,  //binaryfunc nb_inplace_and;           __iand__
2215     0,  //binaryfunc nb_inplace_xor;           __ixor__
2216     0,  //binaryfunc nb_inplace_or;            __ior__
2217
2218     /* Added in release 2.2 */
2219     0,  //binaryfunc nb_floor_divide;          __floordiv__
2220     0,  //binaryfunc nb_true_divide;           __truediv__
2221     0,  //binaryfunc nb_inplace_floor_divide;  __ifloordiv__
2222     (binaryfunc)CudaNdarray_inplace_div,  //binaryfunc nb_inplace_true_divide;   __itruediv__
2223
2224 #if PY_MINOR_VERSION > 4
2225     /* Added in release 2.5 */
2226     0  //unaryfunc nb_index;  __index__
2227 #endif
2228 };
2229 #endif
2230
2231
2232 /////////////////////
2233 // Mapping protocol
2234 /////////////////////
2235
2236 // Will by called by __len__ in Python
2237 static Py_ssize_t
2238 CudaNdarray_len(PyObject * py_self)
2239 {
2240     CudaNdarray * self = (CudaNdarray*) py_self;
2241     if (self->nd <= 0)
2242     {
2243         return (Py_ssize_t) 0;
2244     }
2245     else
2246     {
2247         return (Py_ssize_t) CudaNdarray_HOST_DIMS(self)[0];
2248     }
2249 }
2250
2251 // Will by called by __getitem__ in Python
2252 PyObject *
2253 CudaNdarray_Subscript(PyObject * py_self, PyObject * key)
2254 {
2255     int verbose = 0;
2256     if (verbose) fprintf(stderr, "Subscript .... \n");
2257     CudaNdarray * self = (CudaNdarray*) py_self;
2258     PyObject * py_rval = NULL;
2259     CudaNdarray * rval = NULL;
2260     PyObject * intobj = NULL;
2261
2262     //PyObject_Print(key, stderr, 0);
2263
2264     if (key == Py_Ellipsis)
2265     {
2266         Py_INCREF(py_self);
2267         return py_self;
2268     }
2269     if ((intobj=PyNumber_Int(key))) //INDEXING BY INTEGER
2270     //else if (PyInt_Check(key)) //INDEXING BY INTEGER
2271     {
2272         int d_idx = PyInt_AsLong(intobj);
2273         Py_DECREF(intobj); intobj=NULL;
2274         //int d_idx = PyInt_AsLong(key);
2275         if (self->nd == 0)
2276         {
2277             PyErr_SetString(PyExc_IndexError, "0-d arrays can't be indexed");
2278             return NULL;
2279         }
2280         int d_dim = CudaNdarray_HOST_DIMS(self)[0];
2281         int offset = 0;
2282
2283         if ((d_idx >= 0) && (d_idx < d_dim))
2284         {
2285             //normal indexing
2286             offset += d_idx * CudaNdarray_HOST_STRIDES(self)[0];
2287         }
2288         else if ((d_idx < 0) && (d_idx >= -d_dim))
2289         {
2290             //end-based indexing
2291             // d_idx is negative
2292             offset += (d_dim + d_idx) * CudaNdarray_HOST_STRIDES(self)[0];
2293         }
2294         else
2295         {
2296             PyErr_Format(PyExc_IndexError,
2297                          "index out of bounds. Asked %d, but size of %d",
2298                          d_idx, d_dim);
2299             return NULL;
2300         }
2301
2302         //allocate our subtensor view
2303         py_rval = CudaNdarray_new_nd(self->nd - 1);
2304         rval = (CudaNdarray*) py_rval;
2305         if (!rval) return NULL;
2306         assert (0 == rval->data_allocated);
2307
2308         //initialize the view's data pointer to our own.
2309         if (CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self) + offset, self))
2310         {
2311             Py_DECREF(rval);
2312             return NULL;
2313         }
2314         for (int d = 1; d < self->nd; ++d)
2315         {
2316             CudaNdarray_set_stride(rval, d-1, CudaNdarray_HOST_STRIDES(self)[d]);
2317             CudaNdarray_set_dim(rval, d-1, CudaNdarray_HOST_DIMS(self)[d]);
2318         }
2319     }
2320     else
2321     {
2322         PyErr_Clear();
2323     }
2324     if (PySlice_Check(key)) //INDEXING BY SLICE
2325     {
2326         if (verbose) fprintf(stderr, "by slice\n");
2327         if (self->nd == 0)
2328         {
2329             PyErr_SetString(PyExc_ValueError, "cannot slice a 0-d array");
2330             return NULL;
2331         }
2332
2333         int d_dim = CudaNdarray_HOST_DIMS(self)[0];
2334         Py_ssize_t start, stop, step, slen;
2335         if (PySlice_GetIndicesEx(SLICE_CAST(key), d_dim, &start, &stop, &step, &slen))
2336         {
2337             if (verbose)
2338                 fprintf(stderr, "PySlice_GetIndicesEx failed\n");
2339             return NULL;
2340         }
2341         if (verbose)
2342         {
2343             std::cerr << "start " << start << "\n";
2344             std::cerr << "stop " << stop << "\n";
2345             std::cerr << "step " << step << "\n";
2346             std::cerr << "slen " << slen << "\n";
2347         }
2348
2349         //allocate our subtensor view
2350         py_rval = CudaNdarray_new_nd(self->nd);
2351         rval = (CudaNdarray*) py_rval;
2352         if (!rval) return NULL;
2353         assert (0 == rval->data_allocated);
2354
2355
2356         //initialize the view's data pointer to our own.
2357         if (CudaNdarray_set_device_data(rval,
2358                     CudaNdarray_DEV_DATA(self) + start * CudaNdarray_HOST_STRIDES(self)[0],
2359                     self))
2360         {
2361             Py_DECREF(rval);
2362             return NULL;
2363         }
2364         //initialize dimension 0 of rval
2365         CudaNdarray_set_stride(rval, 0,
2366                 (slen == 1) ? 0 : step * CudaNdarray_HOST_STRIDES(self)[0]);
2367         CudaNdarray_set_dim(rval, 0, slen);
2368         if (verbose) std::cerr << "rval stride " << CudaNdarray_HOST_STRIDES(rval)[0] << "\n";
2369         // initialize dimensions > 0 of rval
2370         for (int d = 1; d < self->nd; ++d)
2371         {
2372             CudaNdarray_set_stride(rval, d, CudaNdarray_HOST_STRIDES(self)[d]);
2373             CudaNdarray_set_dim(rval, d, CudaNdarray_HOST_DIMS(self)[d]);
2374         }
2375     }
2376     if (PyTuple_Check(key)) //INDEXING BY TUPLE
2377     {
2378         if (verbose) fprintf(stderr, "by tuple\n");
2379         //elements of the tuple can be either integers or slices
2380         //the dimensionality of the view we will return is diminished for each slice in the tuple
2381
2382         if (PyTuple_Size(key) > self->nd)
2383         {
2384             PyErr_SetString(PyExc_IndexError, "index error");
2385             return NULL;
2386         }
2387
2388         //calculate the number of dimensions in the return value
2389         int rval_nd = self->nd;
2390         for (int d = 0; d < PyTuple_Size(key); ++d)
2391         {
2392             //On some paltform PyInt_Check(<type 'numpy.int64'>) return true, other it return false.
2393             //So we use PyArray_IsAnyScalar that should covert everything.
2394             rval_nd -= PyArray_IsAnyScalar(PyTuple_GetItem(key, d));
2395         }
2396
2397         //allocate our subtensor view
2398         py_rval = CudaNdarray_new_nd(rval_nd);
2399         rval = (CudaNdarray*) py_rval;
2400         if (!rval) return NULL;
2401         assert (0 == rval->data_allocated);
2402
2403         //initialize the view's data pointer to our own.
2404         if (CudaNdarray_set_device_data(rval, CudaNdarray_DEV_DATA(self), self))
2405         {
2406             Py_DECREF(rval);
2407             return NULL;
2408         }
2409
2410         // rval_d will refer to the current dimension in the rval.
2411         // It will not be incremented for integer keys, but will be incremented for slice
2412         // keys
2413         int rval_d = 0;
2414
2415         for (int d = 0; d < self->nd; ++d)
2416         {
2417             // keys can be shorter than self->nd.
2418             // when that happens, it means that the remaining dimensions are "full slices"
2419             if (d >=PyTuple_Size(key))
2420             {
2421                 CudaNdarray_set_stride(rval, rval_d, CudaNdarray_HOST_STRIDES(self)[d]);
2422                 CudaNdarray_set_dim(rval, rval_d, CudaNdarray_HOST_DIMS(self)[d]);
2423                 ++rval_d;
2424             }
2425             else
2426             {
2427                 PyObject * key_d = PyTuple_GetItem(key, d);
2428
2429                 if (PySlice_Check(key_d))
2430                 {
2431                     Py_ssize_t start, stop, step, slen;
2432                     if (PySlice_GetIndicesEx(SLICE_CAST(key_d), CudaNdarray_HOST_DIMS(self)[d], &start, &stop, &step, &slen))
2433                     {
2434                         Py_DECREF(rval);
2435                         return NULL;
2436                     }
2437                     rval->devdata += start * CudaNdarray_HOST_STRIDES(self)[d];
2438                     CudaNdarray_set_stride(rval, rval_d,
2439                             (slen == 1) ? 0 : step * CudaNdarray_HOST_STRIDES(self)[d]);
2440                     CudaNdarray_set_dim(rval, rval_d, slen);
2441                     if (0)
2442                     {
2443                         std::cerr << "start " << start << "\n";
2444                         std::cerr << "stop " << stop << "\n";
2445                         std::cerr << "step " << step << "\n";
2446                         std::cerr << "slen " << slen << "\n";
2447                     }
2448                     ++rval_d;
2449                 }
2450                 else if ((intobj=PyNumber_Int(key_d)))
2451                 {
2452                     assert(PyArray_IsAnyScalar(key_d));
2453                     int d_idx = PyInt_AsLong(intobj);
2454                     Py_DECREF(intobj);
2455                     intobj = NULL;
2456                     int d_dim = CudaNdarray_HOST_DIMS(self)[d];
2457
2458                     if ((d_idx >= 0) && (d_idx < d_dim))
2459                     {
2460                         //normal indexing
2461                         rval->devdata += d_idx * CudaNdarray_HOST_STRIDES(self)[d];
2462                     }
2463                     else if ((d_idx < 0) && (d_idx >= -d_dim))
2464                     {
2465                         //end-based indexing
2466                         rval->devdata += (d_dim + d_idx) * CudaNdarray_HOST_STRIDES(self)[d];
2467                     }
2468                     else
2469                     {
2470                         PyErr_Format(PyExc_IndexError,
2471                                      "index out of bounds. Asked %d for dimensions %d, but size of %d",
2472                                      d_idx, d, d_dim);
2473                         Py_DECREF(rval);
2474                         return NULL;
2475                     }
2476                 }
2477                 else
2478                 {
2479                     PyErr_Clear(); // clear the error set by PyNumber_Int
2480                     PyErr_SetString(PyExc_IndexError, "index must be either int or slice");
2481                     Py_DECREF(rval);
2482                     return NULL;
2483                 }
2484             }
2485         }
2486     }
2487     if (py_rval)
2488     {
2489         if (verbose) fprint_CudaNdarray(stderr, self);
2490         if (verbose) fprint_CudaNdarray(stderr, rval);
2491     }
2492     else
2493     {
2494         PyErr_SetString(PyExc_NotImplementedError, "Unknown key type");
2495         return NULL;
2496     }
2497     return py_rval;
2498 }
2499
2500 // Will by called by __setitem__ in Python
2501 // See http://docs.python.org/dev/py3k/c-api/object.html#PyObject_SetItem
2502 // Doesn't handle broadcasting, e.g. a[:] = 5
2503 // Can only be assigned from a CudaNdarray on the right side
2504 // Or a ndarray
2505 // Or a python scalar with value 0 when the left side part is c contiguous.
2506 static int
2507 CudaNdarray_setitem(PyObject *o, PyObject  *key, PyObject  *value)
2508 {
2509     int verbose = 0;
2510     if (verbose) fprintf(stderr, "CudaNdarray_setitem start\n");
2511     // We try to copy directly into this CudaNdarray from the ndarray
2512     CudaNdarray* rval = (CudaNdarray*)CudaNdarray_Subscript(o, key);
2513     CudaNdarray* new_value = NULL;
2514
2515     if(!rval){
2516         // CudaNdarray_Subscript failed and set the error msg.
2517         Py_XDECREF(rval);
2518         return -1;
2519     }
2520
2521     if(rval != (CudaNdarray*)o &&
2522                 (rval->data_allocated ||
2523                  // The new array should have a base
2524                  !(((CudaNdarray*)rval)->base) ||
2525                  // If the original array has no base, the base of the new
2526                  // array should be the original one
2527                  (!((CudaNdarray*)o)->base && ((CudaNdarray*)rval)->base != o) ||
2528                  // Else, the two arrays should have the same base
2529                  (((CudaNdarray*)o)->base && ((CudaNdarray*)rval)->base != ((CudaNdarray*)o)->base)))
2530     {
2531         // This case shouldn't happen, based on what I see in Subscript
2532         // but just in case it happens sometime in the future
2533
2534         PyErr_Format(PyExc_RuntimeError,
2535                      "__getitem__ must return a CudaNdarray that refers to"
2536                      " the original CudaNdarray, not a copy. rval.base=%p"
2537                      " o.base=%p o=%p",
2538                      (((CudaNdarray*)rval)->base), ((CudaNdarray*)o)->base, o);
2539         Py_DECREF(rval);
2540         return -1;
2541     }
2542
2543     PyObject * intobj = NULL;
2544     if (CudaNdarray_Check(o)  && PyArray_Check(value)){
2545         if (verbose)
2546             fprintf(stderr,
2547                     "CudaNdarray_setitem dest is a CudaNdarray and"
2548                     " value is a ndarray\n");
2549         new_value = (CudaNdarray*) CudaNdarray_New();
2550         if(!new_value)
2551         {
2552             return -1;
2553         }
2554         if (CudaNdarray_CopyFromArray(new_value, (PyArrayObject *) value))
2555         {
2556             Py_XDECREF(new_value);
2557             Py_XDECREF(rval);
2558             return -1;
2559         }
2560         value = (PyObject *) new_value;
2561     }
2562     else if ((intobj=PyNumber_Int(value)))
2563     {
2564         if (verbose)
2565             fprintf(stderr,
2566                     "CudaNdarray_setitem dest and value is a python number\n");
2567         if(! CudaNdarray_is_c_contiguous(rval)){
2568             PyErr_SetString(PyExc_NotImplementedError,
2569                  "CudaNdarray.__setitem__: When the new value is a scalar"
2570                  " of value 0 the part where we copy to must be c contiguous.");
2571             Py_XDECREF(rval);
2572             return -1;
2573         }
2574
2575         long val = PyInt_AsLong(intobj);
2576         Py_DECREF(intobj); intobj=NULL;
2577         if (val == 0)
2578         {
2579             cudaError_t err = cudaMemset(rval->devdata, 0,
2580                                          CudaNdarray_SIZE(rval) * sizeof(real));
2581             Py_XDECREF(rval);
2582             if (err)
2583             {
2584                 // Clear the error flag, cudaMemset doesn't do it.
2585                 // Currently this returns the same thing as err, but if in future
2586                 // it returns something else I still don't see why we should ignore
2587                 // it.  All we want to do here is reset the flag.
2588                 cudaGetLastError();
2589                 PyErr_SetString(PyExc_RuntimeError,
2590                                 "CudaNdarray.__setitem__: cudaMemset failed");
2591                 return -1;
2592             }
2593             return 0;
2594         } else {
2595             Py_XDECREF(rval);
2596             PyErr_SetString(PyExc_NotImplementedError,
2597                   "CudaNdarray.__setitem__: we support setting only python"
2598                   " scalar of value 0, numpy nd array and CudaNdarray.");
2599                 return -1;
2600         }
2601     }
2602
2603     PyErr_Clear(); // clear PyNumber_Int error.
2604
2605     if(!CudaNdarray_Check(o) || !CudaNdarray_Check(value))
2606     {
2607         PyErr_SetString(PyExc_TypeError,
2608           "CudaNdarray.__setitem__: left must be a CudaNdarrays and right"
2609           " must be a CudaNdarrays, an ndarray or a python scalar of value 0.");
2610         Py_XDECREF(new_value);
2611         return -1;
2612     }
2613
2614     if (verbose)
2615         fprintf(stderr, "CudaNdarray_setitem dest and value are CudaNdarray\n");
2616
2617     if (cnda_copy_structure_to_device(rval))
2618     {
2619         PyErr_SetString(PyExc_RuntimeError,
2620                 "CudaNdarray.__setitem__: syncing structure to device failed");
2621         Py_DECREF(rval);
2622         Py_XDECREF(new_value);
2623
2624         if (verbose)
2625             fprintf(stderr, "CudaNdarray_setitem error end\n");
2626         return -1;
2627     }
2628
2629     PyObject *baseSavedForComparison = rval->base;
2630
2631     if (CudaNdarray_CopyFromCudaNdarray(rval, (CudaNdarray*)value, true))
2632     {
2633         Py_DECREF((PyObject*)rval);
2634         Py_XDECREF(new_value);
2635
2636         if (verbose)
2637             fprintf(stderr, "CudaNdarray_setitem error end\n");
2638         return -1;
2639     }
2640
2641     assert (rval->base == baseSavedForComparison);
2642     assert (rval->dev_structure_fresh);
2643
2644     // Clean up locally-created references
2645     Py_DECREF(rval);
2646     Py_XDECREF(new_value);
2647
2648     return 0;
2649 }
2650
2651
2652 PyMappingMethods CudaNdarrayMappingMethods = {
2653     CudaNdarray_len, //lenfunc mp_length;               __len__
2654     CudaNdarray_Subscript, //binaryfunc mp_subscript;   __getitem__
2655     CudaNdarray_setitem //objobjargproc mp_ass_subscript;                __setitem__
2656 };
2657
2658 ////////////////////
2659 //
2660 ////////////////////
2661
2662 static PyObject *
2663 CudaNdarray_get_shape(CudaNdarray *self, void *closure)
2664 {
2665     if (self->nd < 0)
2666     {
2667         PyErr_SetString(PyExc_ValueError, "CudaNdarray not initialized");
2668         return NULL;
2669     }
2670     PyObject * rval = PyTuple_New(self->nd);
2671     for (int i = 0; i < self->nd; ++i)
2672     {
2673         if (!rval || PyTuple_SetItem(rval, i, PyInt_FromLong(CudaNdarray_HOST_DIMS(self)[i])))
2674         {
2675             Py_XDECREF(rval);
2676             return NULL;
2677         }
2678
2679     }
2680     return rval;
2681 }
2682
2683 static int
2684 CudaNdarray_set_shape(CudaNdarray *self, PyObject *value, void *closure)
2685 {
2686     PyErr_SetString(PyExc_NotImplementedError, "TODO: call reshape");
2687     return -1;
2688 }
2689
2690 static PyObject *
2691 CudaNdarray_get_strides(CudaNdarray *self, void *closure)
2692 {
2693     if (self->nd < 0)
2694     {
2695         PyErr_SetString(PyExc_ValueError, "CudaNdarray not initialized");
2696         return NULL;
2697     }
2698     PyObject * rval = PyTuple_New(self->nd);
2699     for (int i = 0; i < self->nd; ++i)
2700     {
2701         if (!rval || PyTuple_SetItem(rval, i, PyInt_FromLong(CudaNdarray_HOST_STRIDES(self)[i])))
2702         {
2703             Py_XDECREF(rval);
2704             return NULL;
2705         }
2706
2707     }
2708     return rval;
2709 }
2710
2711 static int
2712 CudaNdarray_set_strides(CudaNdarray *self, PyObject *value, void *closure)
2713 {
2714     //npy_intp newstrides_bytes[PyTuple_Size(value)];
2715     if (PyTuple_Check(value)){
2716         if (PyTuple_Size(value) != CudaNdarray_NDIM(self)){
2717             PyErr_SetString(PyExc_ValueError,
2718                             "The new strides tuple must have the same length"
2719                             " as the number of dimensions");
2720             return -1;
2721         }
2722     }else if (PyList_Check(value)){
2723         if (PyList_Size(value) != CudaNdarray_NDIM(self)){
2724             PyErr_SetString(PyExc_ValueError,
2725                             "The new strides list must have the same length"
2726                             " as the number of dimensions");
2727             return -1;
2728         }
2729     }else{
2730         PyErr_SetString(PyExc_ValueError,
2731                         "The new strides need to be encoded in a tuple or list");
2732         return -1;
2733     }
2734     npy_intp* newstrides = (npy_intp*) alloca(CudaNdarray_NDIM(self) * sizeof(npy_intp));
2735     if (PyTuple_Check(value)){
2736         for(int i=0; i < CudaNdarray_NDIM(self); i++){
2737             newstrides[i] = PyInt_AsLong(PyTuple_GetItem(value, Py_ssize_t(i)));
2738             //newstrides_bytes[i] = newstrides[i] * 4;
2739         }
2740     }else if (PyList_Check(value)){
2741         for(int i=0; i < CudaNdarray_NDIM(self); i++){
2742             newstrides[i] = PyInt_AsLong(PyList_GetItem(value, Py_ssize_t(i)));
2743             //newstrides_bytes[i] = newstrides[i] * 4;
2744         }
2745     }
2746     /*
2747     // Do not do this check, as ExtractDiag needs that, and NumPy does not seem
2748     // to do it.
2749     npy_intp dims[PyTuple_Size(value)];
2750     for(int i=0; i < CudaNdarray_NDIM(self); i++){
2751         dims[i] = CudaNdarray_HOST_DIMS(self)[i];
2752     }
2753     if (!PyArray_CheckStrides(4,
2754                               CudaNdarray_NDIM(self),
2755                               0, 0,
2756                               dims,
2757                               newstrides_bytes)){
2758         PyErr_SetString(PyExc_ValueError, "bad new strides");
2759         return -1;
2760         }
2761     */
2762     for(int i=0; i < CudaNdarray_NDIM(self); i++){
2763         CudaNdarray_set_stride(self, i, newstrides[i]);
2764     }
2765     return 0;
2766 }
2767
2768 static PyObject *
2769 CudaNdarray_get_dev_data(CudaNdarray *self, void *closure)
2770 {
2771     float * p =  CudaNdarray_DEV_DATA(self);
2772     //printf("get_dev_data %p %li \n", p, (long int)p );
2773     return PyInt_FromSize_t((size_t) CudaNdarray_DEV_DATA(self));
2774 }
2775
2776 static int
2777 CudaNdarray_set_dev_data(CudaNdarray *self, PyObject *value, void *closure)
2778 {
2779     Py_ssize_t newdevdata = PyInt_AsSsize_t(value);
2780     //printf("set_dev_data %p %li \n",(float*)newdevdata ,newdevdata);
2781     if (PyErr_Occurred())
2782     {
2783         return -1;
2784     }
2785     return  CudaNdarray_set_device_data(self, (float*)newdevdata, (CudaNdarray*)self->base);
2786 }
2787
2788 static PyObject *
2789 CudaNdarray_get_dtype(CudaNdarray *self, void *closure)
2790 {
2791     return PyString_FromString("float32");
2792 }
2793
2794 static PyObject *
2795 CudaNdarray_get_ndim(CudaNdarray *self, void *closure)
2796 {
2797     return PyInt_FromLong(self->nd);
2798 }
2799
2800 static PyObject *
2801 CudaNdarray_get_base(CudaNdarray *self, void *closure)
2802 {
2803     PyObject * base = self->base;
2804     if (!base)
2805     {
2806         // We cannot return a NULL pointer, use None instead
2807         base = Py_None;
2808     }
2809     Py_INCREF(base);
2810     return base;
2811 }
2812
2813 void put_in_dict(PyObject * dict, const char * key, int val)
2814 {
2815   PyObject * k = PyString_FromString(key);
2816   PyObject * v = PyInt_FromLong(val);
2817   PyDict_SetItem(dict, k, v);
2818   Py_DECREF(k);
2819   Py_DECREF(v);
2820 }
2821
2822 PyObject *
2823 GetDeviceProperties(PyObject* _unused, PyObject* args)
2824 {
2825   int dev_id = -1;
2826   if (! PyArg_ParseTuple(args, "i", &dev_id))
2827     return NULL;
2828   cudaDeviceProp deviceProp;
2829   cudaGetDeviceProperties(&deviceProp, dev_id);
2830
2831   PyObject * dict = PyDict_New();
2832   PyObject * str= PyString_FromString("name");
2833   PyObject * i = PyString_FromString(deviceProp.name);
2834   PyDict_SetItem(dict, str, i);
2835   Py_DECREF(str);
2836   Py_DECREF(i);
2837
2838   put_in_dict(dict, "major", deviceProp.major);
2839   put_in_dict(dict, "minor", deviceProp.minor);
2840 #if CUDART_VERSION >= 2020
2841   int driverVersion = 0, runtimeVersion = 0;
2842   cudaDriverGetVersion(&driverVersion);
2843   cudaRuntimeGetVersion(&runtimeVersion);
2844   put_in_dict(dict, "driverVersion", driverVersion);
2845   put_in_dict(dict, "runtimeVersion", runtimeVersion);
2846 #endif
2847 #if CUDART_VERSION >= 2000
2848
2849   put_in_dict(dict, "multiProcessorCount", deviceProp.multiProcessorCount);
2850   //if ConvertSMVer2Cores is not defined in cuda_runtime_api.h, the run time is too old.
2851   int sm_cores = -1;
2852   if(deviceProp.major==1)
2853     sm_cores = 32;
2854   else if(deviceProp.major==2 && deviceProp.minor==0)
2855     sm_cores = 32;
2856   else if(deviceProp.major==2 && deviceProp.minor==1)
2857     sm_cores = 48;
2858   put_in_dict(dict, "coresCount", sm_cores * deviceProp.multiProcessorCount);
2859 #endif
2860   put_in_dict(dict, "totalConstMem", deviceProp.totalConstMem);
2861   put_in_dict(dict, "sharedMemPerBlock", deviceProp.sharedMemPerBlock);
2862   put_in_dict(dict, "regsPerBlock", deviceProp.regsPerBlock);
2863   put_in_dict(dict, "warpSize", deviceProp.warpSize);
2864   put_in_dict(dict, "maxThreadsPerBlock", deviceProp.maxThreadsPerBlock);
2865   put_in_dict(dict, "maxThreadsDim0", deviceProp.maxThreadsDim[0]);
2866   put_in_dict(dict, "maxThreadsDim1", deviceProp.maxThreadsDim[1]);
2867   put_in_dict(dict, "maxThreadsDim2", deviceProp.maxThreadsDim[2]);
2868   put_in_dict(dict, "maxGridSize0", deviceProp.maxGridSize[0]);
2869   put_in_dict(dict, "maxGridSize1", deviceProp.maxGridSize[1]);
2870   put_in_dict(dict, "maxGridSize2", deviceProp.maxGridSize[2]);
2871   put_in_dict(dict, "memPitch", deviceProp.memPitch);
2872   put_in_dict(dict, "textureAlignment", deviceProp.textureAlignment);
2873   put_in_dict(dict, "clockRate", deviceProp.clockRate);
2874 #if CUDART_VERSION >= 2000
2875   put_in_dict(dict, "deviceOverlap", deviceProp.deviceOverlap);
2876 #endif
2877 #if CUDART_VERSION >= 2020
2878   put_in_dict(dict, "kernelExecTimeoutEnabled", deviceProp.kernelExecTimeoutEnabled);
2879   put_in_dict(dict, "integrated", deviceProp.integrated);
2880   put_in_dict(dict, "canMapHostMemory", deviceProp.canMapHostMemory);
2881   put_in_dict(dict, "computeMode", deviceProp.computeMode);
2882   //in the doc of this fct tell that 0 - Normal mode, 1 - only 1 context, 2 - no context
2883 #endif
2884 #if CUDART_VERSION >= 3000
2885   put_in_dict(dict, "concurrentKernels", deviceProp.concurrentKernels);
2886 #endif
2887 #if CUDART_VERSION >= 3010
2888   put_in_dict(dict, "ECCEnabled", deviceProp.ECCEnabled);
2889 #endif
2890 #if CUDART_VERSION >= 3020
2891   put_in_dict(dict, "tccDriver", deviceProp.tccDriver);
2892 #endif
2893
2894   return dict;
2895 }
2896
2897 /*
2898  * Returns in *free and *total respectively, the free and total amount of memory available for allocation by the device in bytes.
2899  */
2900 PyObject *
2901 GetDeviceMemInfo(PyObject* _unused, PyObject* dummy)
2902 {
2903     size_t free = 0, total = 0;
2904     if(g_gpu_context_active == 0){
2905         PyErr_Format(PyExc_RuntimeError, "No gpu device selected yet. Please make sure the gpu device was initialized by Theano before.");
2906         return NULL;
2907     }
2908
2909     cudaError_t err = cudaMemGetInfo(&free, &total);
2910     if (err != cudaSuccess){
2911         // Clear the error flag, cudaMemGetInfo doesn't do it.
2912         // Currently this returns the same thing as err, but if in future
2913         // it returns something else I still don't see why we should ignore
2914         // it.  All we want to do here is reset the flag.
2915         cudaGetLastError();
2916         PyErr_Format(PyExc_RuntimeError,
2917                      "Error while getting memory info about the gpu: %s",
2918                      cudaGetErrorString(err));
2919         return NULL;
2920     }
2921     return PyTuple_Pack(2, PyLong_FromLong(free), PyLong_FromLong(total));
2922 }
2923
2924 /*
2925  * Synchronize with all the gpu device stream.
2926  */
2927 PyObject *
2928 CudaNdarray_synchronize(PyObject* _unused, PyObject* dummy)
2929 {
2930     CNDA_BEGIN_ALLOW_THREADS
2931     cudaThreadSynchronize();
2932     CNDA_END_ALLOW_THREADS
2933     Py_INCREF(Py_None);
2934     return Py_None;
2935 }
2936
2937 /*
2938  * Exist and return true if we link with cublas v2.
2939  */
2940 PyObject *
2941 CudaNdarray_cublasv2(PyObject* _unused, PyObject* dummy)
2942 {
2943     Py_INCREF(Py_True);
2944     return Py_True;
2945 }
2946
2947 PyObject *
2948 CudaNdarray_select_a_gpu(PyObject* _unused, PyObject* dummy)
2949 {
2950     void * rval = NULL;
2951     cudaError_t err;
2952     int num_gpus = 0;
2953
2954     err = cudaGetDeviceCount(&num_gpus);
2955     if (cudaSuccess != err){
2956         printf("ERR!\\n");
2957             PyErr_Format(PyExc_RuntimeError,
2958                          "Not able to get number of GPUs (%s).",
2959                          cudaGetErrorString(err));
2960             return NULL;
2961     }
2962
2963     for (int device = 0; device < num_gpus; device++) {
2964         cudaSetDevice(device);
2965         err = cudaDeviceSynchronize(); // << CUDA context gets created here.
2966         cudaGetLastError(); // reset the error state
2967         if (cudaSuccess == err)
2968             break;
2969     }
2970
2971     if (cudaSuccess != err){
2972             printf("ERR!\\n");
2973                 PyErr_Format(PyExc_RuntimeError,
2974                              "Not able to select available GPU from %d cards (%s).",
2975                              num_gpus, cudaGetErrorString(err));
2976                 return NULL;
2977     }
2978
2979     Py_INCREF(Py_None);
2980     return Py_None;
2981 }
2982
2983 #if COMPUTE_GPU_MEM_USED
2984 /*
2985  * Return the size in bytes that Theano currently have allocated on the gpu.
2986  */
2987 PyObject *
2988 GetTheanoAllocInfo(PyObject* _unused, PyObject* dummy)
2989 {
2990     PyObject* a = PyLong_FromLong(_allocated_size);
2991     PyObject* b = PyLong_FromLong(_max_allocated_size);
2992
2993     PyObject* tuple = PyTuple_New(2);
2994     PyTuple_SetItem(tuple, 0, a);
2995     PyTuple_SetItem(tuple, 1, b);
2996     return tuple;
2997 }
2998 #endif
2999
3000 static PyGetSetDef CudaNdarray_getset[] = {
3001     {"shape",
3002         (getter)CudaNdarray_get_shape,
3003         (setter)CudaNdarray_set_shape,
3004         "shape of this ndarray (tuple)",
3005         NULL},
3006     {"_strides",
3007         (getter)CudaNdarray_get_strides,
3008         (setter)CudaNdarray_set_strides,
3009         "data pointer strides (in elements)",
3010         NULL},
3011     {"strides",
3012         (getter)CudaNdarray_get_strides,
3013         (setter)CudaNdarray_set_strides,
3014         "data pointer strides (in elements)",
3015         NULL},
3016     //gpudata is needed to allow calling pycuda fct with CudaNdarray input.
3017     {"gpudata",
3018         (getter)CudaNdarray_get_dev_data,
3019         NULL,
3020         "device data pointer",
3021         NULL},
3022     {"_dev_data",
3023         (getter)CudaNdarray_get_dev_data,
3024         (setter)CudaNdarray_set_dev_data,
3025         "device data pointer",
3026         NULL},
3027     {"dtype",
3028         (getter)CudaNdarray_get_dtype,
3029         NULL,
3030         "The dtype of the element. Now always float32",
3031         NULL},
3032     {"size",
3033         (getter)CudaNdarray_SIZE_Object,
3034         NULL,
3035         "The number of elements in this object.",
3036         NULL},
3037     //mem_size is neede for pycuda.elementwise.ElementwiseKernel Why do they use size and mem_size of the same value?
3038     {"mem_size",
3039         (getter)CudaNdarray_SIZE_Object,
3040         NULL,
3041         "The number of elements in this object.",
3042         NULL},
3043     {"ndim",
3044         (getter)CudaNdarray_get_ndim,
3045         NULL,
3046         "The number of dimensions in this object.",
3047         NULL},
3048     {"base",
3049         (getter)CudaNdarray_get_base,
3050         NULL,
3051         "If this ndarray is a view, base is the original ndarray.",
3052         NULL},
3053
3054     {NULL, NULL, NULL, NULL}  /* Sentinel */
3055 };
3056
3057 PyObject *CudaNdarray_repr(PyObject *self)
3058 {
3059     CudaNdarray *object = (CudaNdarray *)self;
3060     PyObject * np_object = CudaNdarray_CreateArrayObj(object);
3061     PyObject * str = PyObject_Str((PyObject *) np_object);
3062     char * cstr = PyString_AsString(str);
3063     PyObject * out = PyString_FromFormat("%s%s%s",
3064                         "CudaNdarray(",
3065                         cstr,
3066                         ")");
3067     Py_DECREF(str);
3068     Py_DECREF(np_object);
3069     #if PY_MAJOR_VERSION >= 3
3070     // In Python 3 PyString_FromFormat return a Bytes object
3071     PyObject* out2 = PyObject_Str(out);
3072     Py_DECREF(out);
3073     return out2;
3074     #endif
3075     return out;
3076 }
3077
3078 static PyTypeObject CudaNdarrayType =
3079 {
3080 #if PY_MAJOR_VERSION >= 3
3081     PyVarObject_HEAD_INIT(NULL, 0)
3082 #else
3083     PyObject_HEAD_INIT(NULL)
3084     0,                         /*ob_size*/
3085 #endif
3086     "CudaNdarray",             /*tp_name*/
3087     sizeof(CudaNdarray),       /*tp_basicsize*/
3088     0,                         /*tp_itemsize*/
3089     (destructor)CudaNdarray_dealloc, /*tp_dealloc*/
3090     0,                         /*tp_print*/
3091     0,                         /*tp_getattr*/
3092     0,                         /*tp_setattr*/
3093     0,                         /*tp_compare*/
3094     CudaNdarray_repr,          /*tp_repr*/
3095     &CudaNdarrayNumberMethods, /*tp_as_number*/
3096     0,                         /*tp_as_sequence*/
3097     &CudaNdarrayMappingMethods,/*tp_as_mapping*/
3098     0,                         /*tp_hash */
3099     0,                         /*tp_call*/
3100     0,                         /*tp_str*/
3101     0,                         /*tp_getattro*/
3102     0,                         /*tp_setattro*/
3103     0,                         /*tp_as_buffer*/
3104 #if PY_MAJOR_VERSION >= 3
3105     // Py_TPFLAGS_CHECKTYPES is always true and was removed in Python 3.
3106     Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, /*tp_flags*/
3107 #else
3108     Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_CHECKTYPES, /*tp_flags*/
3109 #endif
3110     "CudaNdarray objects",     /* tp_doc */
3111     0,                         /* tp_traverse */
3112     0,                         /* tp_clear */
3113     0,                         /* tp_richcompare */
3114     0,                         /* tp_weaklistoffset */
3115     0,                         /* tp_iter */
3116     0,                         /* tp_iternext */
3117     CudaNdarray_methods,       /* tp_methods */
3118     CudaNdarray_members,       /* tp_members */
3119     CudaNdarray_getset,        /* tp_getset */
3120     0,                         /* tp_base */
3121     0,                         /* tp_dict */
3122     0,                         /* tp_descr_get */
3123     0,                         /* tp_descr_set */
3124     0,                         /* tp_dictoffset */
3125     (initproc)CudaNdarray_init,/* tp_init */
3126     0,                         /* tp_alloc */
3127     CudaNdarray_new,           /* tp_new */
3128 };
3129
3130 static __global__ void get_gpu_ptr_size(int* dst)
3131 {
3132     dst[0] = sizeof(float*);
3133     dst[1] = sizeof(int);
3134 }
3135
3136 PyObject *
3137 CudaNdarray_ptr_int_size(PyObject* _unused, PyObject* args)
3138 {
3139     int *gpu_data = (int*)device_malloc(sizeof(int)*2);
3140     if(gpu_data == NULL){
3141         return NULL;
3142     }
3143     get_gpu_ptr_size<<<1,1>>>(gpu_data);
3144
3145     cudaError_t cudaErr = cudaGetLastError();
3146     if (cudaSuccess != cudaErr){
3147
3148         device_free(gpu_data);
3149         return PyErr_Format(PyExc_RuntimeError,
3150                             "CudaNdarray_ptr_int_size: error when calling the gpu code. (%s)",
3151                             cudaGetErrorString(cudaErr));
3152     }
3153
3154     // Transfer the result to cpu
3155     int gpu_sizes[] = {-1,-1};
3156     cublasStatus_t err;
3157     err = cublasGetVector(2, sizeof(int), gpu_data, 1, gpu_sizes, 1);
3158     device_free(gpu_data);
3159
3160     if (CUBLAS_STATUS_SUCCESS != err){
3161         PyErr_SetString(PyExc_RuntimeError, "error copying data to from memory");
3162         return NULL;
3163     }
3164     return Py_BuildValue("iiii", (int) gpu_sizes[0], (int)sizeof(float*),
3165                          (int)sizeof(int), (int) gpu_sizes[1]);
3166 }
3167
3168 static int cublas_init();
3169 static void cublas_shutdown();
3170 // Initialize the gpu.
3171 // Takes two optional parameters, the device number and if we should use cnmem.
3172 // If the device number is provided, it sets that device to be the active device.
3173 // If not provided (usually just to test whether the gpu is available at all),
3174 // it does not set an active device.
3175 // Raises EnvironmentError or ValueError (as appropriate) if the initialization failed.
3176 // cnmem is threaded like a bool. If converted to 0, don't use cnmem. Otherwise, use it.
3177 PyObject *
3178 CudaNdarray_gpu_init(PyObject* _unused, PyObject* args)
3179 {
3180     int card_nb = 0;
3181     int card_number_provided = 1;
3182     float cnmem = 0; // Theano flag lib.cnmem
3183     // if we're given something wildly invalid, this will throw a TypeError
3184     if(!PyArg_ParseTuple(args, "|if", &card_nb, &cnmem))
3185         return NULL;
3186     if(cnmem)
3187         g_use_cnmem = true;
3188
3189     if(PyTuple_Size(args) == 0) {
3190         card_number_provided = 0;
3191         card_nb = 0;
3192     }
3193
3194     int deviceCount;
3195     cudaError err = cudaGetDeviceCount(&deviceCount);
3196     if(cudaSuccess != err) {
3197         return PyErr_Format(PyExc_EnvironmentError,
3198                             "Unable to get the number of gpus available: %s",
3199                             cudaGetErrorString(cudaGetLastError()));
3200     }
3201
3202     // as soon as the first successful call to a cuda* function is made, a
3203     // gpu context has been created
3204     g_gpu_context_active = 1;
3205
3206     if(deviceCount <= 0) {
3207         return PyErr_Format(PyExc_EnvironmentError,
3208                             "Can't use the GPU, no devices support CUDA");
3209     }
3210     if(card_number_provided && (card_nb < 0 || card_nb > (deviceCount - 1))) {
3211         return PyErr_Format(PyExc_ValueError,
3212                             "Bad device number %d. Only %d devices available.",
3213                             card_nb,
3214                             deviceCount);
3215     }
3216
3217     cudaDeviceProp deviceProp;
3218     err = cudaGetDeviceProperties(&deviceProp, card_nb);
3219     if(cudaSuccess != err) {
3220         return PyErr_Format(PyExc_EnvironmentError,
3221                             "Unable to get properties of gpu %i: %s",
3222                             card_nb,
3223                             cudaGetErrorString(cudaGetLastError()));
3224     }
3225
3226     if(deviceProp.major == 9999 && deviceProp.minor == 9999 ){
3227         return PyErr_Format(PyExc_EnvironmentError,
3228                             "There is no device that supports CUDA");
3229     }
3230
3231     if(card_number_provided) {
3232         err = cudaSetDevice(card_nb);
3233         if(cudaSuccess != err) {
3234             return PyErr_Format(PyExc_EnvironmentError,
3235                                 "Unable to set device %i: %s",
3236                                 card_nb,
3237                                 cudaGetErrorString(cudaGetLastError()));
3238         }
3239         if (cublas_init() == -1)
3240             return NULL;
3241     }
3242     if(card_number_provided && g_use_cnmem) {
3243         size_t mem = 0;
3244         if (cnmem > 1)
3245             mem = cnmem * 1024 * 1024;
3246         else{
3247             // Clip to 95% to let memory for the driver.
3248             // 98% didn't worked in some cases.
3249             if (cnmem > .95){
3250                 cnmem = .95;
3251             }
3252             size_t free = 0, total = 0;
3253             cudaError_t err = cudaMemGetInfo(&free, &total);
3254             if (err != cudaSuccess){
3255                 // Clear the error flag, cudaMemGetInfo doesn't do it.
3256                 // Currently this returns the same thing as err, but if in future
3257                 // it returns something else I still don't see why we should ignore
3258                 // it.  All we want to do here is reset the flag.
3259                 cudaGetLastError();
3260                 PyErr_Format(PyExc_RuntimeError,
3261                              "Error while getting memory info about the gpu: %s",
3262                              cudaGetErrorString(err));
3263                 return NULL;
3264             }
3265             mem = total * cnmem;
3266         }
3267         if(initCnmem(card_number_provided, card_nb, mem) == -1){
3268             return NULL;
3269         }
3270     }
3271
3272     Py_INCREF(Py_None);
3273     return Py_None;
3274 }
3275
3276 PyObject *
3277 CudaNdarray_active_device_number(PyObject* _unused, PyObject* _unused_args) {
3278     // NB: No cuda error checking here; keeps things simple, and it's not
3279     // really necessary.
3280     int currentDevice;
3281     cudaGetDevice(&currentDevice);
3282     return PyInt_FromLong(currentDevice);
3283 }
3284
3285 PyObject *
3286 CudaNdarray_active_device_name(PyObject* _unused, PyObject* _unused_args) {
3287     // NB: No cuda error checking here; keeps things simple, and it's not
3288     // really necessary.
3289     int currentDevice;
3290     cudaGetDevice(&currentDevice);
3291
3292     cudaDeviceProp deviceProp;
3293     cudaGetDeviceProperties(&deviceProp, currentDevice);
3294     return PyString_FromString(deviceProp.name);
3295 }
3296
3297 PyObject *
3298 CudaNdarray_gpu_shutdown(PyObject* _unused, PyObject* _unused_args) {
3299     // Don't handle errors here
3300     cublas_shutdown();
3301     g_gpu_context_active = 0; // context has now been closed down
3302     if(g_use_cnmem) {
3303         cnmemStatus_t status = cnmemFinalize();
3304         if(status != CNMEM_STATUS_SUCCESS) {
3305             fprintf(stderr, "CudaNdarray_gpu_shutdown: cnmemFinalize failed! Reason=%s\n",
3306                     cnmemGetErrorString(status));
3307             if(status == CNMEM_STATUS_CUDA_ERROR) {
3308                 fprintf(stderr, "  Cuda-Reason=%s\n",
3309                         cudaGetErrorString(cudaGetLastError()));
3310             }
3311         }
3312     }
3313     cudaThreadExit();
3314
3315     Py_INCREF(Py_None);
3316     return Py_None;
3317 }
3318
3319 /*
3320  * This function is tested in theano/misc/test_pycuda_theano_simple.py
3321  */
3322 PyObject *
3323 CudaNdarray_from_gpu_pointer(PyObject* _unused, PyObject* args)
3324 {
3325     int verbose = 0;
3326     PyObject *gpu_ptr = NULL;
3327     PyObject *shapes = NULL;
3328     PyObject *strides = NULL;
3329     PyObject *base = NULL;
3330     PyObject *rval = NULL;
3331
3332     //args should consist of 3 python objects
3333     //The first is the gpu ptr
3334     //The second if the shape
3335     //The third if the strides
3336     if (! PyArg_ParseTuple(args, "OOOO", &gpu_ptr, &shapes, &strides, &base))
3337         return NULL;
3338
3339     if (verbose) printf("In CudaNdarray_from_gpu_pointer\n");
3340     if (!PyLong_Check(gpu_ptr))
3341     {
3342         PyErr_Format(PyExc_Exception, "CudaNdarray_from_gpu_pointer: The gpu pointor is not an long");
3343         return NULL;
3344     }
3345
3346     Py_ssize_t nd =  PyObject_Length(shapes);
3347     if (nd < 0)
3348     {
3349         PyErr_SetString(PyExc_TypeError, "CudaNdarray_from_gpu_pointer: Couldn't get length of second argument");
3350         return NULL;
3351     }
3352     Py_ssize_t nd_stride =  PyObject_Length(strides);
3353     if (nd_stride < 0)
3354     {
3355         PyErr_SetString(PyExc_TypeError, "CudaNdarray_from_gpu_pointer: Couldn't get length of third argument");
3356         return NULL;
3357     }
3358
3359     if (nd != nd_stride)
3360     {
3361         PyErr_SetString(PyExc_TypeError, "CudaNdarray_from_gpu_pointer: We need the same number of shapes and strides");
3362         return NULL;
3363     }
3364
3365     rval = CudaNdarray_New();
3366
3367     if (CudaNdarray_set_nd((CudaNdarray *)rval, nd))
3368     {
3369         //CudaNdarray_set_nd set the error msg
3370         return NULL;
3371     }
3372     // set gpu pointeur
3373     assert(((CudaNdarray *)rval)->data_allocated == 0);
3374     if (CudaNdarray_set_device_data((CudaNdarray *)rval, (float *)PyInt_AsLong(gpu_ptr), base))
3375     {
3376         PyErr_SetString(PyExc_TypeError, "CudaNdarray_from_gpu_pointer: Error while setting the gpu pointor");
3377         return NULL;
3378
3379     }
3380
3381     // Set dims and strides
3382     for (int i = nd-1; i >= 0; --i)
3383     {
3384         PyObject * idx = PyLong_FromLong(i);
3385         if (idx == NULL)
3386         {
3387             PyErr_SetString(PyExc_Exception, "CudaNdarray_from_gpu_pointer: Couldn't make long object to loop over list/tuple");
3388             return NULL;
3389         }
3390         PyObject* dim_ = PyObject_GetItem(shapes, idx);
3391         PyObject* strd_ = PyObject_GetItem(strides, idx);
3392         if (!PyInt_Check(dim_))
3393         {
3394             PyErr_Format(PyExc_Exception, "CudaNdarray_from_gpu_pointer: shapes[%d] is not an int", i);
3395             return NULL;
3396         }
3397         if (!PyInt_Check(strd_))
3398         {
3399             PyErr_Format(PyExc_Exception, "CudaNdarray_from_gpu_pointer: strides[%d] is not an int", i);
3400             return NULL;
3401         }
3402         int dim = PyInt_AsLong(dim_);
3403         int strd = PyInt_AsLong(strd_);
3404         CudaNdarray_set_stride((CudaNdarray *)rval, i, strd);
3405         CudaNdarray_set_dim((CudaNdarray *)rval, i, dim);
3406         Py_DECREF(idx);
3407         Py_DECREF(dim_);
3408         Py_DECREF(strd_);
3409     }
3410     if (verbose) printf("CudaNdarray_from_gpu_pointer normal return\n");
3411     return rval;
3412 }
3413
3414 PyObject *
3415 CudaNdarray_Dot(PyObject* _unused, PyObject* args)
3416 {
3417     PyObject *l=NULL;
3418     PyObject *r=NULL;
3419     PyObject * rval = NULL;
3420
3421     //args should consist of two python objects ("OO")
3422     if (! PyArg_ParseTuple(args, "OO", &l, &r))
3423         return NULL;
3424
3425     if (!CudaNdarray_Check(l) || !CudaNdarray_Check(r))
3426     {
3427         PyErr_SetString(PyExc_TypeError, "CudaNdarray arguments required ");
3428         goto CudaNdarray_dot_fail;
3429     }
3430     if (((CudaNdarray*)l)->nd != 2)
3431     {
3432         PyErr_SetString(PyExc_TypeError, "need 2d CudaNdarray arg for now");
3433         goto CudaNdarray_dot_fail;
3434     }
3435     if (((CudaNdarray*)r)->nd != 2)
3436     {
3437         PyErr_SetString(PyExc_TypeError, "need 2d CudaNdarray arg for now");
3438         goto CudaNdarray_dot_fail;
3439     }
3440     rval = CudaNdarray_New();
3441     if (!rval)
3442     {
3443         goto CudaNdarray_dot_fail;
3444     }
3445     int dims[2];
3446     dims[0] = CudaNdarray_HOST_DIMS((CudaNdarray*)l)[0];
3447     dims[1] = CudaNdarray_HOST_DIMS((CudaNdarray*)r)[1];
3448     if (CudaNdarray_alloc_contiguous((CudaNdarray*)rval, 2, dims))
3449     {
3450         goto CudaNdarray_dot_fail;
3451     }
3452     if (CudaNdarray_gemm(1.0, (CudaNdarray*)l, (CudaNdarray*)r, 0.0, (CudaNdarray*)rval))
3453     {
3454         goto CudaNdarray_dot_fail;
3455     }
3456
3457     return rval;
3458
3459     CudaNdarray_dot_fail:
3460     Py_XDECREF(rval);
3461     return NULL;
3462 }
3463
3464 static PyObject *
3465 filter(PyObject* __unsed_self, PyObject *args) // args = (data, broadcastable, strict, storage)
3466 {
3467     /*
3468      * TODO: DOC what this function should do in the various cases of
3469      * What is 'strict' supposed to mean in the context of this function?
3470      * What do we do with input that could be interpreted as matching the broadcastable pattern in strict vs. non-strict cases?
3471      *
3472      */
3473     PyObject *py_data=NULL;
3474     PyArrayObject * data = NULL;
3475     int strict = 0;
3476     PyObject * broadcastable=NULL;
3477     PyObject * storage=NULL;
3478     CudaNdarray * rval=NULL;
3479
3480     //Python object references which are provided to the caller are borrowed references
3481     if (!PyArg_ParseTuple(args, "OOiO", &py_data, &broadcastable, &strict, &storage)) return NULL;
3482
3483     if (!PyTuple_Check(broadcastable)){
3484         PyErr_SetString(PyExc_TypeError, "broadcastable arg should be a tuple of int.");
3485         return NULL;
3486     }
3487     Py_INCREF(py_data);
3488     Py_INCREF(broadcastable);
3489
3490     CudaNdarray * cnda = (CudaNdarray*)py_data;
3491
3492     if (strict || CudaNdarray_Check(py_data))
3493     {
3494         //TODO: support non-strict "casting" from a vt to the broadcastable/type/size that we need.
3495         if (!CudaNdarray_Check(py_data))
3496         {
3497             Py_DECREF(py_data);
3498             Py_DECREF(broadcastable);
3499             PyErr_SetString(PyExc_TypeError, "strict mode requires CudaNdarray");
3500             return NULL;
3501         }
3502         if (cnda->nd != PyTuple_Size(broadcastable))
3503         {
3504             Py_DECREF(py_data);
3505             Py_DECREF(broadcastable);
3506             PyErr_Format(PyExc_TypeError, "Wrong rank: %i vs %li", cnda->nd, (long)PyTuple_Size(broadcastable));
3507             return NULL;
3508         }
3509         for (int i = 0; i < cnda->nd; ++i)
3510         {
3511             if ((CudaNdarray_HOST_DIMS(cnda)[i] > 1) && PyInt_AsLong(PyTuple_GetItem(broadcastable, Py_ssize_t(i))))
3512             {
3513                 PyErr_Format(PyExc_TypeError, "Non-unit size in broadcastable vt dimension %i", i);
3514                 Py_DECREF(py_data);
3515                 Py_DECREF(broadcastable);
3516                 return NULL;
3517             }else if (CudaNdarray_HOST_DIMS(cnda)[i] == 1 && CudaNdarray_HOST_STRIDES(cnda)[i] != 0){
3518                 PyErr_Format(PyExc_TypeError, "Non-zeros strides(%d) on dimension %d of size 1",
3519                              CudaNdarray_HOST_STRIDES(cnda)[i], i);
3520                 Py_DECREF(py_data);
3521                 Py_DECREF(broadcastable);
3522                 return NULL;
3523             }
3524         }
3525         Py_DECREF(broadcastable);
3526         return py_data;
3527     }
3528     else
3529     {
3530         data = (PyArrayObject*)PyArray_FromObject(py_data, REAL_TYPENUM, PyTuple_Size(broadcastable), PyTuple_Size(broadcastable));
3531         if (!data)
3532         {
3533             //err message already defined
3534             Py_DECREF(py_data);
3535             Py_DECREF(broadcastable);
3536             return NULL;
3537         }
3538         for (int i = 0; i < PyArray_NDIM(data); ++i)
3539         {
3540             if ((PyArray_DIMS(data)[i] > 1) && PyInt_AsLong(PyTuple_GetItem(broadcastable, Py_ssize_t(i))))
3541             {
3542                 PyErr_Format(PyExc_TypeError, "Non-unit size in broadcastable dimension %i", i);
3543                 Py_DECREF(data);
3544                 Py_DECREF(py_data);
3545                 Py_DECREF(broadcastable);
3546                 return NULL;
3547             }
3548         }
3549         if (storage && CudaNdarray_Check(storage))
3550         {
3551             rval = (CudaNdarray*) storage;
3552             Py_INCREF(rval);
3553         }
3554         else
3555         {
3556             rval = (CudaNdarray*) CudaNdarray_New();
3557         }
3558         if (rval)
3559         {
3560             if (CudaNdarray_CopyFromArray(rval, data))
3561             {
3562                 Py_DECREF(rval);
3563                 rval = NULL;
3564             }
3565         }
3566         Py_DECREF(data);
3567         Py_DECREF(py_data);
3568         Py_DECREF(broadcastable);
3569         return (PyObject*)rval;
3570     }
3571 }
3572
3573 //TODO-- CudaNdarray_Dot and CudaNdarray_active_device_name are following different capitalization conventions.
3574 //       Pick one and standardize it, this file is already annoying enough to grep through
3575 static PyMethodDef module_methods[] = {
3576     {"dimshuffle", CudaNdarray_Dimshuffle, METH_VARARGS, "Returns the dimshuffle of a CudaNdarray."},
3577     {"dot", CudaNdarray_Dot, METH_VARARGS, "Returns the matrix product of two CudaNdarray arguments."},
3578     {"gpu_init", CudaNdarray_gpu_init, METH_VARARGS, "Select the gpu card to use; also usable to test whether CUDA is available."},
3579     {"select_a_gpu", CudaNdarray_select_a_gpu, METH_NOARGS, "Call this method if you want to select a GPU before gpu_init call and let the driver choose the GPU."}
,
3580     {"active_device_name", CudaNdarray_active_device_name, METH_VARARGS, "Get the name of the active device."},
3581     {"active_device_number", CudaNdarray_active_device_number, METH_VARARGS, "Get the number of the active device."},
3582     {"gpu_shutdown", CudaNdarray_gpu_shutdown, METH_VARARGS, "Shut down the gpu."},
3583     {"device_properties", GetDeviceProperties, METH_VARARGS, "Return a dictionary with the device properties."},
3584     {"mem_info", GetDeviceMemInfo, METH_NOARGS, "Return a tuple with the free and total memory on the gpu in bytes."},
3585 #if COMPUTE_GPU_MEM_USED
3586     {"theano_allocated", GetTheanoAllocInfo, METH_NOARGS, "Return the size in bytes of memory Theano currently have allocated on the gpu."},
3587 #endif
3588     {"ptr_int_size", CudaNdarray_ptr_int_size, METH_VARARGS, "Return a tuple with the size of gpu pointer, cpu pointer and int in bytes."},
3589     {"filter", filter, METH_VARARGS, "filter(obj, broadcastable, strict, storage) returns a CudaNdarray initialized to obj if it matches the constraints of broadca
stable.  strict=True prevents any numeric casting. If storage is a CudaNdarray it may be overwritten and used as the return value."},
3590     {"outstanding_mallocs", outstanding_mallocs, METH_VARARGS, "how many more mallocs have been called than free's"},
3591     {"from_gpu_pointer", CudaNdarray_from_gpu_pointer, METH_VARARGS, "Used to create a CudaNdarray from already allocated memory on the gpu.(example by pycuda)"},
3592     {"synchronize", CudaNdarray_synchronize, METH_NOARGS, "Used to synchronize the device"},
3593     {"cublas_v2", CudaNdarray_cublasv2, METH_NOARGS,
3594      "Used to know if this version of cuda_ndarray is linked with cublas v2."},
3595     {NULL, NULL, NULL, NULL}  /* Sentinel */
3596 };
3597
3598 #define CNDA_MOD_NAME "cuda_ndarray"
3599 #define CNDA_DOCSTRING "CUDA implementation of a numpy ndarray-like object."
3600
3601 #if PY_MAJOR_VERSION == 3
3602 static struct PyModuleDef cuda_ndarray_moduledef =
3603 {
3604     PyModuleDef_HEAD_INIT,
3605     CNDA_MOD_NAME,
3606     CNDA_DOCSTRING,
3607     -1,     /* size of per-interpreter state of the module,
3608                or -1 if the module keeps state in global variables. */
3609     module_methods
3610 };
3611
3612 PyMODINIT_FUNC
3613 PyInit_cuda_ndarray(void)
3614 #else
3615 PyMODINIT_FUNC
3616 initcuda_ndarray(void)
3617 #endif
3618 {
3619     import_array();
3620
3621     PyObject* m;
3622
3623     if (PyType_Ready(&CudaNdarrayType) < 0) {
3624 #if PY_MAJOR_VERSION == 3
3625         return NULL;
3626 #else
3627         return;
3628 #endif
3629     }
3630
3631 #if PY_MAJOR_VERSION == 3
3632     m = PyModule_Create(&cuda_ndarray_moduledef);
3633 #else
3634     m = Py_InitModule3(CNDA_MOD_NAME, module_methods, CNDA_DOCSTRING);
3635 #endif
3636
3637     if (m == NULL) {
3638 #if PY_MAJOR_VERSION == 3
3639         return NULL;
3640 #else
3641         return;
3642 #endif
3643     }
3644
3645     Py_INCREF(&CudaNdarrayType);
3646     PyModule_AddObject(m, "CudaNdarray", (PyObject *)&CudaNdarrayType);
3647 #if COMPUTE_GPU_MEM_USED
3648     for(int i=0;i<TABLE_SIZE;i++){
3649         _alloc_size_table[i].ptr=NULL;
3650         _alloc_size_table[i].size=0;
3651     }
3652 #endif
3653     //    cublasInit();
3654     //if (0&&CUBLAS_STATUS_SUCCESS != cublasGetError())
3655     //{
3656         //std::cerr << "WARNING: initcuda_ndarray: error initializing device\n";
3657     //}
3658     if (0) //TODO: is this necessary?
3659     {
3660         int deviceId = 0; // TODO: what number goes here?
3661         cudaSetDevice(deviceId);
3662         cudaError_t err = cudaGetLastError();
3663         if( cudaSuccess != err)
3664         {
3665             std::cerr << "Error in SetDevice:" << cudaGetErrorString(err) << "\n";
3666         }
3667     }
3668
3669 #if PY_MAJOR_VERSION == 3
3670     return m;
3671 #endif
3672 }
3673
3674
3675 //////////////////////////////////////
3676 //
3677 // C API FOR CudaNdarray
3678 //
3679 //////////////////////////////////////
3680
3681 int
3682 CudaNdarray_Check(const PyObject * ob)
3683 {
3684     //TODO: doesn't work with inheritance
3685     return CudaNdarray_CheckExact(ob);
3686 }
3687 int
3688 CudaNdarray_CheckExact(const PyObject * ob)
3689 {
3690     return ((Py_TYPE(ob) == &CudaNdarrayType) ? 1 : 0);
3691 }
3692
3693 PyObject *
3694 CudaNdarray_New(int nd)
3695 {
3696     CudaNdarray *self = (CudaNdarray *)CudaNdarrayType.tp_alloc(&CudaNdarrayType, 0);
3697     if (self == NULL)
3698     {
3699         PyErr_SetString(PyExc_RuntimeError, "CudaNdarray_New failed to allocate self");
3700         return NULL;
3701     }
3702     CudaNdarray_null_init(self);
3703
3704     if (nd == 0)
3705     {
3706         self->nd = 0;
3707     }
3708     else if (nd > 0)
3709     {
3710         if (CudaNdarray_set_nd(self, nd))
3711         {
3712             Py_DECREF(self);
3713             return NULL;
3714         }
3715     }
3716     ++_outstanding_mallocs[1];
3717     return (PyObject *)self;
3718 }
3719
3720
3721
3722 //////////////////////////////
3723 //
3724 // Published helper functions
3725 //
3726 //////////////////////////////
3727
3728 static int
3729 cublas_init()
3730 {
3731     cublasStatus_t err;
3732     err = cublasCreate(&handle);
3733     if (CUBLAS_STATUS_SUCCESS != err)
3734     {
3735         if(CUBLAS_STATUS_NOT_INITIALIZED == err)
3736             PyErr_SetString(PyExc_RuntimeError,
3737                             "cublasCreate() returned this error "
3738                             "'the CUDA Runtime initialization failed'");
3739         else if(CUBLAS_STATUS_ALLOC_FAILED == err)
3740             PyErr_SetString(PyExc_RuntimeError,
3741                             "cublasCreate() returned this error "
3742                             "'the resources could not be allocated'");
3743         else
3744             PyErr_SetString(PyExc_RuntimeError,
3745                             "unknow error during returned by cublasCreate()");
3746         return -1;
3747     }
3748     // Set the default stream as the one to execute on (default)
3749     cublasSetStream(handle, NULL);
3750     // Pointer to scalars are on the host (also default)
3751     cublasSetPointerMode(handle, CUBLAS_POINTER_MODE_HOST);
3752 #if CUDA_VERSION >= 5000
3753     // atomics can be used in kernels to speed up operations (not default)
3754     // This may lead to a slight variance from run to run in some operations
3755     cublasSetAtomicsMode(handle, CUBLAS_ATOMICS_ALLOWED);
3756 #endif
3757     return 0;
3758 }
3759
3760 static void
3761 cublas_shutdown()
3762 {
3763     if (handle != NULL)
3764         cublasDestroy(handle);
3765     // No point in handling any errors here
3766     handle = NULL;
3767 }
3768
3769 int
3770 CudaNdarray_CopyFromArray(CudaNdarray * self, PyArrayObject*obj)
3771 {
3772     int err = CudaNdarray_alloc_contiguous(self, PyArray_NDIM(obj),
3773                                            PyArray_DIMS(obj));
3774     if (err) {
3775         return err;
3776     }
3777
3778     int typenum = PyArray_TYPE(obj);
3779     if (typenum != REAL_TYPENUM)
3780     {
3781         PyErr_SetString(PyExc_TypeError, "can only copy from float arrays");
3782         return -1;
3783     }
3784     assert( 4 ==  PyArray_ITEMSIZE(obj));
3785     PyArrayObject * py_src = (PyArrayObject *)PyArray_ContiguousFromAny(
3786         (PyObject*)obj, typenum, self->nd, self->nd);
3787     if (!py_src) {
3788         return -1;
3789     }
3790     npy_intp py_src_size = PyArray_SIZE(py_src);
3791     void *py_src_data = PyArray_DATA(py_src);
3792     cudaError_t cerr;
3793     CNDA_BEGIN_ALLOW_THREADS;
3794     cerr = cudaMemcpy(self->devdata, py_src_data,
3795                       py_src_size * sizeof(real),
3796                       cudaMemcpyHostToDevice);
3797     //CNDA_THREAD_SYNC;  // unneeded because cudaMemcpy is blocking anyway
3798     CNDA_END_ALLOW_THREADS;
3799     if (cudaSuccess != cerr)
3800     {
3801         PyErr_Format(PyExc_RuntimeError,
3802                      "Cuda error '%s' while copying %lli data element"
3803                      " to device memory. str ptr=%p. dst ptr=%p",
3804                      cudaGetErrorString(cerr),
3805                      (long long)py_src_size,
3806                      py_src_data,
3807                      self->devdata);
3808         Py_DECREF(py_src);
3809         return -1;
3810     }
3811     Py_DECREF(py_src);
3812     return 0;
3813 }
3814
3815 PyObject *
3816 CudaNdarray_new_nd(int nd)
3817 {
3818     CudaNdarray * rval = (CudaNdarray*) CudaNdarray_New();
3819     if (!rval || CudaNdarray_set_nd(rval, nd))
3820     {
3821         Py_XDECREF(rval);
3822         rval = NULL;
3823     }
3824     return (PyObject *) rval;
3825 }
3826
3827
3828 /**
3829  * Initialize 'self' as a view of 'base', with memory storage 'data'
3830  */
3831
3832 int CudaNdarray_set_device_data(CudaNdarray * self, float * data, PyObject * base)
3833 {
3834     if (self->data_allocated)
3835     {
3836         assert(self->devdata);
3837         if (device_free(self->devdata))
3838         {
3839             self->devdata = NULL;
3840             self->data_allocated = 0;
3841             return -1;
3842         }
3843     }
3844     // Get the original base object (base.base.base...)
3845     PyObject * orig_base = base;
3846     // base is not always a CudaNdarray. It can be a GpuArray from pycuda, ...
3847     while (orig_base && CudaNdarray_Check(orig_base) && ((CudaNdarray*) orig_base)->base)
3848     {
3849         // base_base is itself a view
3850         orig_base = ((CudaNdarray*) orig_base)->base;
3851     }
3852     //N.B. XDECREF and XINCREF are no-ops for NULL pointers
3853     if (self->base != orig_base)
3854     {
3855         Py_XDECREF(self->base);
3856         self->base = orig_base;
3857         Py_XINCREF(self->base);
3858     }
3859     self->data_allocated = 0;
3860     self->devdata = data;
3861     return 0;
3862 }
3863
3864 static __global__ void k_copy_1d(const int N, const float * x, const int sx, float * y, const int sy)
3865 {
3866     for (int i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += gridDim.x*blockDim.x)
3867     {
3868         y[i*sy] = x[i*sx];
3869     }
3870 }
3871
3872 // N1 through N4 are the size of y
3873 static __global__ void k_copy_4d(const int N1,
3874         const int N2, const int N3, const int N4,
3875         const float * x, const int sx1, const int sx2, const int sx3,
3876         const int sx4,  float * y, const int sy1, const int sy2,
3877         const int sy3, const int sy4)
3878 {
3879     // These must be made int instead of unsigned int due to a bug in nvcc
3880     int bx = blockIdx.x;
3881     int by = blockIdx.y;
3882
3883     for (int i = bx; i < N1; i += gridDim.x)
3884     {
3885         for (int j = by; j < N2; j += gridDim.y)
3886         {
3887             for (int k = threadIdx.x; k < N3; k += (int) blockDim.x)
3888             {
3889                 for (int l = threadIdx.y; l < N4; l += (int) blockDim.y)
3890                 {
3891                     y[i * sy1 + j * sy2 + k * sy3 + l * sy4] =
3892                         x[i * sx1 + j * sx2 + k * sx3 + l * sx4];
3893                 }
3894             }
3895         }
3896     }
3897 }
3898
3899 //copy from other into self
3900 int CudaNdarray_CopyFromCudaNdarray(CudaNdarray * self,
3901                                     const CudaNdarray * other,
3902                                     bool unbroadcast)
3903 {
3904     int verbose = 0;
3905     if (verbose>1) fprintf(stderr, "CudaNdarray_CopyFromCudaNdarray\n");
3906
3907     //standard elemwise size checks
3908     if (self->nd == -1)
3909     {
3910         PyErr_SetString(PyExc_TypeError,
3911                         "can't copy into un-initialized CudaNdarray");
3912         return -1;
3913     }
3914     CudaNdarray * new_other = NULL;
3915
3916     if (self->nd < other->nd)
3917     {
3918         PyErr_Format(PyExc_NotImplementedError,
3919             "CudaNdarray_CopyFromCudaNdarray: The number of dimensions of the "
3920             "destination needs to be >= the number of dimensions of the "
3921             "source. Got %d and %d.", self->nd, other->nd);
3922         return -1;
3923     }
3924     else if (self->nd != other->nd)
3925     {
3926         new_other = (CudaNdarray *) CudaNdarray_View(other);
3927         int added_dims = self->nd - other->nd;
3928         int* pattern = (int*) alloca(self->nd * sizeof(int));
3929         for(int i = 0; i < added_dims; i++)
3930             pattern[i] = -1;
3931         for(int i = 0; i < other->nd; i++)
3932             pattern[i + added_dims] = i;
3933         CudaNdarray_dimshuffle(new_other, self->nd, pattern);
3934         other = new_other;
3935     }
3936     assert(self->nd == other->nd);
3937     //standard elemwise dim checks (also compute total size)
3938     unsigned int size = 1;
3939     unsigned int size_source = 1;
3940     for (int i = 0; i< self->nd; ++i)
3941     {
3942         if ((CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(other)[i])
3943             && (1!=CudaNdarray_HOST_DIMS(other)[i] || !unbroadcast) )
3944         {
3945           PyErr_Format(PyExc_ValueError,
3946                        "CudaNdarray_CopyFromCudaNdarray:"
3947                        " need same dimensions for dim %d,"
3948                        " destination=%d, source=%d",
3949                        i, CudaNdarray_HOST_DIMS(self)[i],
3950                        CudaNdarray_HOST_DIMS(other)[i]);
3951           Py_XDECREF(new_other);
3952           return -1;
3953         }
3954         size *= (unsigned int) CudaNdarray_HOST_DIMS(self)[i];
3955         size_source *= (unsigned int) CudaNdarray_HOST_DIMS(other)[i];
3956     }
3957     if (0 == size)
3958     {
3959         Py_XDECREF(new_other);
3960         return 0; //nothing to copy, we're done.
3961     }
3962     if (CudaNdarray_is_c_contiguous(self) &&
3963         CudaNdarray_is_c_contiguous(other) &&
3964         size == size_source)
3965     {
3966         if (verbose)
3967             fprintf(stderr, "Copying contiguous vector with cublasScopy\n");
3968
3969         cublasStatus_t err;
3970         err = cublasScopy(handle, size, CudaNdarray_DEV_DATA(other), 1,
3971                           CudaNdarray_DEV_DATA(self), 1);
3972         CNDA_THREAD_SYNC;
3973         Py_XDECREF(new_other);
3974         if (CUBLAS_STATUS_SUCCESS != err)
3975         {
3976             PyErr_SetString(PyExc_RuntimeError, "Error copying memory");
3977             return -1;
3978         }
3979         return 0;
3980     }
3981     //TODO: rewrite these copy operations to be more efficient
3982     //      See, for example the transpose example in the cuda_sdk.
3983     switch (self->nd)
3984     {
3985         case 0: // scalar
3986             {
3987                 // THIS CASE SHOULD NEVER HAPPEN BECAUSE SCALARS ARE ALWAYS C CONTIGUOUS
3988                 assert(0);
3989             }; break;
3990         case 1: // vector
3991             {
3992                 if (verbose) fprintf(stderr, "Copying non-contiguous vector\n");
3993                 if (verbose) fprint_CudaNdarray(stderr, other);
3994                 unsigned int n_blocks = std::min(size,
3995                                                  (unsigned int)NUM_VECTOR_OP_BLOCKS);
3996                 unsigned int n_threads = std::min(ceil_intdiv(size, n_blocks),
3997                                                   (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);
3998                 k_copy_1d<<<n_blocks, n_threads>>>(size,
3999                                             CudaNdarray_DEV_DATA(other),
4000                                             CudaNdarray_HOST_STRIDES(other)[0],
4001                                             CudaNdarray_DEV_DATA(self),
4002                                             CudaNdarray_HOST_STRIDES(self)[0]);
4003                 CNDA_THREAD_SYNC;
4004                 cudaError_t err = cudaGetLastError();
4005                 if( cudaSuccess != err)
4006                 {
4007                     PyErr_Format(PyExc_RuntimeError,
4008                                  "Cuda error: %s: %s. (n_blocks=%i,"
4009                                  " n_threads_per_block=%i)\n", "k_copy_1d",
4010                                  cudaGetErrorString(err), n_blocks, n_threads);
4011                     Py_XDECREF(new_other);
4012                     return -1;
4013                 }
4014             }; break;
4015         case 4: // 4-tensor
4016             {
4017                 if (verbose)
4018                 {
4019                     if (0 != fprint_CudaNdarray(stderr, other))
4020                     {
4021                         Py_XDECREF(new_other);
4022                         return -1;
4023                     }
4024                 }
4025
4026                 // The blocks implement the looping over the first two axes so
4027                 // this needs to be (N1, N2)
4028                 dim3 n_blocks( std::min(CudaNdarray_HOST_DIMS(self)[0],
4029                                         NUM_VECTOR_OP_BLOCKS),
4030                                std::min(CudaNdarray_HOST_DIMS(self)[1],
4031                                         NUM_VECTOR_OP_BLOCKS));
4032                 // For the threads, just make as many as possible
4033                 dim3 n_threads( std::min( (unsigned int) CudaNdarray_HOST_DIMS(self)[2],
4034                                  (unsigned int) NUM_VECTOR_OP_THREADS_PER_BLOCK),
4035                                 std::min( (unsigned int) CudaNdarray_HOST_DIMS(self)[3],
4036                                     (unsigned int) NUM_VECTOR_OP_THREADS_PER_BLOCK));
4037
4038                 n_threads.x = std::min( (unsigned int) 32, (unsigned int) n_threads.x);
4039                 n_threads.y = std::min( n_threads.y, NUM_VECTOR_OP_THREADS_PER_BLOCK / n_threads.x);
4040
4041                 k_copy_4d<<<n_blocks, n_threads>>>(
4042                                             // size of y
4043                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[0], // N1
4044                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[1], // N2
4045                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[2], // N3
4046                                             (unsigned int) CudaNdarray_HOST_DIMS(self)[3], // N4
4047                                             CudaNdarray_DEV_DATA(other), // x
4048                                             // x strides
4049                                             CudaNdarray_HOST_STRIDES(other)[0],
4050                                             CudaNdarray_HOST_STRIDES(other)[1],
4051                                             CudaNdarray_HOST_STRIDES(other)[2],
4052                                             CudaNdarray_HOST_STRIDES(other)[3],
4053                                             CudaNdarray_DEV_DATA(self), // y
4054                                             // y strides
4055                                             CudaNdarray_HOST_STRIDES(self)[0],
4056                                             CudaNdarray_HOST_STRIDES(self)[1],
4057                                             CudaNdarray_HOST_STRIDES(self)[2],
4058                                             CudaNdarray_HOST_STRIDES(self)[3]
4059                                             );
4060                 CNDA_THREAD_SYNC;
4061                 cudaError_t err = cudaGetLastError();
4062                 if( cudaSuccess != err)
4063                 {
4064                     PyErr_Format(PyExc_RuntimeError,
4065                                  "Cuda error: %s: %s.",
4066                                  "k_copy_4d",
4067                                  cudaGetErrorString(err));
4068                     Py_XDECREF(new_other);
4069                     return -1;
4070                 }
4071             }; break;
4072         default:
4073             {
4074                 cudaError_t err = cudaGetLastError();
4075                 if(cudaSuccess != err){
4076                     PyErr_Format(PyExc_RuntimeError,
4077                                  "Unexpected Cuda error: %s: %s\n",
4078                                  "CudaNdarray_CopyFromCudaNdarray",
4079                                  cudaGetErrorString(err));
4080                     Py_XDECREF(new_other);
4081                     return -1;
4082                 }
4083
4084                 if (verbose)
4085                     fprintf(stderr,
4086                             "Copying with default version unbroadcast=%d\n",
4087                             unbroadcast);
4088                 // call worker routine
4089                 unsigned int threads_per_block = std::min(size,
4090                                                           (unsigned int)NUM_VECTOR_OP_THREADS_PER_BLOCK);
4091                 unsigned int n_blocks = std::min(ceil_intdiv(size, threads_per_block),
4092                                                  (unsigned int)NUM_VECTOR_OP_BLOCKS);
4093                 const CudaNdarray * cuda_dims = other;
4094                 if(unbroadcast)
4095                     cuda_dims = self;
4096                 //copy from other into self
4097                 k_elemwise_unary_rowmajor_copy<<<n_blocks, threads_per_block>>>(
4098                         size,
4099                         (unsigned int)other->nd,
4100                         (const int *)CudaNdarray_DEV_DIMS(cuda_dims),
4101                         (const float*)CudaNdarray_DEV_DATA(other),
4102                         (const int *)CudaNdarray_DEV_STRIDES(other),
4103                         CudaNdarray_DEV_DATA(self),
4104                         (const int *)CudaNdarray_DEV_STRIDES(self));
4105                 CNDA_THREAD_SYNC;
4106                 err = cudaGetLastError();
4107                 if(verbose>1)
4108                     fprintf(stderr,
4109                             "INFO k_elemwise_unary_rowmaj (n_blocks=%i,"
4110                             " n_threads_per_block=%i)\n",
4111                             n_blocks, threads_per_block);
4112                 if( cudaSuccess != err)
4113                 {
4114                     //fprint_CudaNdarray(stderr, self);
4115                     //fprint_CudaNdarray(stderr, other);
4116                     PyErr_Format(PyExc_RuntimeError,
4117                                  "Cuda error: %s: %s. (n_blocks=%i,"
4118                                  " n_threads_per_block=%i)\n",
4119                                  "k_elemwise_unary_rowmajor_copy",
4120                                  cudaGetErrorString(err), n_blocks,
4121                                  threads_per_block);
4122                     Py_XDECREF(new_other);
4123                     return -1;
4124                 }
4125             }
4126     };
4127     Py_XDECREF(new_other);
4128     return 0;
4129 }
4130
4131 int CudaNdarray_gemm(float alpha, const CudaNdarray * A, const CudaNdarray * B, float beta, CudaNdarray * C)
4132 {
4133     if (A->nd != 2)
4134     {
4135         PyErr_SetString(PyExc_ValueError, "non-matrix arg A to gemm");
4136         return -1;
4137     }
4138     if (B->nd != 2)
4139     {
4140         PyErr_SetString(PyExc_ValueError, "non-matrix arg B to gemm");
4141         return -1;
4142     }
4143     if (C->nd != 2)
4144     {
4145         PyErr_SetString(PyExc_ValueError, "non-matrix arg C to gemm");
4146         return -1;
4147     }
4148
4149     // We must allow dimensions to be zeros.
4150     if ((CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(B)[0])
4151             || (CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(C)[0])
4152             || (CudaNdarray_HOST_DIMS(B)[1] != CudaNdarray_HOST_DIMS(C)[1]))
4153     {
4154         PyErr_Format(PyExc_ValueError, "dimension mismatch in args to gemm (%i,%i)x(%i,%i)->(%i,%i)",
4155                 CudaNdarray_HOST_DIMS(A)[0],
4156                 CudaNdarray_HOST_DIMS(A)[1],
4157                 CudaNdarray_HOST_DIMS(B)[0],
4158                 CudaNdarray_HOST_DIMS(B)[1],
4159                 CudaNdarray_HOST_DIMS(C)[0],
4160                 CudaNdarray_HOST_DIMS(C)[1]);
4161         return -1;
4162     }
4163
4164     // If matrix A or B has non-unit size and non-unit stride in both
4165     // dimensions, we can make a copy.
4166     CudaNdarray * A_new = NULL;
4167     CudaNdarray * B_new = NULL;
4168     if (((CudaNdarray_HOST_DIMS(A)[0] > 1)
4169          && (CudaNdarray_HOST_STRIDES(A)[0] != 1)
4170          && (CudaNdarray_HOST_DIMS(A)[1] > 1)
4171          && (CudaNdarray_HOST_STRIDES(A)[1] != 1))
4172         || (CudaNdarray_HOST_STRIDES(A)[0] < 0)
4173         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))
4174     {
4175         A_new = (CudaNdarray*) CudaNdarray_Copy(A);
4176         if (!A_new)
4177             return -1;
4178         A = A_new;
4179     }
4180
4181     if (((CudaNdarray_HOST_DIMS(B)[0] > 1)
4182          && (CudaNdarray_HOST_STRIDES(B)[0] != 1)
4183          && (CudaNdarray_HOST_DIMS(B)[1] > 1)
4184          && (CudaNdarray_HOST_STRIDES(B)[1] != 1))
4185         || (CudaNdarray_HOST_STRIDES(B)[0] < 0)
4186         || (CudaNdarray_HOST_STRIDES(B)[1] < 0))
4187     {
4188         B_new = (CudaNdarray*) CudaNdarray_Copy(B);
4189         if (!B_new)
4190         {
4191             // If A_new is NULL, meaning A was not copied nothing happens
4192             Py_XDECREF(A_new);
4193             return -1;
4194         }
4195         B = B_new;
4196     }
4197
4198     // If matrix C has non-unit size and non-unit stride in both
4199     // dimensions, or negative strides, we can't operate. We cannot copy
4200     // C either, because the calling code will expect the result to be
4201     // in the original C container.
4202     if (((CudaNdarray_HOST_DIMS(C)[0] > 1)
4203          && (CudaNdarray_HOST_STRIDES(C)[0] != 1)
4204          && (CudaNdarray_HOST_DIMS(C)[1] > 1)
4205          && (CudaNdarray_HOST_STRIDES(C)[1] != 1))
4206         || (CudaNdarray_HOST_STRIDES(C)[0] < 0)
4207         || (CudaNdarray_HOST_STRIDES(C)[1] < 0))
4208     {
4209         PyErr_Format(PyExc_AssertionError,
4210                      "non-unit or negative stride in gemm arg C (%i,%i) of shape (%i,%i)",
4211                      CudaNdarray_HOST_STRIDES(C)[0],
4212                      CudaNdarray_HOST_STRIDES(C)[1],
4213                      CudaNdarray_HOST_DIMS(C)[0],
4214                      CudaNdarray_HOST_DIMS(C)[1]);
4215         Py_XDECREF(A_new);
4216         Py_XDECREF(B_new);
4217         return -1;
4218     }
4219
4220     // the unit integer is divided logically into three fields of 4 bits
4221     // the lowermost 4 bits encode the stride pattern of the output
4222     // the next higher 4 bits encode the B variable (or y)
4223     // the next higher 4 bits encode the C variable (or x)
4224     //
4225     // the stride pattern for each input is encoded as 0 for unit stride from col to col (Row major)
4226     //                                                 1 for unit stride from row to row (Col major)
4227
4228     // a stride of 0 implies a dimension of 1 - so we can actually define
4229     // a stride of 0 as a 'unit' stride because gemm will never use it.
4230     // If a dimension is 0, its stride will not be used either, so we can
4231     // consider it a 'unit' stride too.
4232     int unit = 0;
4233     if (CudaNdarray_HOST_STRIDES(A)[1] == 1 || CudaNdarray_HOST_DIMS(A)[1] <= 1) {
4234         unit |= (0x0 << 8);
4235     } else if (CudaNdarray_HOST_STRIDES(A)[0] == 1 || CudaNdarray_HOST_DIMS(A)[0] <= 1) {
4236         unit |= (0x1 << 8);
4237     } else {
4238         unit |= (0x2 << 8);
4239     }
4240     if (CudaNdarray_HOST_STRIDES(B)[1] == 1 || CudaNdarray_HOST_DIMS(B)[1] <= 1) {
4241         unit |= (0x0 << 4);
4242     } else if (CudaNdarray_HOST_STRIDES(B)[0] == 1 || CudaNdarray_HOST_DIMS(B)[0] <= 1) {
4243         unit |= (0x1 << 4);
4244     } else {
4245         unit |= (0x2 << 4);
4246     }
4247     if (CudaNdarray_HOST_STRIDES(C)[1] == 1 || CudaNdarray_HOST_DIMS(C)[1] <= 1) {
4248         unit |= (0x0 << 0);
4249     } else if (CudaNdarray_HOST_STRIDES(C)[0] == 1 || CudaNdarray_HOST_DIMS(C)[0] <= 1) {
4250         unit |= (0x1 << 0);
4251     } else {
4252         unit |= (0x2 << 0);
4253     }
4254
4255     /* create appropriate strides for malformed matrices that are row or column
4256      * vectors
4257      */
4258     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0] : CudaNdarray_HOST_DIMS(A)[1];
4259     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1] : CudaNdarray_HOST_DIMS(A)[0];
4260     int sb_0 = (CudaNdarray_HOST_DIMS(B)[0] > 1) ? CudaNdarray_HOST_STRIDES(B)[0] : CudaNdarray_HOST_DIMS(B)[1];
4261     int sb_1 = (CudaNdarray_HOST_DIMS(B)[1] > 1) ? CudaNdarray_HOST_STRIDES(B)[1] : CudaNdarray_HOST_DIMS(B)[0];
4262     int sc_0 = (CudaNdarray_HOST_DIMS(C)[0] > 1) ? CudaNdarray_HOST_STRIDES(C)[0] : CudaNdarray_HOST_DIMS(C)[1];
4263     int sc_1 = (CudaNdarray_HOST_DIMS(C)[1] > 1) ? CudaNdarray_HOST_STRIDES(C)[1] : CudaNdarray_HOST_DIMS(C)[0];
4264
4265     float* a = CudaNdarray_DEV_DATA(A);
4266     float* b = CudaNdarray_DEV_DATA(B);
4267     float* c = CudaNdarray_DEV_DATA(C);
4268     cublasOperation_t N = CUBLAS_OP_N;
4269     cublasOperation_t T = CUBLAS_OP_T;
4270     //std::cerr << (unit/256) MOD 16 << (unit / 16) MOD 16 << unit MOD 16<< '\\n';
4271     // There should be no negative stride at that point
4272 #define CHK_STRIDE_SGEMM(T0, T1, D0, D1, D2, a, x, sx, y, sy, b, z, sz) \
4273     if (sx == 0){sx = 1;}\
4274     if (sy == 0){sy = 1;}\
4275     if (sz == 0){sz = 1;}\
4276     if ((sx > 0) && (sy > 0) && (sz > 0)) { \
4277         err = cublasSgemm(handle, T0, T1, D0, D1, D2, &a, x, sx, y, sy, &b, z, sz); \
4278     } else { \
4279         PyErr_SetString(PyExc_AssertionError, "negative stride to sGemm");\
4280         Py_XDECREF(A_new);\
4281         Py_XDECREF(B_new);\
4282         return -1; \
4283     }
4284
4285     cublasStatus_t err;
4286     switch(unit)
4287     {
4288         case 0x000: CHK_STRIDE_SGEMM(N, N, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_0, a, sa_0, beta, c,
 sc_0); break;
4289         case 0x100: CHK_STRIDE_SGEMM(N, T, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_0, a, sa_1, beta, c,
 sc_0); break;
4290         case 0x010: CHK_STRIDE_SGEMM(T, N, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_1, a, sa_0, beta, c,
 sc_0); break;
4291         case 0x110: CHK_STRIDE_SGEMM(T, T, CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(A)[1], alpha, b, sb_1, a, sa_1, beta, c,
 sc_0); break;
4292         case 0x001: CHK_STRIDE_SGEMM(T, T, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_0, b, sb_0, beta, c,
 sc_1); break;
4293         case 0x101: CHK_STRIDE_SGEMM(N, T, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_1, b, sb_0, beta, c,
 sc_1); break;
4294         case 0x011: CHK_STRIDE_SGEMM(T, N, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_0, b, sb_1, beta, c,
 sc_1); break;
4295         case 0x111: CHK_STRIDE_SGEMM(N, N, CudaNdarray_HOST_DIMS(C)[0], CudaNdarray_HOST_DIMS(C)[1], CudaNdarray_HOST_DIMS(A)[1], alpha, a, sa_1, b, sb_1, beta, c,
 sc_1); break;
4296         default: PyErr_Format(PyExc_ValueError, "some matrix has no unit stride (unit=%x)", unit);
4297                  return -1;
4298     };
4299     CNDA_THREAD_SYNC;
4300     Py_XDECREF(A_new);
4301     Py_XDECREF(B_new);
4302
4303     if (CUBLAS_STATUS_SUCCESS != err)
4304     {
4305         PyErr_Format(PyExc_RuntimeError,
4306                      "cublasSgemm failed (%i) %s\n"
4307                      " unit=%x N=%d, c.dims=[%d %d], a.dim=[%d %d], alpha=%f, beta=%f, a=%p, b=%p, c=%p"
4308                      " sa_0=%d, sa_1=%d, sb_0=%d, sb_1=%d, sc_0=%d, sc_1=%d",
4309                      err,  cublasGetErrorString(err),
4310                      unit, N,
4311                      CudaNdarray_HOST_DIMS(C)[0],
4312                      CudaNdarray_HOST_DIMS(C)[1],
4313                      CudaNdarray_HOST_DIMS(A)[0], CudaNdarray_HOST_DIMS(A)[1],
4314                      alpha, beta, a, b, c, sa_0, sa_1, sb_0, sb_1, sc_0, sc_1);
4315
4316         return -1;
4317     }
4318     return 0;
4319 }
4320
4321 int CudaNdarray_sgemv(float alpha, const CudaNdarray * A, const CudaNdarray * B, float beta, CudaNdarray * C)
4322 {
4323     /**
4324     * C <- alpha A B + beta C
4325     *    A : matrix
4326     *    B, C: vector
4327     *    alpha, beta: scalars
4328     */
4329     if (A->nd != 2) { PyErr_SetString(PyExc_ValueError, "non-matrix arg to gemv"); return -1; }
4330     if (B->nd != 1) { PyErr_SetString(PyExc_ValueError, "non-vector arg to gemv"); return -1; }
4331     if (C->nd != 1) { PyErr_SetString(PyExc_ValueError, "non-vector arg to gemv"); return -1; }
4332
4333     // We must allow dimensions to be zeros.
4334     if ((CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(B)[0])
4335             || (CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(C)[0]))
4336     {
4337         PyErr_Format(PyExc_ValueError, "dimension mismatch in args to gemv (%i,%i)x(%i)->(%i)",
4338                 CudaNdarray_HOST_DIMS(A)[0],
4339                 CudaNdarray_HOST_DIMS(A)[1],
4340                 CudaNdarray_HOST_DIMS(B)[0],
4341                 CudaNdarray_HOST_DIMS(C)[0]);
4342         return -1;
4343     }
4344
4345     // If matrix A has non-unit size and non-unit stride in both
4346     // dimensions, or negative strides, we cannot operate, but we can
4347     // make a copy.
4348     CudaNdarray * A_new = NULL;
4349     CudaNdarray * B_new = NULL;
4350     if (((CudaNdarray_HOST_DIMS(A)[0] > 1)
4351          && (CudaNdarray_HOST_STRIDES(A)[0] != 1)
4352          && (CudaNdarray_HOST_DIMS(A)[1] > 1)
4353          && (CudaNdarray_HOST_STRIDES(A)[1] != 1))
4354         || (CudaNdarray_HOST_STRIDES(A)[0] < 0)
4355         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))
4356     {
4357         A_new = (CudaNdarray*) CudaNdarray_Copy(A);
4358         if (!A_new)
4359             return -1;
4360         A = A_new;
4361     }
4362
4363     // If vector B as a negative stride, we also have to make a copy.
4364     if (CudaNdarray_HOST_STRIDES(B)[0] < 0)
4365     {
4366         B_new = (CudaNdarray*) CudaNdarray_Copy(B);
4367         if (!B_new)
4368         {
4369             // If A was not copied, A_new is NULL, and Py_XDECREF does not
4370             // do anything
4371             Py_XDECREF(A_new);
4372             return -1;
4373         }
4374         B = B_new;
4375     }
4376
4377     // cudablas does not handle negative strides as expected
4378     if (   (CudaNdarray_HOST_STRIDES(A)[0] < 0)
4379         || (CudaNdarray_HOST_STRIDES(A)[1] < 0))
4380     {
4381         PyErr_Format(PyExc_ValueError, "illegal strides in args to gemv (%i,%i)",
4382                 CudaNdarray_HOST_STRIDES(A)[0],
4383                 CudaNdarray_HOST_STRIDES(A)[1]);
4384         Py_XDECREF(A_new);
4385         Py_XDECREF(B_new);
4386         return -1;
4387     }
4388
4389     /* create appropriate strides for malformed matrices that are row or column
4390      * vectors
4391      */
4392     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0] : CudaNdarray_HOST_DIMS(A)[1];
4393     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1] : CudaNdarray_HOST_DIMS(A)[0];
4394     int sb_0 = (CudaNdarray_HOST_DIMS(B)[0] > 1) ? CudaNdarray_HOST_STRIDES(B)[0] : 1;
4395     int sc_0 = (CudaNdarray_HOST_DIMS(C)[0] > 1) ? CudaNdarray_HOST_STRIDES(C)[0] : 1;
4396
4397     if (sa_0 == 0)
4398         sa_0 = 1;
4399     if (sa_1 == 0)
4400         sa_1 = 1;
4401
4402     // This is important because we can end up not calling Sgemv at all
4403     cublasStatus_t err = CUBLAS_STATUS_SUCCESS;
4404     if (CudaNdarray_SIZE(C)) {
4405         if ((CudaNdarray_HOST_DIMS(A)[0] <= 1)
4406             || ((CudaNdarray_HOST_STRIDES(A)[0] == 1)
4407                 && (CudaNdarray_HOST_STRIDES(A)[1] > 0)))
4408         {
4409             err = cublasSgemv(handle, CUBLAS_OP_N,
4410                     CudaNdarray_HOST_DIMS(A)[0], CudaNdarray_HOST_DIMS(A)[1],
4411                     &alpha,
4412                     CudaNdarray_DEV_DATA(A), sa_1,
4413                     CudaNdarray_DEV_DATA(B), sb_0,
4414                     &beta,
4415                     CudaNdarray_DEV_DATA(C), sc_0);
4416         }
4417         else if ((CudaNdarray_HOST_DIMS(A)[1] <= 1)
4418                 || ((CudaNdarray_HOST_STRIDES(A)[1] == 1)
4419                     && (CudaNdarray_HOST_STRIDES(A)[0] > 0)))
4420         {
4421             err = cublasSgemv(handle, CUBLAS_OP_T,
4422                     CudaNdarray_HOST_DIMS(A)[1], CudaNdarray_HOST_DIMS(A)[0],
4423                     &alpha,
4424                     CudaNdarray_DEV_DATA(A), sa_0,
4425                     CudaNdarray_DEV_DATA(B), sb_0,
4426                     &beta,
4427                     CudaNdarray_DEV_DATA(C), sc_0);
4428         }
4429         else
4430         {
4431             PyErr_Format(PyExc_AssertionError,
4432                          "Unexpected stride pattern in gemv: (%i, %i) x %i -> %i.\n"
4433                          "Shapes are: (%i, %i) x %i -> %i\n",
4434                          CudaNdarray_HOST_STRIDES(A)[0],
4435                          CudaNdarray_HOST_STRIDES(A)[1],
4436                          CudaNdarray_HOST_STRIDES(B)[0],
4437                          CudaNdarray_HOST_STRIDES(C)[0],
4438                          CudaNdarray_HOST_DIMS(A)[0],
4439                          CudaNdarray_HOST_DIMS(A)[1],
4440                          CudaNdarray_HOST_DIMS(B)[0],
4441                          CudaNdarray_HOST_DIMS(C)[0]);
4442             Py_XDECREF(A_new);
4443             Py_XDECREF(B_new);
4444             return -1;
4445         }
4446     }
4447
4448     CNDA_THREAD_SYNC;
4449     Py_XDECREF(A_new);
4450     Py_XDECREF(B_new);
4451
4452     if (CUBLAS_STATUS_SUCCESS != err)
4453     {
4454         PyErr_Format(PyExc_RuntimeError,
4455                      "cublasSgemv failed (%i)",
4456                      err);
4457         return -1;
4458     }
4459     return 0;
4460 }
4461
4462 int CudaNdarray_sger(float alpha, const CudaNdarray * x, const CudaNdarray * y, CudaNdarray * A) {
4463     if (x->nd != 1) { PyErr_SetString(PyExc_ValueError, "non-vector arg x to sger"); return -1; }
4464     if (y->nd != 1) { PyErr_SetString(PyExc_ValueError, "non-vector arg y to sger"); return -1; }
4465     if (A->nd != 2) { PyErr_SetString(PyExc_ValueError, "non-matrix arg A to sger"); return -1; }
4466
4467     if ((CudaNdarray_HOST_DIMS(A)[0] != CudaNdarray_HOST_DIMS(x)[0])
4468         || (CudaNdarray_HOST_DIMS(A)[1] != CudaNdarray_HOST_DIMS(y)[0])) {
4469         PyErr_Format(PyExc_ValueError,
4470                      "dimension mismatch in args to sger (%i)x(%i)->(%i,%i)",
4471                      CudaNdarray_HOST_DIMS(x)[0],
4472                      CudaNdarray_HOST_DIMS(y)[0],
4473                      CudaNdarray_HOST_DIMS(A)[0],
4474                      CudaNdarray_HOST_DIMS(A)[1]);
4475         return -1;
4476     }
4477
4478     int x_strides = CudaNdarray_HOST_STRIDES(x)[0];
4479     CudaNdarray * x_new = NULL;
4480     if(x_strides == 0){
4481         if(CudaNdarray_HOST_DIMS(x)[0] != 1){
4482             PyErr_Format(PyExc_RuntimeError,
4483                          "CudaNdarray_sger: Invalid input x (should not happen)."
4484                          " We received a CudaNdarray vector with a stride of 0"
4485                          " that has more than 1 element!");
4486             return -1;
4487         }
4488         x_strides = 1;
4489     } else if(x_strides < 0){
4490         x_new = (CudaNdarray*) CudaNdarray_Copy(x);
4491         x = x_new;
4492         x_strides = CudaNdarray_HOST_STRIDES(x)[0];
4493     }
4494
4495     int y_strides = CudaNdarray_HOST_STRIDES(y)[0];
4496     CudaNdarray * y_new = NULL;
4497     if(y_strides == 0){
4498         if(CudaNdarray_HOST_DIMS(y)[0] != 1){
4499             PyErr_Format(PyExc_RuntimeError,
4500                          "CudaNdarray_sger: Invalid input y (should not happen)."
4501                          " We received a CudaNdarray vector with a stride of 0"
4502                          " that has more than 1 elements!");
4503             Py_XDECREF(x_new);
4504             return -1;
4505         }
4506         y_strides = 1;
4507     } else if(y_strides < 0){
4508         y_new = (CudaNdarray*) CudaNdarray_Copy(y);
4509         y = y_new;
4510         y_strides = CudaNdarray_HOST_STRIDES(y)[0];
4511     }
4512
4513     // Create appropriate strides if A is a row or column vector
4514     int sa_0 = (CudaNdarray_HOST_DIMS(A)[0] > 1) ? CudaNdarray_HOST_STRIDES(A)[0]
4515                                                  : CudaNdarray_HOST_DIMS(A)[1];
4516     int sa_1 = (CudaNdarray_HOST_DIMS(A)[1] > 1) ? CudaNdarray_HOST_STRIDES(A)[1]
4517                                                  : CudaNdarray_HOST_DIMS(A)[0];
4518
4519     // This is important because we can end up not calling Sger at all
4520     cublasStatus_t err = CUBLAS_STATUS_SUCCESS;
4521     if(CudaNdarray_SIZE(A)){
4522         // If A is in col-major
4523         if ((CudaNdarray_HOST_DIMS(A)[0] <= 1)
4524             || ((CudaNdarray_HOST_STRIDES(A)[0] == 1)
4525                 && (CudaNdarray_HOST_STRIDES(A)[1] > 0)))
4526         {
4527             err = cublasSger(handle, CudaNdarray_HOST_DIMS(x)[0], CudaNdarray_HOST_DIMS(y)[0], &alpha,
4528                        CudaNdarray_DEV_DATA(x), x_strides,
4529                        CudaNdarray_DEV_DATA(y), y_strides,
4530                        CudaNdarray_DEV_DATA(A), sa_1);
4531         }
4532         // Since Sger expects A in col-major, we invert x and y to fake this.
4533         else if ((CudaNdarray_HOST_DIMS(A)[1] <= 1)
4534                 || ((CudaNdarray_HOST_STRIDES(A)[1] == 1)
4535                     && (CudaNdarray_HOST_STRIDES(A)[0] > 0)))
4536         {
4537             err = cublasSger(handle, CudaNdarray_HOST_DIMS(y)[0], CudaNdarray_HOST_DIMS(x)[0], &alpha,
4538                        CudaNdarray_DEV_DATA(y), y_strides,
4539                        CudaNdarray_DEV_DATA(x), x_strides,
4540                        CudaNdarray_DEV_DATA(A), sa_0);
4541         }
4542         // A has to be either c- or f-contiguous, with no negative strides
4543         else
4544         {
4545             PyErr_SetString(PyExc_NotImplementedError,
4546                             "non-contiguous A, or negative strides, in sger");
4547             Py_XDECREF(x_new);
4548             Py_XDECREF(y_new);
4549             return -1;
4550         }
4551     }
4552     CNDA_THREAD_SYNC;
4553     Py_XDECREF(x_new);
4554     Py_XDECREF(y_new);
4555
4556     if (CUBLAS_STATUS_SUCCESS != err)
4557     {
4558         PyErr_Format(PyExc_RuntimeError,
4559                      "cublasSger failed (%i)",
4560                      err);
4561         return -1;
4562     }
4563
4564     return 0;
4565 }
4566
4567 /**
4568  *
4569  * Precondition:
4570  *  a->dim[d] == (dims_a[d]==0) ? (1 << log2_dims_a[d]) : dims_a[d]
4571  *  z->dim[d] == (z_str[d]==0) ? 1 : dims_a[d];
4572  *
4573  *  TODO: templatize this function to support other reductions.
4574  *  All that needs to change is the initial value for sum, and the reduction operator.
4575  */
4576
4577 static __global__ void kernel_reduce_sum(const unsigned int size_z,
4578         const unsigned int nd,
4579         const int * dims_a,
4580         const int * log2_dims_a,
4581         const int * a_str,
4582         const float * a_data,
4583         const int * z_str,
4584         float * z_data)
4585 {
4586     const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;
4587     const unsigned int numThreads = blockDim.x * gridDim.x;
4588
4589     //structure data contains the strides and dimensions of both a and z
4590     // a_dim[0], a_dim[1], ... a_dim[nd-1],
4591     // a_log2dim[0], a_log2dim[1], ... a_log2dim[nd-1],
4592     // a_str[0], ... a_str[nd-1],
4593     // z_str[0], ... z_str[nd-1]
4594     extern __shared__ int structure_data[];
4595     for (unsigned int i = threadIdx.x; i < nd; i += blockDim.x)
4596     {
4597         structure_data[i+0*nd] = dims_a[i];
4598         structure_data[i+1*nd] = log2_dims_a[i];
4599         structure_data[i+2*nd] = a_str[i];
4600         structure_data[i+3*nd] = z_str[i];
4601     }
4602     dims_a = structure_data;
4603     log2_dims_a = structure_data + nd;
4604     a_str = structure_data + 2*nd;
4605     z_str = structure_data + 3*nd;
4606
4607     __syncthreads(); //wait for all the shared structure to be loaded
4608
4609     for (unsigned int i = idx; i < size_z; i += numThreads)
4610     {
4611         unsigned int ii = i;
4612         const float * a_data_i = a_data;
4613         float * z_data_i = z_data;
4614         unsigned int n_reduce_elements = 1;
4615         unsigned int n_reduce_dims = 0;
4616         unsigned int reduce_dim0 = nd-1;
4617
4618
4619         //In this loop, we locate the initial element of the slice that we'd like to reduce with this thread
4620         //  At the same time, we [re]calculate the size of that slice (n_reduce_elements)
4621         for (unsigned int d = 0; d < nd; ++d)
4622         {
4623             if (a_str[d] && (!z_str[d])) // this means 'd' is a dimension we are reducing over
4624             {
4625                 n_reduce_elements *= dims_a[d];
4626                 n_reduce_dims += 1;
4627                 reduce_dim0 = (d < reduce_dim0) ? d : reduce_dim0;
4628             }
4629             else //'d' is not a dimension that we are reducing over
4630             {
4631                 unsigned int pos_d;
4632                 if (log2_dims_a[d]==-1) //TODO: when things are working, use this switch
4633                 {
4634                     // this branch is not preferred,
4635                     // because the manual said that integer mod and div operations are slow on gpu
4636                     pos_d = (ii % dims_a[d]);
4637                     ii = (ii / dims_a[d]);
4638                 }
4639                 else
4640                 {
4641                     pos_d = (ii & ((1 << log2_dims_a[d])-1)); //take the lower log2_dims bits
4642                     ii = (ii >> log2_dims_a[d]);  //shift those lower log2_dims bits off of ii
4643                 }
4644                 a_data_i += pos_d * a_str[d];
4645                 z_data_i += pos_d * z_str[d];
4646             }
4647         }
4648         // now we've got pointers a_data_i and z_data_i into element 0 of the slice over which we are reducing
4649         // do a similar loop
4650
4651         float sum = 0.0f;
4652         switch(n_reduce_dims)
4653         {
4654             case 0:
4655                 {
4656                     sum = a_data_i[0];
4657                 }
4658                 break;
4659             case 1:
4660                 {
4661                     const int stride = a_str[reduce_dim0];
4662                     const float * a_data_i_max = a_data_i + dims_a[reduce_dim0] * stride;
4663                     while (a_data_i != a_data_i_max)
4664                     {
4665                         sum += a_data_i[0];
4666                         a_data_i += stride;
4667                     }
4668                 }
4669                 break;
4670             case 2:
4671                 {
4672                     int rd = reduce_dim0+1;
4673                     for (; rd < nd; ++rd)
4674                     {
4675                         if (a_str[rd] && (!z_str[rd])) // this means 'rd' is a dimension we are reducing over
4676                             break;
4677                     }
4678                     const int stride0 = a_str[reduce_dim0];
4679                     const int stride1 = a_str[rd];
4680                     for (int ii = 0; ii < dims_a[rd]; ++ii)
4681                     {
4682                         const float * a_data_ri = a_data_i + ii * stride1;
4683                         const float * a_data_ri_max = a_data_ri + dims_a[reduce_dim0] * stride0;
4684                         while (a_data_ri != a_data_ri_max)
4685                         {
4686                             sum += a_data_ri[0];
4687                             a_data_ri += stride0;
4688                         }
4689                     }
4690                 };
4691                 break;
4692             default:
4693                 {
4694                     for (unsigned int reduce_i = 0; reduce_i < n_reduce_elements; ++reduce_i)
4695                     {
4696                         //TODO: optimize this loop to work more like theano's Elemwise.  It's serial code.
4697                         unsigned int reduce_ii = reduce_i;
4698                         const float * a_data_ri = a_data_i;
4699
4700                         //This loop finds the element in the a slice to add.
4701                         for (unsigned int rd = reduce_dim0; rd < nd; ++rd)
4702                         {
4703                             unsigned int pos_d;
4704                             if (a_str[rd] && (!z_str[rd])) // this means 'd' is a dimension we are reducing over
4705                             {
4706                                 if (log2_dims_a[rd]==-1)
4707                                 {
4708                                     // this branch is not preferred,
4709                                     // because the manual said that integer mod and div operations are slow on gpu
4710                                     pos_d = (reduce_ii % dims_a[rd]);
4711                                     reduce_ii = (reduce_ii / dims_a[rd]);
4712                                 }
4713                                 else
4714                                 {
4715                                     pos_d = (reduce_ii & ((1 << log2_dims_a[rd])-1)); //take the lower log2_dims bits
4716                                     reduce_ii = (reduce_ii >> log2_dims_a[rd]);  //shift those lower log2_dims bits off of ii
4717                                 }
4718                                 a_data_ri += pos_d * a_str[rd];
4719                             }
4720                         }
4721                         sum += a_data_ri[0];
4722                     }
4723                 }
4724         }
4725         z_data_i[0] = sum;
4726     }
4727 }
4728
4729 static __global__ void kernel_reduce_sum_1011(
4730         const unsigned int d0,
4731         const unsigned int d1,
4732         const unsigned int d2,
4733         const unsigned int d3,
4734         const float *A, const int sA0, const int sA1, const int sA2, const int sA3,
4735         float * Z, const int sZ0)
4736 {
4737     const int threadCount = blockDim.x * blockDim.y * blockDim.z;
4738     const int threadNum = threadIdx.z * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x;
4739     extern __shared__ float buf[];
4740     float mysum = 0.0f;
4741
4742     if (warpSize != 32)
4743     {
4744         return;  //TODO: set error code
4745     }
4746
4747     for (int i0 = threadIdx.z; i0 < d0; i0 += blockDim.z)
4748     {
4749         float Ai = A[i0 * sA0 + blockIdx.x * sA1 + threadIdx.y * sA2 + threadIdx.x * sA3];
4750         mysum += Ai;
4751     }
4752     buf[threadNum] = mysum;
4753     __syncthreads();
4754
4755     // rest of function is handled by one warp
4756     if (threadNum < warpSize)
4757     {
4758         for (int i = threadNum + warpSize; i < threadCount; i += warpSize)
4759         {
4760             mysum += buf[i];
4761         }
4762         buf[threadNum] = mysum;
4763         if (threadNum < 16)
4764         {
4765             //reduce so that threadNum 0 has the sum of everything
4766             if(threadNum + 16 < threadCount) buf[threadNum] += buf[threadNum+16];
4767             if(threadNum + 8 < threadCount) buf[threadNum] += buf[threadNum+8];
4768             if(threadNum + 4 < threadCount) buf[threadNum] += buf[threadNum+4];
4769             if(threadNum + 2 < threadCount) buf[threadNum] += buf[threadNum+2];
4770             if(threadNum + 1 < threadCount) buf[threadNum] += buf[threadNum+1];
4771             if (threadNum == 0)
4772             {
4773                 Z[blockIdx.x*sZ0] = buf[0];
4774             }
4775         }
4776     }
4777 }
4778 /**
4779  * Dimensions in which the self has size 1 and A has size > 1 are considered summing dimensions
4780  * Dimensions in which self has size > 1 and A has size > 1 are considered non-summing dimensions, and in this case their sizes must be equal.
4781  */
4782 int
4783 CudaNdarray_reduce_sum(CudaNdarray * self, CudaNdarray * A)
4784 {
4785     int verbose = 0;
4786     //check input rank
4787     if (self->nd != A->nd)
4788     {
4789         PyErr_Format(PyExc_TypeError, "Rank mismatch in CudaNdarray_sum: %i vs %i", self->nd, A->nd);
4790         return -1;
4791     }
4792     for (int i = 0; i < self->nd; ++i)
4793     {
4794         if ((CudaNdarray_HOST_DIMS(self)[i] > 1) && (CudaNdarray_HOST_DIMS(self)[i] != CudaNdarray_HOST_DIMS(A)[i]))
4795         {
4796             PyErr_Format(PyExc_TypeError, "Dimension mismatch in CudaNdarray_sum: self->dim[%i] == %i , A->dim[%i] = %i",
4797                     i, CudaNdarray_HOST_DIMS(self)[i], i, CudaNdarray_HOST_DIMS(A)[i]);
4798             return -1;
4799         }
4800     }
4801
4802     int n_summations = (unsigned int)CudaNdarray_SIZE(self);
4803     if (verbose)
4804     {
4805         std::cerr << "reduce_sum n_summations " << n_summations  << '\n';
4806         std::cerr << "reduce_sum nd " << self->nd  << '\n';
4807         fprint_CudaNdarray(stderr, A);
4808         fprint_CudaNdarray(stderr, self);
4809     }
4810     if (0 && (A->nd == 4) //check to see if kernel_reduce_sum_1011 applies
4811             && (CudaNdarray_HOST_DIMS(self)[0] == 1)
4812             && (CudaNdarray_HOST_DIMS(self)[2] == 1)
4813             && (CudaNdarray_HOST_DIMS(self)[3] == 1)
4814        )
4815     {
4816         dim3 n_threads(CudaNdarray_HOST_DIMS(A)[3], CudaNdarray_HOST_DIMS(A)[2]);
4817         dim3 n_blocks(CudaNdarray_HOST_DIMS(A)[1]);
4818         while (n_threads.x * n_threads.y * n_threads.z < NUM_VECTOR_OP_THREADS_PER_BLOCK) ++n_threads.z;
4819         n_threads.z -= 1;
4820         if (n_threads.z > 64) n_threads.z = 64;
4821         if (n_threads.z)
4822         {
4823             if (verbose) printf("trying kernel_reduce_sum_1011\n");
4824             int n_shared = sizeof(float) * n_threads.x * n_threads.y * n_threads.z;
4825             kernel_reduce_sum_1011<<<n_blocks, n_threads, n_shared>>>(
4826                     CudaNdarray_HOST_DIMS(A)[0],
4827                     CudaNdarray_HOST_DIMS(A)[1],
4828                     CudaNdarray_HOST_DIMS(A)[2],
4829                     CudaNdarray_HOST_DIMS(A)[3],
4830                     CudaNdarray_DEV_DATA(A),
4831                     CudaNdarray_HOST_STRIDES(A)[0],
4832                     CudaNdarray_HOST_STRIDES(A)[1],
4833                     CudaNdarray_HOST_STRIDES(A)[2],
4834                     CudaNdarray_HOST_STRIDES(A)[3],
4835                     CudaNdarray_DEV_DATA(self),
4836                     CudaNdarray_HOST_STRIDES(self)[1]);
4837             CNDA_THREAD_SYNC;
4838             if (cudaSuccess == cudaGetLastError()) return 0;
4839             if (verbose) printf("failed, falling back to kernel_reduce_sum\n");
4840         }
4841     }
4842
4843     int n_threads_per_block = std::min(n_summations,
4844             NUM_VECTOR_OP_THREADS_PER_BLOCK);
4845     int n_blocks = std::min(ceil_intdiv(n_summations,n_threads_per_block),
4846             NUM_VECTOR_OP_BLOCKS);
4847     int n_structure_cache = self->nd * 4 * sizeof(int);
4848
4849     if (verbose)
4850     {
4851         std::cerr << "n_blocks, n_threads_per_block " << n_blocks << ' ' << n_threads_per_block  << '\n';
4852     }
4853     assert (self->nd > 0);
4854     assert (self->nd == A->nd);
4855     kernel_reduce_sum<<<n_blocks, n_threads_per_block, n_structure_cache>>>(
4856             n_summations,
4857             self->nd,
4858             CudaNdarray_DEV_DIMS(A),
4859             CudaNdarray_DEV_LOG2DIMS(A),
4860             CudaNdarray_DEV_STRIDES(A),
4861             CudaNdarray_DEV_DATA(A),
4862             CudaNdarray_DEV_STRIDES(self),
4863             CudaNdarray_DEV_DATA(self));
4864     CNDA_THREAD_SYNC;
4865     cudaError_t err = cudaGetLastError();
4866     if (cudaSuccess != err)
4867     {
4868         PyErr_Format(PyExc_RuntimeError, "Cuda error: %s: %s.\n", "kernel_reduce_sum", cudaGetErrorString(err));
4869         return -1;
4870     }
4871     return 0;
4872 }
4873 int
4874 CudaNdarray_reduce_prod(CudaNdarray * self, const CudaNdarray * A)
4875 {
4876     PyErr_SetString(PyExc_NotImplementedError, "");
4877     return -1;
4878 }
4879 int
4880 CudaNdarray_reduce_min(CudaNdarray * self, const CudaNdarray * A)
4881 {
4882     PyErr_SetString(PyExc_NotImplementedError, "");
4883     return -1;
4884 }
4885 int
4886 CudaNdarray_reduce_max(CudaNdarray * self, const CudaNdarray * A)
4887 {
4888     PyErr_SetString(PyExc_NotImplementedError, "");
4889     return -1;
4890 }
4891
4892
4893 /**
4894  *
4895  *  pattern is a permutation of [0, 1, ... self->nd-1] with the following twists:
4896  *  - an element 'd' of the permutation can be dropped if CudaNdarray_HOST_DIMS(self)[d] == 1
4897  *  - any number of '-1' elements can be in the pattern, and they will cause new ranks (with dim==1) to be inserted.
4898  *
4899  *  For example, if CudaNdarray_HOST_DIMS(self) == [4, 5, 1, 6], and pattern = [0,3,-1,-1, 1], then CudaNdarray_HOST_DIMS(self) would be modified to become:
4900  *     [4, 6, 1, 1, 5] (we dropped the original dim[2]==1, and inserted two singleton dimensions with the -1s.
4901  */
4902 int
4903 CudaNdarray_dimshuffle(CudaNdarray * self, unsigned int len, const int * pattern)
4904 {
4905     //TODO: pass a workspace pointer to avoid the internal malloc
4906     int * newdims = (int *)malloc(sizeof(int) * (len + len + self->nd)); //we tack on the taken buffer here for speed of not having to malloc twice.
4907     int * newstrides = newdims + len;
4908     int * dims_taken = newstrides + len;
4909     if (!newdims)
4910     {
4911         PyErr_SetString(PyExc_MemoryError, "CudaNdarray_dimshuffle: Failed to allocate temporary space");
4912         return -1;
4913     }
4914     for (int i = 0; i < self->nd; ++i)
4915     {
4916         dims_taken[i] = 0;
4917     }
4918     for (int i = 0; i < len; ++i)
4919     {
4920         if (pattern[i] < 0)
4921         {
4922             newdims[i] = 1;
4923             newstrides[i] = 0;
4924         }
4925         else if(dims_taken[pattern[i]])
4926         {
4927             PyErr_Format(PyExc_ValueError, "Cudandarray_dimshuffle: invalid pattern for Cudandarray_dimshuffle. You used the dimensions %d multiple time",
4928                          pattern[i]);
4929             free(newdims);
4930             return -1;
4931         }
4932         else if (pattern[i]>= self->nd)
4933         {
4934             PyErr_Format(PyExc_ValueError, "Cudandarray_dimshuffle: invalid pattern for Cudandarray_dimshuffle. You asked for a dimensions that don't exist %d for
a %d dims CudaNdarray",
4935                          pattern[i], self->nd);
4936             free(newdims);
4937             return -1;
4938         }
4939         else
4940         {
4941             newdims[i] = CudaNdarray_HOST_DIMS(self)[pattern[i]];
4942             newstrides[i] = CudaNdarray_HOST_STRIDES(self)[pattern[i]];
4943             dims_taken[pattern[i]] = 1;
4944         }
4945     }
4946     //Check if we dropped not broadcastable dims
4947     for (int i = 0; i < self->nd; ++i)
4948     {
4949         if (dims_taken[i]==0 && CudaNdarray_HOST_DIMS(self)[i]!=1)
4950         {
4951             PyErr_SetString(PyExc_ValueError, "Cudandarray_dimshuffle: You cannot drop a non-broadcastable dimension.");
4952             free(newdims);
4953             return -1;
4954         }
4955     }
4956     //swap this structure in for the one in self, and sync to the card
4957     if (CudaNdarray_set_nd(self, len))
4958     {
4959         free(newdims);
4960         return -1;
4961     }
4962     for (int i = 0; i < len; ++i)
4963     {
4964         CudaNdarray_set_dim(self, i, newdims[i]);
4965         CudaNdarray_set_stride(self, i, newstrides[i]);
4966     }
4967     if (cnda_copy_structure_to_device(self))
4968     {
4969         free(newdims);
4970         return -1;
4971     }
4972     free(newdims);
4973     return 0;
4974 }
4975
4976
4977
4978 /**
4979  *
4980  *  This is the function that bind to python.
4981  *  See CudaNdarray_dimshuffle to call from C.
4982  *  We use -1 to mean 'x' as in Tensor Dimshuffle.
4983  */
4984 PyObject *
4985 CudaNdarray_Dimshuffle(PyObject* _unused, PyObject* args)
4986 {
4987     PyObject * self = NULL;
4988     PyObject * pattern_object = NULL;
4989     int * pattern = NULL;
4990     PyObject * rval = NULL;
4991     int success = -1;
4992     //const int * dims = NULL;
4993
4994     //args should consist of two python objects ("OO")
4995     if (! PyArg_ParseTuple(args, "OO", &self, &pattern_object))
4996         return NULL;
4997
4998     if (!CudaNdarray_Check(self) )
4999     {
5000         PyErr_SetString(PyExc_TypeError, "First argument to cuda_ndarray.dimshuffle must be a CudaNdarray");
5001         return NULL;
5002     }
5003
5004     //parse pattern_object into int * pattern
5005
5006     Py_ssize_t pattern_dim =  PyObject_Length(pattern_object);
5007
5008     if (pattern_dim < 0)
5009     {
5010         PyErr_SetString(PyExc_TypeError, "Couldn't get length of third argument to cuda_ndarray.dimshuffle");
5011         return NULL;
5012     }
5013
5014     pattern = (int *) malloc( pattern_dim * sizeof(int));
5015
5016     for (Py_ssize_t i = 0; i < pattern_dim; i++)
5017     {
5018         PyObject * idx = PyLong_FromLong(i);
5019
5020         if (idx == NULL)
5021         {
5022             PyErr_SetString(PyExc_Exception, "Couldn't make long object to loop over list/tuple");
5023             goto CudaNdarray_dimshuffle_fail;
5024         }
5025
5026         long elem_value = 0;
5027
5028         PyObject * elem = PyObject_GetItem(pattern_object, idx);
5029
5030         if (elem == NULL)
5031         {
5032             Py_XDECREF( elem);
5033             PyErr_SetString(PyExc_ValueError, "Third argument to dimshuffle must be list or tuple of integers");
5034             goto CudaNdarray_dimshuffle_fail;
5035         }
5036
5037         elem_value = PyInt_AsLong(elem);
5038
5039         if (elem_value == -1 && PyErr_Occurred() )
5040         {
5041             Py_XDECREF(elem);
5042             PyErr_SetString(PyExc_ValueError, "Third argument to dimshuffle must be list or tuple of integers");
5043             goto CudaNdarray_dimshuffle_fail;
5044         }
5045
5046         pattern[i] = elem_value;
5047
5048         Py_XDECREF( elem );
5049         Py_XDECREF( idx );
5050     }
5051
5052     //allocate rval
5053     rval =  (PyObject *) CudaNdarray_View((CudaNdarray *) self);
5054
5055     if (rval == NULL)
5056     {
5057         //CudaNdarray_New should have set the exception string
5058         goto CudaNdarray_dimshuffle_fail;
5059     }
5060
5061
5062     //printf("pattern_dim: %d\n",pattern_dim);
5063     //printf("pattern: %d %d\n",pattern[0],pattern[1]);
5064     //dims = CudaNdarray_HOST_DIMS( (CudaNdarray *) self);
5065     //printf("dims before: %d %d\n",dims[0],dims[1]);
5066
5067     success = CudaNdarray_dimshuffle((CudaNdarray *) rval, pattern_dim, pattern);
5068
5069     if (success != 0)
5070     {
5071         //Exception string should already be set by CudaNdarray_dimshuffle
5072         goto CudaNdarray_dimshuffle_fail;
5073     }
5074
5075     free(pattern);
5076
5077     return rval;
5078
5079     CudaNdarray_dimshuffle_fail:
5080
5081     if (pattern != NULL)
5082         free(pattern);
5083
5084     Py_XDECREF(rval);
5085     return NULL;
5086 }
5087
5088
5089 int
5090 cnda_structure_size(int nd)
5091 {
5092     // dim0, dim1, ...
5093     // str0, str1, ...
5094     // log2(dim0), log2(dim1), ...
5095     return nd + nd + nd;
5096 }
5097
5098 const int *
5099 CudaNdarray_HOST_DIMS(const CudaNdarray * self)
5100 {
5101     return self->host_structure;
5102 }
5103
5104 const int *
5105 CudaNdarray_HOST_STRIDES(const CudaNdarray * self)
5106 {
5107     return self->host_structure + self->nd;
5108 }
5109 const int *
5110 CudaNdarray_HOST_LOG2DIMS(const CudaNdarray * self)
5111 {
5112     return self->host_structure + 2*self->nd;
5113 }
5114
5115 int
5116 CudaNdarray_EqualAndIgnore(CudaNdarray *cnda1, CudaNdarray *cnda2, int ignoreSync, int ignoreBase)
5117 {
5118     int verbose = 0;
5119
5120     if (!ignoreSync && cnda1->dev_structure_fresh != cnda2->dev_structure_fresh)
5121     {
5122         if(verbose) fprintf(stdout, "CUDANDARRAY_EQUAL FAILED : 1\n");
5123         return 0;
5124     }
5125
5126     if (cnda1->nd != cnda2->nd)
5127     {
5128         if(verbose) fprintf(stdout, "CUDANDARRAY_EQUAL FAILED : 2\n");
5129         return 0;
5130     }
5131
5132     for (int i=0; i < 2*cnda1->nd; i++)
5133     {
5134         if (cnda1->host_structure[i] != cnda2->host_structure[i])
5135         {
5136             if(verbose)
5137                 fprintf(stdout, "CUDANDARRAY_EQUAL : host_structure : %d, %d, %d\n", i, cnda1->host_structure[i], cnda2->host_structure[i]);
5138             return 0;
5139         }
5140     }
5141
5142     if (!ignoreBase && cnda1->base != cnda2->base)
5143     {
5144         if(verbose) fprintf(stdout, "CUDANDARRAY_EQUAL FAILED : 4");
5145         return 0;
5146     }
5147     else if (cnda1->data_allocated != cnda2->data_allocated)
5148     {
5149         if(verbose) fprintf(stdout, "CUDANDARRAY_EQUAL FAILED : 5");
5150         return 0;
5151     }
5152     else if (cnda1->data_allocated && cnda1->devdata != cnda2->devdata)
5153     {
5154         if(verbose) fprintf(stdout, "CUDANDARRAY_EQUAL FAILED : 6");
5155         // no need to check devdata if data is not allocated
5156         return 0;
5157     }
5158
5159     return 1;
5160 }
5161
5162
5163 int
5164 CudaNdarray_Equal(CudaNdarray *cnda1, CudaNdarray *cnda2)
5165 {
5166     return CudaNdarray_EqualAndIgnore(cnda1, cnda2, 0, 0);
5167 }
5168
5169 int
5170 cnda_copy_structure_to_device(const CudaNdarray * self)
5171 {
5172     //If the device structure do not exists, create it.
5173     //We allocate it here as we do not need it often.
5174     //In fact, we need it so infrequently that we expect
5175     //that most object won't need it. Not allocating it
5176     //save a significant when creating object.
5177     //This speed up a benchmark by 8% with the gc.
5178     if (!self->dev_structure)
5179     {
5180         int struct_size = cnda_structure_size(self->nd);
5181         if (struct_size)
5182         {
5183             self->dev_structure = (int*)device_malloc(struct_size* sizeof(int));
5184             if (NULL == self->dev_structure)
5185             {
5186                 return -1;
5187             }
5188         }
5189     }
5190     if (cublasSetVector(cnda_structure_size(self->nd),
5191                         sizeof(int),
5192                         self->host_structure,
5193                         1,
5194                         self->dev_structure,
5195                         1) != CUBLAS_STATUS_SUCCESS)
5196     {
5197         PyErr_SetString(PyExc_RuntimeError, "error copying structure to device memory");
5198         return -1;
5199     }
5200     self->dev_structure_fresh = 1;
5201     return 0;
5202 }
5203
5204 const int *
5205 CudaNdarray_DEV_DIMS(const CudaNdarray * self)
5206 {
5207     if (!self->dev_structure_fresh)
5208     {
5209         if (cnda_copy_structure_to_device(self))
5210             return NULL;
5211     }
5212     return self->dev_structure;
5213 }
5214 const int *
5215 CudaNdarray_DEV_STRIDES(const CudaNdarray * self)
5216 {
5217     if (!self->dev_structure_fresh)
5218     {
5219         if (cnda_copy_structure_to_device(self))
5220             return NULL;
5221     }
5222     return self->dev_structure + self->nd;
5223 }
5224 const int *
5225 CudaNdarray_DEV_LOG2DIMS(const CudaNdarray * self)
5226 {
5227     if (!self->dev_structure_fresh)
5228     {
5229         if (cnda_copy_structure_to_device(self))
5230             return NULL;
5231     }
5232     return self->dev_structure + 2*self->nd;
5233 }
5234 float *
5235 CudaNdarray_DEV_DATA(const CudaNdarray * self)
5236 {
5237     return self->devdata;
5238 }
5239
5240 /**
5241  * Return the number of elements in the ndarray (product of the dimensions)
5242  */
5243 size_t
5244 CudaNdarray_SIZE(const CudaNdarray *self)
5245 {
5246     if (self->nd == -1) return 0;
5247     size_t size = 1;
5248     for (int i = 0; i < self->nd; ++i)
5249     {
5250         size *= CudaNdarray_HOST_DIMS(self)[i];
5251     }
5252     return size;
5253 }
5254
5255 PyObject *
5256 CudaNdarray_SIZE_Object(const CudaNdarray *self, void *closure)
5257 {
5258     return PyInt_FromLong(CudaNdarray_SIZE(self));
5259 }
5260
5261 int CudaNdarray_set_device_data(CudaNdarray * self, float * data, const CudaNdarray * base)
5262 {
5263     return CudaNdarray_set_device_data(self, data, (PyObject *) base);
5264 }
5265
5266 PyObject * CudaNdarray_IS_C_Contiguous(CudaNdarray * self)
5267 {
5268     return PyBool_FromLong(CudaNdarray_is_c_contiguous(self));
5269 }
5270
5271 int fprint_CudaNdarray(FILE * fd, const CudaNdarray *self)
5272 {
5273     cudaError_t err = cudaGetLastError();
5274     if( cudaSuccess != err)
5275     {
5276         PyErr_Format(PyExc_RuntimeError,
5277                      "Cuda error: %s: %s.",
5278                      "fprint_CudaNdarray was called with an uncleared error",
5279                      cudaGetErrorString(err));
5280         return -1;
5281     }
5282     fprintf(fd, "CudaNdarray <%p, %p> nd=%i dev_structure_fresh=%d data_allocated=%d\n",
5283             self, self->devdata, self->nd, self->dev_structure_fresh, self->data_allocated);
5284     fprintf(fd, "\tHOST_DIMS:      ");
5285     for (int i = 0; i < self->nd; ++i)
5286     {
5287         fprintf(fd, "%i\t", CudaNdarray_HOST_DIMS(self)[i]);
5288     }
5289     fprintf(fd, "\n\tHOST_STRIDES: ");
5290     for (int i = 0; i < self->nd; ++i)
5291     {
5292         fprintf(fd, "%i\t", CudaNdarray_HOST_STRIDES(self)[i]);
5293     }
5294
5295     if (self->dev_structure)
5296     {
5297         int data=0;
5298         fprintf(fd, "\n\tDEV_DIMS:      ");
5299         for (int i = 0; i < self->nd; ++i)
5300         {
5301             cublasGetVector(1, sizeof(int),
5302                             self->dev_structure+i, 1,
5303                             &data, 1);
5304             fprintf(fd, "%i\t", data);
5305         }
5306         fprintf(fd, "\n\tDEV_STRIDES: ");
5307         for (int i = 0; i < self->nd; ++i)
5308         {
5309             cublasGetVector(1, sizeof(int),
5310                             self->dev_structure + self->nd+i, 1,
5311                             &data, 1);
5312             fprintf(fd, "%i \t", data);
5313         }
5314         fprintf(fd, "\n");
5315     }
5316     else
5317     {
5318         fprintf(fd, "\n\tdev_structure not allocated\n");
5319     }
5320
5321     err = cudaGetLastError();
5322     if( cudaSuccess != err)
5323     {
5324         PyErr_Format(PyExc_RuntimeError,
5325                      "Cuda error: %s: %s.",
5326                      "fprint_CudaNdarray",
5327                      cudaGetErrorString(err));
5328         return -1;
5329     }
5330     return 0;
5331 }
5332
5333
5334 int CudaNdarray_prep_output(CudaNdarray ** arr, int nd,
5335                             const int * dims, int fortran)
5336 {
5337     bool allocated = false;
5338     if (*arr == NULL)
5339     {
5340         // This allocates the metadata but not the data
5341         *arr = (CudaNdarray *) CudaNdarray_new_nd(nd);
5342         if (*arr == NULL)
5343             return -1;
5344         allocated = true;
5345     }
5346
5347     if (CudaNdarray_alloc_contiguous(*arr, nd, dims, fortran))
5348     {
5349         if (allocated)
5350         {
5351             Py_DECREF(*arr);
5352             *arr = NULL;
5353         }
5354         return -1;
5355     }
5356     return 0;
5357 }
5358
5359
5360 /*
5361   Local Variables:
5362   mode:c++
5363   c-basic-offset:4
5364   c-file-style:"stroustrup"
5365   indent-tabs-mode:nil
5366   fill-column:79
5367   End:
5368 */
5369 // vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:textwidth=79 :
5370
===============================
nvcc warning : The 'compute_20', 'sm_20', and 'sm_21' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress
warning).
In file included from /usr/local/cuda/bin/../targets/x86_64-linux/include/cuda_runtime.h:78:0,
                 from <command-line>:0:
/usr/local/cuda/bin/../targets/x86_64-linux/include/host_config.h:119:2: error: #error -- unsupported GNU version! gcc versions later than 5 are not supported!
 #error -- unsupported GNU version! gcc versions later than 5 are not supported!
  ^~~~~
In file included from mod.cu:10:0:
/home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:17:0: warning: "PyString_Check" redefined
 #define PyString_Check PyUnicode_Check

In file included from /home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,
                 from mod.cu:10:
/home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:71:0: note: this is the location of the previous definition
 #define PyString_Check PyBytes_Check

In file included from mod.cu:10:0:
/home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:18:0: warning: "PyString_FromString" redefined
 #define PyString_FromString PyUnicode_FromString

In file included from /home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,
                 from mod.cu:10:
/home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:73:0: note: this is the location of the previous definition
 #define PyString_FromString PyBytes_FromString

In file included from mod.cu:10:0:
/home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:19:0: warning: "PyString_AsString" redefined
 #define PyString_AsString PyUnicode_AsUTF8

In file included from /home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,
                 from mod.cu:10:
/home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:80:0: note: this is the location of the previous definition
 #define PyString_AsString PyBytes_AsString

In file included from mod.cu:10:0:
/home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:20:0: warning: "PyString_FromStringAndSize" redefined
 #define PyString_FromStringAndSize PyUnicode_FromStringAndSize

In file included from /home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,
                 from mod.cu:10:
/home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:74:0: note: this is the location of the previous definition
 #define PyString_FromStringAndSize PyBytes_FromStringAndSize

In file included from mod.cu:10:0:
/home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:21:0: warning: "PyString_Size" redefined
 #define PyString_Size PyUnicode_GET_SIZE

In file included from /home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/theano/sandbox/cuda/cuda_ndarray.cuh:11:0,
                 from mod.cu:10:
/home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/numpy/core/include/numpy/npy_3kcompat.h:82:0: note: this is the location of the previous definition
 #define PyString_Size PyBytes_Size


['nvcc', '-shared', '-O3', '-m64', '-Xcompiler', '-DCUDA_NDARRAY_CUH=mc72d035fdf91890f3b36710688069b2e,-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,-fPIC,-fvisibility=hi
dden', '-Xlinker', '-rpath,/home/holas/.theano/compiledir_Linux-4.13--generic-x86_64-with-debian-stretch-sid-x86_64-3.5.2-64/cuda_ndarray', '-I/home/holas/anaconda2/env
s/rllab3/lib/python3.5/site-packages/theano/sandbox/cuda', '-I/home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/numpy/core/include', '-I/home/holas/anaconda
2/envs/rllab3/include/python3.5m', '-I/home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/theano/gof', '-o', '/home/holas/.theano/compiledir_Linux-4.13--gener
ic-x86_64-with-debian-stretch-sid-x86_64-3.5.2-64/cuda_ndarray/cuda_ndarray.so', 'mod.cu', '-L/home/holas/anaconda2/envs/rllab3/lib', '-lcublas', '-lpython3.5m', '-lcud
art']
ERROR (theano.sandbox.cuda): Failed to compile cuda_ndarray.cu: ('nvcc return status', 1, 'for cmd', 'nvcc -shared -O3 -m64 -Xcompiler -DCUDA_NDARRAY_CUH=mc72d035fdf918
90f3b36710688069b2e,-DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION,-fPIC,-fvisibility=hidden -Xlinker -rpath,/home/holas/.theano/compiledir_Linux-4.13--generic-x86_64-with
-debian-stretch-sid-x86_64-3.5.2-64/cuda_ndarray -I/home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/theano/sandbox/cuda -I/home/holas/anaconda2/envs/rllab3
/lib/python3.5/site-packages/numpy/core/include -I/home/holas/anaconda2/envs/rllab3/include/python3.5m -I/home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/t
heano/gof -o /home/holas/.theano/compiledir_Linux-4.13--generic-x86_64-with-debian-stretch-sid-x86_64-3.5.2-64/cuda_ndarray/cuda_ndarray.so mod.cu -L/home/holas/anacond
a2/envs/rllab3/lib -lcublas -lpython3.5m -lcudart')
/home/holas/anaconda2/envs/rllab3/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.s
ignal.pool module.
  "downsample module has been moved to the theano.tensor.signal.pool module.")
2018-04-08 17:02:47.556622 CEST | Populating workers...
2018-04-08 17:02:47.556940 CEST | Populated
0% [##############################] 100% | ETA: 00:00:00
Total time elapsed: 00:00:01
2018-04-08 17:02:48.846224 CEST | itr #0 | fitting baseline...
2018-04-08 17:02:48.917599 CEST | itr #0 | fitted
... toto je uz ok ...
